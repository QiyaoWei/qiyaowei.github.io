<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      A Meticulous Guide to Advances in Deep Learning Efficiency over the Years | Alex L. Zhang
    
  
</title>
<meta name="author" content="Alex L. Zhang">
<meta name="description" content="A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">






  <!-- OpenGraph -->
  <meta property="og:site_name" content="Alex L. Zhang">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Alex L. Zhang | A Meticulous Guide to Advances in Deep Learning Efficiency over the Years">
  <meta property="og:url" content="https://alexzhang13.github.io/blog/2024/efficient-dl/">
  <meta property="og:description" content="A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.">
  
    <meta property="og:image" content="https://alexzhang13.github.io/assets/img/preview/default.png">
  
  <meta property="og:locale" content="en">

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="A Meticulous Guide to Advances in Deep Learning Efficiency over the Years">
  <meta name="twitter:description" content="A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.">
  
    <meta name="twitter:image" content="https://alexzhang13.github.io/assets/img/preview/default.png">
  
  
    <meta name="twitter:site" content="@a1zhang">
    <meta name="twitter:creator" content="@a1zhang">
  



  <!-- Schema.org -->
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Alex L. Zhang"
        },
        "url": "https://alexzhang13.github.io/blog/2024/efficient-dl/",
        "@type": "BlogPosting",
        "description": "A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.",
        "headline": "A Meticulous Guide to Advances in Deep Learning Efficiency over the Years",
        
        "sameAs": ["https://scholar.google.com/citations?user=rtCr0q4AAAAJ", "https://github.com/alexzhang13", "https://www.linkedin.com/in/alexzhang13", "https://twitter.com/a1zhang"],
        
        "name": "Alex L. Zhang",
        "@context": "https://schema.org"
    }
  </script>



<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AB%A1&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://alexzhang13.github.io/blog/2024/efficient-dl/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
} ul li {
  margin: 0px 0;
  margin-bottom: 0px;
} ol li {
  margin: 0px 0;
  margin-bottom: 0px;
} ul {
  margin: 0;
} hr {
  margin-top: 10px;
  margin-bottom: 10px;
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "A Meticulous Guide to Advances in Deep Learning Efficiency over the Years",
            "description": "A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.",
            "published": "October 30, 2024",
            "authors": [
              
              {
                "author": "Alex Zhang",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Princeton University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Alex</span>
            
            L.
            Zhang
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/publications/">publications and pre-prints
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/projects/">projects
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/academics/">academics
                    
                  </a>
                </li>
              
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>A Meticulous Guide to Advances in Deep Learning Efficiency over the Years</h1>
        <p>A very long and thorough guide how deep learning algorithms, hardware, libraries, compilers, and more have become more efficient.</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#part-i-the-beginning-1980s-2011">Part I. The Beginning (1980s - 2011)</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#i1-existing-fast-linear-algebra-methods">I1. Existing Fast Linear Algebra Methods</a>
                      </li>
                    
                      <li>
                        <a href="#i2-compute-unified-device-architecture-cuda-2006">I2. Compute Unified Device Architecture (CUDA), 2006</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-ii-oh-s-deep-learning-works-2012-2020">Part II. Oh s*** — Deep learning works! (2012 - 2020)</a>
                </div>
                
              
                <div>
                  <a href="#part-ii1-the-first-breakthrough-on-images">Part II1. The first breakthrough on images!</a>
                </div>
                
              
                <div>
                  <a href="#part-ii2-deep-learning-frameworks-emerge">Part II2. Deep learning frameworks emerge</a>
                </div>
                
              
                <div>
                  <a href="#part-ii3-new-deep-learning-architectures-emerge">Part II3. New deep learning architectures emerge</a>
                </div>
                
              
                <div>
                  <a href="#part-ii4-efficient-convergence-inductive-biases-and-architectural-choices">Part II4. Efficient convergence. Inductive biases and architectural choices</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#ii4a-inductive-biases-that-lead-to-better-convergence-behavior">II4a. Inductive biases that lead to better convergence behavior</a>
                      </li>
                    
                      <li>
                        <a href="#ii4b-searching-the-space-of-solutions-meta-optimization">II4b. Searching the space of solutions (meta-optimization)</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-ii5-efficient-convergence-optimizers">Part II5. Efficient convergence. Optimizers</a>
                </div>
                
              
                <div>
                  <a href="#part-ii6-pause-how-much-of-this-scale-is-really-necessary">Part II6. Pause. How much of this scale is really necessary</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#ii6a-model-pruning">II6a. Model Pruning</a>
                      </li>
                    
                      <li>
                        <a href="#ii6b-embedding-pruning-or-hashing">II6b. Embedding Pruning or Hashing</a>
                      </li>
                    
                      <li>
                        <a href="#ii6c-quantization">II6c. Quantization</a>
                      </li>
                    
                      <li>
                        <a href="#ii6d-the-grandfather-of-efficient-ml-and-tinyml">II6d. The Grandfather of Efficient ML and TinyML</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#iix-hardware">IIx. Hardware</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#iix1-nvidia-gpus-from-tesla-2006-to-ampere-2020">IIx1. NVIDIA GPUs from Tesla (2006) to Ampere (2020)</a>
                      </li>
                    
                      <li>
                        <a href="#iix2-googles-tensor-processing-units-tpus">IIx2. Googles Tensor Processing Units (TPUs)</a>
                      </li>
                    
                      <li>
                        <a href="#iix3-potpourri-of-other-interesting-hardware">IIx3. Potpourri of other interesting hardware</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-iii-the-era-of-scale-till-we-fail-2020-now">Part III. The Era of Scale till we Fail (2020 - Now)</a>
                </div>
                
              
                <div>
                  <a href="#part-iii0-lets-talk-about-the-h100-gpu">Part III0. Lets talk about the H100 GPU</a>
                </div>
                
              
                <div>
                  <a href="#part-iii1-the-era-of-scale-on-a-single-gpu">Part III1. The Era of Scale (on a single GPU)</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#iii10-early-insights">III10. Early insights</a>
                      </li>
                    
                      <li>
                        <a href="#iii1a-shaving-complexity-through-approximate-methods">III1a. Shaving complexity through Approximate Methods</a>
                      </li>
                    
                      <li>
                        <a href="#iii1b-architecture-design">III1b. Architecture Design</a>
                      </li>
                    
                      <li>
                        <a href="#iii1c-fine-tuning-large-models-efficiently">III1c. Fine-tuning Large Models Efficiently</a>
                      </li>
                    
                      <li>
                        <a href="#iii1d-fused-kernels-and-the-gpgpu">III1d. Fused kernels and the GPGPU</a>
                      </li>
                    
                      <li>
                        <a href="#iii1e-deep-learning-compilers">III1e. Deep Learning Compilers</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-iii2-the-era-of-scale-distributed-version">Part III2. The Era of Scale (distributed version)</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#iii2a-data-parallelism">III2a. Data parallelism</a>
                      </li>
                    
                      <li>
                        <a href="#iii2b-model-parallelism">III2b. Model parallelism</a>
                      </li>
                    
                      <li>
                        <a href="#iii2c-pipeline-parallelism">III2c. Pipeline parallelism</a>
                      </li>
                    
                      <li>
                        <a href="#iii2d-architecture-specific-parallelism">III2d. Architecture-specific Parallelism</a>
                      </li>
                    
                      <li>
                        <a href="#iii2e-multi-node-distributed-training">III2e. Multi-node distributed training</a>
                      </li>
                    
                      <li>
                        <a href="#iii2f-libraries-for-distributed-deep-learning-workloads">III2f. Libraries for distributed deep learning workloads</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-iii3-scaling-laws">Part III3. Scaling Laws</a>
                </div>
                
              
                <div>
                  <a href="#part-iii4-revisiting-downwards-scaling">Part III4. Revisiting downwards scaling</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#iii4a-small-language-models-slms">III4a. Small Language Models (SLMs)</a>
                      </li>
                    
                      <li>
                        <a href="#iii4b-modern-quantization-techniques">III4b. Modern quantization techniques</a>
                      </li>
                    
                      <li>
                        <a href="#iii4c-sparse-parameters">III4c. Sparse Parameters</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-iii5-what-about-model-inference">Part III5. What about model inference?</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#iii5a-generative-model-serving">III5a. Generative model serving</a>
                      </li>
                    
                      <li>
                        <a href="#iii5b-fast-decoding-strategies">III5b. Fast decoding strategies</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#part-n-modern-day-and-beyond">Part N. Modern Day and Beyond</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#n1-what-s-up-with-these-superclusters">N1. What’s up with these superclusters?</a>
                      </li>
                    
                      <li>
                        <a href="#n2-how-much-bigger-are-industry-resources-than-academia">N2. How much bigger are industry resources than academia?</a>
                      </li>
                    
                      <li>
                        <a href="#n3-how-fast-can-we-train-old-models-with-modern-techniques">N3. How fast can we train old models with modern techniques?</a>
                      </li>
                    
                      <li>
                        <a href="#n4-recent-efforts-to-scale-hybrid-or-non-transformer">N4. Recent efforts to scale hybrid or non-Transformer.</a>
                      </li>
                    
                      <li>
                        <a href="#n5-model-efficiency-benchmarks">N5. Model efficiency Benchmarks</a>
                      </li>
                    
                      <li>
                        <a href="#n6-startups-in-the-efficient-deep-learning-space">N6. Startups in the Efficient Deep Learning Space</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#resources">Resources</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#a1-where-to-access-free-gpus">A1. Where to access “free” GPUs?</a>
                      </li>
                    
                      <li>
                        <a href="#a2-large-training-and-finetuning-frameworks">A2. Large training and finetuning frameworks.</a>
                      </li>
                    
                      <li>
                        <a href="#a3-model-compression-frameworks">A3. Model compression frameworks.</a>
                      </li>
                    
                      <li>
                        <a href="#a4-profiling-tools">A4. Profiling Tools.</a>
                      </li>
                    
                      <li>
                        <a href="#a5-from-scratch-style-tutorials">A5. “From scratch”-style tutorials.</a>
                      </li>
                    
                      <li>
                        <a href="#a6-designing-deep-learning-clusters-and-network-topology">A6. Designing deep learning clusters and network topology.</a>
                      </li>
                    
                      <li>
                        <a href="#a7-useful-surveys-on-efficiency">A7. Useful surveys on efficiency.</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#acknowledgements">Acknowledgements</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <p><em>This post offers a comprehensive and chronological guide to advances in deep learning from the perspective of efficiency: things like clusters, individual hardware, deep learning libraries, compilers — even architectural changes. This post is not a survey paper, and is intended to provide the reader with broader intuition about this field —  it would be impossible to include every little detail that has emerged throughout the last 40 years. The posted X thread <a href="https://x.com/a1zhang/status/1851963904491950132" rel="external nofollow noopener" target="_blank">https://x.com/a1zhang/status/1851963904491950132</a> also has a very high-level summary of what to expect!</em></p>

<p><strong>Preface.</strong> The field of deep learning has flourished in the past decade to the point where it is hard as both a researcher and a student to keep track of what is going on. Sometimes, I even find it hard to keep track of the <strong>actual</strong> direction of the field. In a field that often feels hand-wavy and where many methods and results feel lackluster in practice, I wanted to at least get a sense for progress in how we got to where we are now.</p>

<p>I wanted to write this post in a narrative form — to 1) be digestible to the reader rather than an information dump, and 2) allow the reader to view the field from a macroscopic lens and understand why the field moved the way it did. I have tried to be as paper-focused as possible (similar to <a href="https://lilianweng.github.io/" rel="external nofollow noopener" target="_blank">Lilian Weng style blogs</a>!) and include as many landmark (or just cool) works as I saw fit; if the reader feels something should be included or edited, please let me know<d-footnote>I really hope all of the information is correct and I’ve tried to make sure of it as much as possible, but it is possible I’ve made errors! If you find any, feel free to shoot me an email and let me know! I’m quite a young person, so I was probably playing Minecraft hypixel when some of these breakthroughs happened. Finally, I always recommend reading the original paper when you want to understand something in more depth. There’s no way for me to fit all of the information about every work here (especially the math), so if you’re ever confused and care enough to know the details, I’ve included both citations and a direct link to every mentioned work.</d-footnote>! Before we begin, let me just list out some relevant numbers to give us a bit of appreciation for all of the advances to come. I’ve also added some notes for folks who aren’t familiar with what these numbers really mean.</p>

<ul>
  <li>NVIDIA’s newest <strong><a href="https://www.anandtech.com/show/21310/nvidia-blackwell-architecture-and-b200b100-accelerators-announced-going-bigger-with-smaller-data" rel="external nofollow noopener" target="_blank">Blackwell B200 GPU</a></strong> is estimated to cost 30k - 40k USD.
    <ul>
      <li>For FP8<d-footnote>Recent NVIDIA hardware includes specialized “tensor cores” that can compute matrix multiplication on 8-bit floating point numbers really fast.</d-footnote>, it can achieve up to ~4500 TeraFLOPS<d-footnote>FLOPS means floating-point operations per second, which is a metric for roughly how fast a processor or algorithm is because most operations in deep learning are over floating point numbers.</d-footnote>, which is absolutely insane!</li>
      <li>It features 192GB of high-bandwidth memory / DRAM, which is the main GPU memory.</li>
    </ul>
  </li>
  <li>
<strong><a href="https://ai.meta.com/blog/meta-llama-3-1/" rel="external nofollow noopener" target="_blank">Llama 3.1 405B</a></strong>, Meta’s latest open-source language model is <strong>405B parameters</strong> (~800GB).
    <ul>
      <li>It was trained on a whopping <strong>16k NVIDIA H100s</strong> (sitting on their 24k GPU cluster)</li>
      <li>It’s training dataset was <strong>15 trillion tokens</strong>.</li>
    </ul>
  </li>
</ul>

<h2 id="part-i-the-beginning-1980s-2011">Part I. The Beginning (1980s-2011)</h2>
<p>The true beginning of deep learning is <a href="https://people.idsia.ch/~juergen/deep-learning-history.html" rel="external nofollow noopener" target="_blank">hotly contested</a>, but I, somewhat arbitrarily, thought it was best to begin with the first usage of backpropagation for deep learning: Yann Lecun’s CNN on a handwritten digits dataset in 1989<d-cite key="6795724"></d-cite>.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/1.png" style="width:50%" alt="Lecun's CNN">
    <figcaption><b>Figure 1.</b> Lecun’s original network (1989) for learning to classify digits. It is a simple convolutional network written in Lisp running on a backpropagation simulator.</figcaption>
</center>
</figure>

<p><strong><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="external nofollow noopener" target="_blank">Backpropagation Applied to Handwritten Zip Code Recognition</a> (Lecun, 1989<d-cite key="6795724"></d-cite>)</strong>. 
It is remarkable how simple this setup is: given a training dataset of 7291 normalized 16x16 images of handwritten digits, they train a 2-layer convolutional network with 12 5x5 learnable kernels, followed by a final projection to 10 logits. They train for <strong>23 epochs (~3 days)</strong>, and approximate the Hessian in Newton’s method to perform weight updates. Without an autodifferentiation engine, they had to write their own backpropagation simulator to compute the relevant derivatives. Finally, these experiments were run on a <a href="https://en.m.wikipedia.org/wiki/Sun-4" rel="external nofollow noopener" target="_blank">SUN-4/260</a> work station, which is a single-core machine running at <strong>16.67 MHz and 128MB of RAM</strong>.<d-footnote>For reference, a Macbook nowadays will have ~2-3 GHz and 16GB of RAM!</d-footnote></p>

<p>Andrej Karpathy has a <a href="https://iclr-blog-track.github.io/2022/03/26/lecun1989/" rel="external nofollow noopener" target="_blank">wonderful blog</a> that attempts to reproduce this paper on modern deep learning libraries with some extra numbers for reference:</p>
<ul>
  <li>The original model contains roughly <strong>9760 learnable parameters, 64K MACs</strong><d-footnote>MAC stands for multiplication-accumulate, which is a common metric for GPUs because they have fused multiply-and-adder instructions for common linear algebra operations</d-footnote>, and <strong>1K activations</strong> in one forward pass.</li>
  <li>On his Macbook M1 CPU, he trains a roughly equivalent setup in <strong>90 seconds</strong> — it goes to show how far the field has progressed!</li>
</ul>

<p>Some other notable works at the time were the <strong>Long Short-Term Memory (1997)<d-cite key="10.1162/neco.1997.9.8.1735"></d-cite></strong>, <strong>Deep Belief Networks (2006)<d-cite key="10.1162/neco.2006.18.7.1527"></d-cite></strong>, and <strong>Restricted Boltsmann Machines (2007)<d-cite key="10.1145/1273496.1273596"></d-cite></strong>, but I couldn’t really find the hardware, software library, or even programming language used to develop these methods (most likely Lisp / CUDA C++). Furthermore, these methods were more concerned with training stability (e.g. vanishing gradient problem<d-cite key="doi:10.1142/S0218488598000094"></d-cite>) and proving that these methods could converge on non-trivial tasks, so I can only assume “scale” was not really a concern here.</p>

<h3 id="i1-existing-fast-linear-algebra-methods">I.1. Existing Fast Linear Algebra Methods</h3>
<p>The introduction of the graphics processors in the late 20th century did not immediately accelerate progress in the deep learning community. While we know GPUs and other parallel processors as the primary workhorse of modern deep learning applications, they were originally designed for efficiently rendering polygons and textures in 3D games — for example, if you look at the design of the <a href="https://en.wikipedia.org/wiki/GeForce_256" rel="external nofollow noopener" target="_blank">NVIDIA GeForce 256 (1999)</a>, you’ll notice a distinct lack of modern components like shared memory<d-footnote>Not to be confused with shared memory in the OS setting, I think this naming convention is bad. Shared memory on an NVIDIA GPU is a low-latency cache / SRAM that can be accessed among threads in a threadblock. It is typically used to quickly communicate between threads.</d-footnote> and tensor cores that are critical for modern deep learning workloads.</p>

<p><strong>Programming a GPU in the 2000s.</strong> By this point the CUDA ecosystem had not matured, so the <a href="https://www.nextplatform.com/2015/10/28/inside-the-programming-evolution-of-gpu-computing/" rel="external nofollow noopener" target="_blank">common method for hacking GPUs</a> for general purpose applications was to configure <strong>DirectX</strong> or <strong>OpenGL</strong>, the popular graphics APIs at the time, to perform some rendering operation that involved say a matrix multiplication.<d-footnote>To corroborate the anecdote above, I had heard that this was true in a talk at Princeton given by Turing award winner Patrick Hanrahan.</d-footnote></p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/2.png" style="width:75%" alt="BLAS Primitives.">
    <figcaption><b>Figure 2.</b> A list of the different BLAS primitives. <a href="https://www.researchgate.net/figure/Some-operations-of-each-level-of-BLAS_tbl1_232641623" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Linear Algebra on a CPU.</strong> During this time, a suite of libraries had emerged in parallel for computing and solving common linear algebra paradigms like matrix multiplication, vector addition, dot products, etc. Many of these libraries used or were built off of the <strong>BLAS (Basic Linear Algebra Subprograms)</strong> specification with bindings for C and Fortran. BLAS divides its routines into three levels, mainly based on their runtime complexity (e.g. level 2 contains matrix-vector operations, which are quadratic with respect to the dimension). On CPUs, these libraries take advantage of <strong>SIMD / vectorization</strong><d-footnote>Modern CPUs allow for processing multiple elements with a single instruction, enabling a form of parallelization. Hardware components like vector registers (see https://cvw.cac.cornell.edu/vector/hardware/registers) also enable this behavior.</d-footnote>, <strong>smart caching</strong>, and <strong>multi-threading</strong> to maximize throughput. It is also pretty well known that MATLAB, NumPy, and SciPy were popular language / libraries used for these tasks, which essentially used BLAS primitives under the hood. Below were some commonly used libraries:</p>
<ol>
  <li>
<strong>LAPACK (1992)</strong>: The <strong>L</strong>inear <strong>A</strong>lgebra <strong>Pack</strong>age provides implementations of common linear algebra solvers like eigendecomposition and linear least squares.</li>
  <li>
<strong>Intel MKL (1994)</strong>: The Intel Math Kernel Library is a closed-source library for performing BLAS (now other) operations on x86 CPUs.</li>
  <li>
<strong>OpenBLAS (2011)</strong>: An open-source version of Intel MKL with similar, but worse, performance on most Intel instruction-set architectures (ISAs).</li>
  <li>
<strong>OpenCL (2009):</strong> An alternative to hacking in OpenGL, OpenCL was a device-agnostic library for performing computations in multiple processors. It was far more flexible for implementing primitives like matrix multiplication.</li>
</ol>

<p>Just for some reference numbers, I just ran a simple matrix multiplication experiment on my Macbook M2 Pro (12-core CPU, 3.5 GHz) with NumPy 1.26.4, which currently uses OpenBLAS under the hood. I found this <a href="https://salykova.github.io/matmul-cpu" rel="external nofollow noopener" target="_blank">blogpost by Aman Salykov</a> which does more extensive experimenting as well.</p>
<d-code block="" language="python" style="font-size:0.7em">
import numpy as np
import time

SZ = 2048
OPS = SZ * SZ * (2 * SZ - 1)
matrix_a = np.random.rand(SZ, SZ).astype(np.float32)
matrix_b = np.random.rand(SZ, SZ).astype(np.float32)

start_time = time.time()
result = np.dot(matrix_a, matrix_b)
end_time = time.time()

time_taken = end_time - start_time
print(f"Average of {(OPS / time_taken * (1e-9)):.4f} GLOPS")
</d-code>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Average of 361.4851 GFLOPS
</code></pre></div></div>

<h3 id="i2-compute-unified-device-architecture-cuda-2006">I.2. Compute Unified Device Architecture (CUDA), 2006</h3>
<p><em>I really like this <a href="https://fabiensanglard.net/cuda/" rel="external nofollow noopener" target="_blank">post by Fabien Sanglard</a>, which explains the history and motivating design patterns of CUDA and NVIDIA GPUs starting from the Tesla architecture over the years.</em></p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/3.png" style="width:90%" alt="BLAS Primitives.">
    <figcaption><b>Figure 3.</b> The CUDA ecosystem from device drivers to specific frameworks has been one of the major reasons behind NVIDIA's success in deep learning. <a href="https://blogs.nvidia.com/blog/what-is-cuda-2/" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>CUDA was originally designed to enable parallel programmers to work with GPUs without having to deal with graphics APIs. The release of CUDA also came with the release of the NVIDIA Tesla microarchitecture, featuring <strong>streaming multiprocessors (SMs)</strong>, which is the standard abstraction for “GPU cores” used today (this is super important for later!). I’m not an expert in GPU hardware design (actually I’m not an expert in anything for that matter), but the basic idea is that <strong>instead of having a lot of complicated hardware units performing specific vectorized tasks, we can divide up computation into general purpose cores (the SMs) that are instead SIMT (single-instruction multiple threads)</strong>. While this design change was meant for graphics programmers, it eventually made NVIDIA GPUs more flexible for generic scientific workloads.</p>

<p>Nowadays, <a href="https://en.wikipedia.org/wiki/CUDA#Programming_abilities" rel="external nofollow noopener" target="_blank">CUDA has evolved beyond just a C API to include several NVIDIA-supported libraries</a> for various workloads. Many recent changes target maximizing <strong>tensor core</strong> usage, which are specialized cores for fast <strong>generalized matrix multiplication (GEMM)</strong> in a single cycle. If what I’m saying makes no sense, don’t worry — I will talk more extensively about tensor cores and roughly how CUDA is used with NVIDIA GPUs in the next section.</p>

<p>Some notable libraries that I’ve used in practice are:</p>
<ul>
  <li>
<strong>cuBLAS</strong> (Introduced in <strong>CUDA 8.0</strong>): The CUDA API for BLAS primitives.</li>
  <li>
<strong>cuDNN</strong>: The CUDA API for standard deep learning operations (e.g. softmax, activation functions, convolutions, etc.).</li>
  <li>
<strong>CUTLASS</strong> (Introduced in <strong>CUDA 9.0</strong>): A template abstraction (<a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md" rel="external nofollow noopener" target="_blank">CuTe layouts</a>) for implementing GEMM for your own kernels — doesn’t have the large overhead of CuBLAS/CuDNN, which supports a wide variety of operations.</li>
  <li>
<strong>cuSPARSE</strong> (Introduced in <strong>CUDA 8.0</strong>): Efficient linear algebra operations on different kinds of sparse storage formats like <a href="https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_matrix.html#coordinate-coo" rel="external nofollow noopener" target="_blank">coordinate format (COO)</a> and <a href="https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_matrix.html#compressed-sparse-row-csr" rel="external nofollow noopener" target="_blank">compressed sparse row (CSR)</a>.</li>
</ul>

<h2 id="part-ii-oh-s-deep-learning-works-2012-2020">Part II: Oh s***— Deep learning works! (2012-2020)</h2>
<p><em>Although this section roughly covers the 2010s, many modern methods were derived from works during this time, so you may find some newer techniques mentioned in this section because it felt more natural.</em></p>

<p>While classical techniques in machine learning and statistics (e.g. <a href="https://www.ibm.com/topics/support-vector-machine#:~:text=What%20are%20SVMs%3F,in%20an%20N%2Ddimensional%20space." rel="external nofollow noopener" target="_blank">SVM</a>, <a href="https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30" rel="external nofollow noopener" target="_blank">boosting</a>, <a href="https://www.coursera.org/articles/decision-tree-machine-learning" rel="external nofollow noopener" target="_blank">tree-based methods</a>, <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c02_p25-46.pdf" rel="external nofollow noopener" target="_blank">kernel-based methods</a>) had been showing promise in a variety of fields such as data science, a lot of people initially did not believe in deep learning. There were definitely people working in the field by the <a href="https://www.reddit.com/r/MachineLearning/comments/hoo6m8/d_ml_oldtimers_when_did_deep_learning_really_take/?captcha=1" rel="external nofollow noopener" target="_blank">early 2010s</a>, but the pre-dominant experiments were considered more “proof-of-concept”. At the time, classical techniques in fields like computer vision (e.g. <a href="https://www.cs.princeton.edu/courses/archive/fall17/cos429/notes/cos429_fall2017_lecture4_interest_points.pdf" rel="external nofollow noopener" target="_blank">SIFT</a> features, <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos429/notes/cos429_f11_lecture03_filtering.pdf" rel="external nofollow noopener" target="_blank">edge detectors</a>) and machine translation were thought to be considerably better than any deep-learning methods. That is, <strong>until 2012, when team SuperVision dominated every other carefully crafted computer vision technique by an absurd margin</strong>.</p>

<h3 id="part-ii1-the-first-breakthrough-on-images">Part II.1: The first breakthrough on images!</h3>

<figure>
<center>
    <img src="/assets/img/efficient_dl/4.png" style="width:90%" alt="ImageNet.">
    <figcaption><b>Figure 4.</b>  Examples of images and annotations from ImageNet. <a href="https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf?ref=blog.roboflow.com" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://ieeexplore.ieee.org/document/5206848" rel="external nofollow noopener" target="_blank">ImageNet, 2009</a>.</strong> In 2009, the ImageNet dataset (shout-out to <strong>Prof. Kai Li, the co-PI, who is now my advisor at Princeton</strong>) was released as “the” canonical visual object recognition benchmark. The dataset itself included over <strong>14 million annotated images</strong> with <strong>&gt;20k unique classes</strong>, and represented the largest annotated image dataset to date. The following is a snippet of 2012 leaderboard for top-5 image classification, where the model is allowed 5 guesses for each image.</p>

<p><strong><a href="https://image-net.org/challenges/LSVRC/2012/results" rel="external nofollow noopener" target="_blank">ImageNet ILSVRC 2012 Leaderboard</a></strong> for classification, first and second place teams.</p>

<table>
  <tr>
    <th>Team</th>
    <th>Accuracy (top-5 predictions)</th>
  </tr>
  <tr>
    <td>SuperVision (<a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="external nofollow noopener" target="_blank">AlexNet</a>)</td>
    <td>84.69% <span style="color:green">▴</span>
</td>
  </tr>
  <tr>
    <td>ISI (<a href="https://ieeexplore.ieee.org/document/5995504" rel="external nofollow noopener" target="_blank">Fisher vectors</a>)</td>
    <td>73.83%</td>
  </tr>
</table>

<figure>
<center>
    <img src="/assets/img/efficient_dl/5.png" style="width:90%" alt="AlexNet.">
    <figcaption><b>Figure 5.</b> AlexNet was split in half in a model parallelism strategy to be able to fit the model in GPU memory (~3GB). <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" rel="external nofollow noopener" target="_blank">AlexNet</a> (Krizhevsky et al., 2012<d-cite key="10.5555/2999134.2999257"></d-cite>)</strong>. AlexNet was one of the first deep convolution networks to be successfully trained on a GPU. The model itself is tiny by today’s standards, but at the time it was far larger than anything that could be trained on a CPU. AlexNet was an <strong>8-layer, 60M parameter</strong> model trained on 2 <strong><a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-580.c270" rel="external nofollow noopener" target="_blank">GTX580 GPUs</a> with 3GB of RAM</strong> for ~5-6 days. It also featured some important design choices like <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="external nofollow noopener" target="_blank">ReLU</a> activations and <a href="https://arxiv.org/abs/1207.0580" rel="external nofollow noopener" target="_blank">dropout</a> that are still common in modern neural networks.</p>
<ul>
  <li>The original source code in CUDA C++ can be found on <a href="https://code.google.com/archive/p/cuda-convnet/" rel="external nofollow noopener" target="_blank">Google Code Archive</a>.</li>
  <li>I came across this <a href="https://github.com/albanie/convnet-burden" rel="external nofollow noopener" target="_blank">GitHub repository by user <code class="language-plaintext highlighter-rouge">albanie</code></a> that estimates the throughput of AlexNet’s forward pass to be <strong>~700 MFLOPS</strong>, but I’m not sure where they got this runtime estimate from or what hardware it was run on. Regardless, it is most likely an upper-bound for the actual performance.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/1202.2745" rel="external nofollow noopener" target="_blank">DanNet</a> (Cireşan, 2011<d-cite key="cireşan2012multicolumndeepneuralnetworks"></d-cite>)</strong>. DanNet was an earlier work by Dan Cireșan in Jürgen Schmidhuber’s lab that similarly implemented a deep convolutional network on GPUs to accelerate training on a variety of tasks. The method itself achieved <a href="https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html" rel="external nofollow noopener" target="_blank">great performance</a> on a variety of image-based benchmarks, but unfortunately the work is often overshadowed by AlexNet and its success on ImageNet.<d-footnote>I want to return to this paper because, while they don’t include the actual hardware used, they mention all the architectural components and dataset details to estimate the efficiency of their approach.</d-footnote></p>

<p><strong>Remark.</strong> Interestingly, I found from this <a href="https://sebastianraschka.com/faq/docs/first-cnn-gpu.html" rel="external nofollow noopener" target="_blank">Sebastian Raschka blog</a> that there were several other works that had adapted deep neural networks on GPUs. Nonetheless, none of these works had implemented a general-enough method to efficiently scale up the training of a convolutional neural network on the available hardware.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="part-ii2-deep-learning-frameworks-emerge">Part II.2: Deep learning frameworks emerge</h3>
<p>So it’s 2012, and Alex Krizhevsky, a GPU wizard, has proven that we can successfully use deep learning to blow out the competition on a serious task. As a community, the obvious next step is to build out the infrastructure for deep learning applications so <em>you don’t need to be a GPU wizard to use these tools</em>.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/6.png" style="width:70%" alt="DL Frameworks.">
    <figcaption><b>Figure 6.</b> The most popular deep learning frameworks as of 2024. <a href="https://www.askpython.com/python-modules/tensorflow-vs-pytorch-vs-jax" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/pdf/1211.5590" rel="external nofollow noopener" target="_blank">Theano</a> (2012)</strong><d-footnote>From what I’m aware of, this library came out earlier, but a lot of the core deep learning features did not come out until 2012.</d-footnote>. Theano was an open-source linear algebra compiler developed by the MILA group at Université de Montréal for Python, and it mainly handled optimizing symbolic tensor expressions under the hood. It also handled multi-GPU setups (e.g. data parallelism) without much effort, making it particularly useful for the new wave of deep learning. Personally, I found it quite unintuitive to use by itself, and nowadays it is used as a backend for Keras.</p>

<p><strong><a href="https://caffe.berkeleyvision.org/" rel="external nofollow noopener" target="_blank">Caffe</a> (2013)</strong>. Developed at UC Berkeley, Caffe was an older, high-performance library for developing neural networks in C/C++. Models are defined in configuration files, and the focus on performance allowed developers to easily deploy on low-cost machines like edge devices and mobile. Eventually, a lot of features in Caffe/Caffe2 were merged into PyTorch, and by this point it’s rarely directly used.</p>

<p><strong><a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1" rel="external nofollow noopener" target="_blank">TensorFlow v1</a> (2015)</strong>. Google’s deep learning library targeted Python applications, and felt far more flexible far dealing with the annoying quirks of tensors<d-footnote>Try dealing with tensors in C++ and you’ll quickly see what I mean.</d-footnote>. Like its predecessors, TensorFlow v1 also favored a “graph execution” workflow, meaning the developer had to define a computational graph of their models statically so it could be compiled for training / inference. For performance sake, this is obviously a good thing, but it also meant these frameworks were difficult to debug and hard to get used to.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" rel="external nofollow noopener" target="_blank">Torch</a> (2002) —&gt; <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a> (2016)</strong>. Torch was originally a linear algebra library for Lua, but eventually it evolved into an “eager execution”-based<d-footnote>The core idea behind eager execution is to execute the model code imperatively. This design paradigm makes the code a lot easier to debug and follow, and is far more “Pythonic” in nature, making it friendly for developers to quickly iterate on their models.</d-footnote> deep learning library for Python. PyTorch is maintained as an open-source software, and is arguably the most popular framework used in deep learning research. It used to be the case that you had to touch TorchScript to make PyTorch code production-level fast, but recent additions like torch.compile(), TorchServe, and ONNX<d-footnote>ONNX was a standard developed jointly by Meta and Microsoft to allow models to be cross-compatible with different frameworks. ONNX is now useful for converting your PyTorch models into other frameworks like Tensorflow for serving. </d-footnote> have made PyTorch more widely used in production code as well.</p>

<p><strong><a href="https://www.tensorflow.org/tutorials/quickstart/beginner" rel="external nofollow noopener" target="_blank">TensorFlow v2</a> (2019) &amp; <a href="https://keras.io/" rel="external nofollow noopener" target="_blank">Keras</a> (2015)</strong>. Keras was developed independently by François Chollet, and like PyTorch, it was designed to be intuitive for developers to define and train their models in a modular way. Eventually, Keras merged into TensorFlow, and TensorFlow 2 was released to enable eager execution development in TensorFlow. TensorFlow 2 has a lot of design differences than PyTorch, but I find it relatively easy to use one after you’ve learned the other.</p>

<p><strong><a href="https://jax.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">Jax</a> (2020)</strong>. Google’s latest deep learning framework that emphasizes its functional design and its <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation" rel="external nofollow noopener" target="_blank">just-in-time</a> (JIT) XLA compiler for automatically fusing operations (we’ll talk about this more in the GPU section). Jax is more analogous to an amped up NumPy with autodifferentiation features, but it also has support for standard deep learning applications through subsequent libraries like Flax and Haiku. Jax has been getting more popular recently and has, in my opinion, replaced TensorFlow as Google’s primary deep learning framework. Finally, Jax has been optimized heavily for Google’s Tensor Processing Units (TPUs), i.e. anyone using cloud TPUs should be using Jax.</p>

<p>By this point, we’ve set the stage for deep learning to flourish — frameworks are being developed to make research on deep learning far easier, so we can now move on to talking about the types of architectures people were interested in and the core research problems of the time.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="part-ii3-new-deep-learning-architectures-emerge">Part II.3: New deep learning architectures emerge</h3>
<p><em>Here is where the focus of the field begins to diverge into applying these networks to different domains. For the sake of brevity, I am going to assume the reader is familiar with all of these works, so I will very loosely gloss over what they are. <strong>Feel free to skip this section</strong>.</em></p>

<p><strong>Recurrent Networks (<a href="https://ai.stackexchange.com/questions/8190/where-can-i-find-the-original-paper-that-introduced-rnns" rel="external nofollow noopener" target="_blank">1980s - 1990s ish</a>)</strong>. Recurrent neural networks (RNNs) were popular at the nascent period of deep learning, with methods like <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" rel="external nofollow noopener" target="_blank">GRU</a> and <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="external nofollow noopener" target="_blank">LSTM</a> being used in many time-series and language tasks. Their sequential nature made them hard to scale on parallel processors, making them somewhat obscure for a long time after. More recently, recurrent networks have been re-popularized in the form of state-space models (SSMs) for linear dynamical systems. Early versions of these SSMs used the <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system" rel="external nofollow noopener" target="_blank">linear-time-invariance (LTI)</a> assumption to rewrite <a href="https://hazyresearch.stanford.edu/blog/2023-02-15-long-convs" rel="external nofollow noopener" target="_blank">sequential computations as a convolution</a> <d-cite key="gu2022efficientlymodelinglongsequences"></d-cite> at the cost of flexibility. Recent works<d-cite key="gu2024mambalineartimesequencemodeling"></d-cite> have removed these assumptions through efficient hardware implementations of critical algorithms like the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" rel="external nofollow noopener" target="_blank">Fast Fourier Transform</a>.</p>

<p><strong>Convolutional Neural Networks (CNN)</strong>. CNNs were there from the beginning, and they still remain popular in the computer vision domain. The main component is the convolutional layer, which contains learnable “kernels”<d-footnote>Kernel is an annoyingly overloaded term. In this case, it just means a small matrix that is convolved around an input.</d-footnote> that are applied through a convolution operation on an N-dimensional input. Convolutional layers are nice because the learned kernels are often somewhat interpretable, and they have built in invariants that work well for learning spatial structure.</p>

<p><strong>Graph Neural Networks.</strong> Graph neural networks are somewhat broad, but generally involve some parameterization of a graph using standard deep learning components like a linear weight matrix. They are very hard to implement efficiently on modern hardware (think how locality would be done) and can be very large and sparse. Even though most information can be represented as a graph, in practice there are only certain settings like social media graphs in recommendation systems and biochemistry where they have seen success.</p>

<p><strong>Deep Reinforcement Learning (DRL).</strong> DRL generally involved approximating value functions (e.g. <a href="https://arxiv.org/abs/1312.5602" rel="external nofollow noopener" target="_blank">DQN</a>) or policies (e.g. <a href="https://arxiv.org/abs/1707.06347" rel="external nofollow noopener" target="_blank">PPO</a>) from the RL setting, which were traditionally represented as some kind of discrete key-value map. The standard RL setting is a Markov Decision Process (MDP) with some kind of unknown reward. DRL has also extended to post-training large language models by re-framing the alignment problem as some kind of reward maximization problem. DRL has traditionally also been hard to make efficient because 1) existing algorithms do not respond well to blind scaling, 2) agents interacting with an environment is inherently not parallelizable, 3) the environment itself is a large bottleneck.</p>

<p><strong>Generative Adversarial Networks <a href="https://arxiv.org/abs/1406.2661" rel="external nofollow noopener" target="_blank">(Goodfellow et al., 2014)</a></strong>. Also <a href="https://arxiv.org/abs/1906.04493" rel="external nofollow noopener" target="_blank">hotly contested whether these actually came out in 2014</a>, but GANs were (a rather unstable) framework for training generative models. They had some nice theoretical guarantees (the input distribution is the optimal generator) but ultimately were hard to train, and they also were not great at high-resolution generations.</p>

<p><strong>Diffusion Models (<a href="https://arxiv.org/abs/1503.03585" rel="external nofollow noopener" target="_blank">Sohl-Dickstein et al., 2015</a> &amp; <a href="https://arxiv.org/abs/2011.13456" rel="external nofollow noopener" target="_blank">Song et al., 2020</a>)</strong>. I don’t have intuition as to why diffusion model generations turn out that much better than GANs, but from my own experience they definitely do. A lot of efficiency work in diffusion looks into reducing the number of noise/de-noising steps (which I find counterintuitive to how diffusion even works), and most parameterizations of diffusion models are just standard modules that are used in deep learning (e.g. MLPs, convolutions, transformers, etc.).</p>

<p><strong>Transformers <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">(Google, 2017)</a></strong>. The Transformer block (and variants of it) are widely used today, and a lot of work over the past 5 years has gone into optimizing each component of the Transformer. Some of the key bottlenecks to get around are 1) the quadratic time and memory complexity of the attention mechanism w.r.t sequence length, 2) the growing KV cache that eats up on-device memory, 3) making Transformer computations faster on existing hardware. We will see a lot of these problems come up in the rest of this post.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="part-ii4-efficient-convergence-inductive-biases-and-architectural-choices">Part II.4: Efficient convergence. Inductive biases and architectural choices</h3>
<p>A natural question any researcher has when first exploring a domain is whether the existing mechanisms and algorithms are optimal. It was known that without tricks like dropout, regularization, the correct activation functions, learning rate scheduler, inductive biases, etc. your model would diverge or overfit on your data. It is way too difficult to pinpoint all of the architectural design changes over the years, and in, for example, the large language space, many of these changes are sort of “open secrets” — many researchers and engineers at large labs are probably aware of these tricks (e.g. <a href="https://arxiv.org/abs/2004.05150" rel="external nofollow noopener" target="_blank">local attention</a>, <a href="https://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">RoPE</a> embeddings, <a href="https://arxiv.org/abs/2109.08668" rel="external nofollow noopener" target="_blank">ReLU^2</a>) but as a regular person like myself, it is hard to figure out these details from academic papers. This section will be dedicated to some cool changes that have emerged as empirically useful over the years.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h4 id="ii4a-inductive-biases-that-lead-to-better-convergence-behavior">II.4.a: Inductive biases that lead to better convergence behavior</h4>
<p>There are many tricks that have been known empirically to lead to better convergence behavior for a lot of models — it is known that many older models struggled to even converge! We still don’t have a rigorous understanding for why many of these tricks are useful, but in this section we list some important architecture changes that have led to better convergence behavior. It’s not always 100% clear why these tricks work so well, so I won’t justify here.</p>

<ul>
  <li>
<strong>Dropout (p)</strong>. During training, randomly mask out $p$. It is believed to be an implicit regularizer.</li>
  <li>
<strong>Residual connections</strong>. First introduced in <a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">ResNet</a>, add back the input to the output, effectively allowing data to skip layers.</li>
  <li>
<strong>Scaling depth/width</strong>. For convolutional networks, <a href="https://arxiv.org/abs/1905.11946" rel="external nofollow noopener" target="_blank">EfficientNet</a> showed scaling depth/width accordingly is useful.</li>
  <li>
<strong>Approximating constraints</strong>. Optimization over constrained spaces can be annoying. It turns out sometimes relaxing constraints, such as birth of the reinforcement learning algorithm <a href="https://arxiv.org/abs/1707.06347" rel="external nofollow noopener" target="_blank">PPO</a> as a relaxed and more widely-used version of <a href="https://arxiv.org/abs/1502.05477" rel="external nofollow noopener" target="_blank">TRPO</a>.</li>
  <li>
<strong>Cosine Learning Rate Scheduler (with Annealing)</strong>. In NLP settings, the cosine learning rate scheduler (with annealing) is widely used over other fixed and decaying learning rates.</li>
  <li>
<strong>Loss scaling</strong>. To prevent gradients from underflow or overflow (especially for quantization), a lot of optimizers have auto-tuned loss scaling enabled to normalize the gradients, then apply the inverse scaling factor.</li>
  <li>
<strong>ReLU and variants</strong>. For a lot of tasks, especially in NLP, ReLU and its smooth variants seem to work very well as activation functions.</li>
  <li>
<strong>Adam &amp; AdamW</strong>. These momentum-based optimizers have proven to be the most impactful in deep learning despite a lot of research being done in this field.</li>
  <li>
<strong>Attention</strong>. The most famous deep learning mechanism today, attention seems to work very well at interactions over sequential data.
    <ul>
      <li>
<strong>RoPE</strong>. <a href="https://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">Rotary embeddings</a> have similar properties to standard positional encodings, but can be written as matrix multiplications (which we love) and work better in a lot of settings.</li>
      <li>
<strong>ALiBi</strong>. Additive <a href="https://arxiv.org/abs/2108.12409" rel="external nofollow noopener" target="_blank">attention biases</a> have proven to work pretty well for length generalization.</li>
    </ul>
  </li>
  <li>
<strong>bfloat16</strong>. Low-precision training in general has shown to be practical and useful, and the <strong>bf16</strong> datatype, which trades of precision for a wider dynamic range than <strong>fp16</strong>, has shown to be more stable in deep learning training.</li>
  <li>
<strong>Mixture of Experts.</strong> It turns out we can keep scaling our models without all the parameters being active, and we still observe scaling laws.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h4 id="ii4b-searching-the-space-of-solutions-meta-optimization">II.4.b: Searching the space of solutions (meta-optimization)</h4>
<p>A lot of traditional machine learning techniques revolve around doing some kind of <a href="https://www.dremio.com/wiki/grid-search/" rel="external nofollow noopener" target="_blank">grid-search</a> and <a href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="external nofollow noopener" target="_blank">k-folds cross-validation</a> to find the best possible model. In modern deep learning, it’s <strong>very hard</strong> to do this, especially when a single <strong>training run can cost millions of dollars</strong>. One of the more interesting spaces is <strong>neural architecture search (NAS)</strong>, where we search a space of model configurations to find models that optimize some metric (e.g. performance, cost, speed) given some set of constraints. NAS isn’t really used in large model training, but it is extremely useful for trying to fit models onto low-cost devices — I’m not sure how much NAS has evolved since 2020, but I would highly recommend reading <a href="https://lilianweng.github.io/posts/2020-08-06-nas/" rel="external nofollow noopener" target="_blank">Lilian Weng’s blog on NAS</a>!</p>

<p><strong><a href="https://sakana.ai/evolutionary-model-merge/" rel="external nofollow noopener" target="_blank">Sakana AI’s Evolutionary Model Merge</a> (Sakana AI, 2024)</strong>. One of the newer works in NAS for language models is the evolutionary model merge algorithm, which takes components of already trained models and combines them to form various language and multi-modal foundation models. I haven’t played enough with these works to understand how effective they are, but they do demonstrate the ability to create unique models like a Japanese Math LLM with SOTA performance.</p>

<h2 id="part-ii5-efficient-convergence-optimizers">Part II.5: Efficient convergence. Optimizers</h2>
<p>Recently, I’ve gotten the sense that optimizers are largely overlooked by many people because <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" rel="external nofollow noopener" target="_blank">Adam</a> “just works”. From the perspective of efficiency, if we can 1) compute our optimizers faster, 2) reduce the memory load of stored statistics, and 3) converge faster, then these are all wins to consider. The standard gradient descent update is written as</p>

<p>
$$ \theta_{t+1} = \theta_{t} - \eta \nabla_{\theta} \mathcal{L}(\theta_t, x^{\mathcal{S}}, y^{\mathcal{S}}) $$
</p>

<p>where $t$ is the iteration, $\eta$ is the learning rate, $\theta$ is the model parameters, $\mathcal{L}$ is the loss function, and $\mathcal{S}$ is the set of training values to use in the update. In standard gradient descent (GD), $\mathcal{S}$ is the entire dataset, in <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="external nofollow noopener" target="_blank">stochastic gradient descent</a> (SGD) it is a randomly sampled $(x,y)$ pair, and in mini-batch gradient descent it is a randomly sampled subset. While GD has some <a href="https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf" rel="external nofollow noopener" target="_blank">nice and easy-to-prove theoretical guarantees</a><d-footnote>It is quite well known, but look up the proofs for convergence for GD, descent lemma, and even the related empirics surrounding the edge of stability.</d-footnote>, SGD has similar guarantees and is often used in practice because it converges faster and is easier to compute.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Momentum [<a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="external nofollow noopener" target="_blank">intro</a>]</strong>. Theoretical guarantees for SGD and GD require knowing the smoothness behavior of the loss function, which in practice is not known. In practice, SGD suffers from “steep” regions in the loss curve that cause oscillatory behavior, motivating the use of the descent trajectory as a prior to dampen oscillations. The canonical momentum update is (where $\gamma$ is a constant around $0.9$ according to (<a href="https://arxiv.org/abs/1609.04747" rel="external nofollow noopener" target="_blank">Ruder et al. 2016</a>)). The momentum version of SGD introduces a new term that depends on the gradient:</p>
<p><span>
<center>
$$
\begin{aligned}
v_{t} &amp;= \gamma v_{t-1} + \eta \nabla_{\theta} \mathcal{L}(\theta_t, x^{\mathcal{S}}, y^{\mathcal{S}})
\\
\theta_{t+1} &amp;= \theta_{t} - v_t
\end{aligned}
$$
</center>
</span></p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/1412.6980" rel="external nofollow noopener" target="_blank">Adam</a> (Kingma and Ba, 2014<d-cite key="kingma2017adammethodstochasticoptimization"></d-cite>)</strong>. It wasn’t mentioned, but <a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="external nofollow noopener" target="_blank">Adagrad</a> and <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="external nofollow noopener" target="_blank">RMSprop</a> introduced <strong>per-parameter adaptive learning rates</strong> and an <strong>exponentially decaying average of past gradients</strong>. Adam combines these ideas by storing first and second moment estimates of the gradients $g_t$, which is shown in the equations below.<d-footnote>The actual Adam also introduces a bias-correcting beta scheduler that modifies the first and second moment estimates slightly. They observed that because the estimates are zero-initialized, they are biased towards 0 if not normalized properly. Furthermore, a variant of Adam, called AdamW, also introduces iterative weight decay and is shown to work well in practice.</d-footnote></p>
<p><span>
<center>
$$
\begin{aligned}
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t
\\
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\\
\theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \hat{m}_t
\end{aligned}
$$
</center>
</span></p>

<p>From a memory perspective, storing these extra statistics per parameter implies at least an <strong>extra 2x the number of model parameters</strong> has to be stored in memory during training. For large models, this extra burden is extremely problematic, as we have to figure out 1) how to fit this either into <strong>one device’s memory or multiple device’s memory</strong>, and 2) if we are using multiple devices, how to <strong>move data around effectively</strong>. Adam/<a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" rel="external nofollow noopener" target="_blank">AdamW</a> is currently the standard for most large language model training as of 2024.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Preconditioning [<a href="https://www.mit.edu/~gfarina/2024/67220s24_L12_newton/L12.pdf" rel="external nofollow noopener" target="_blank">intro</a>]</strong>. Adam has remained the canonical optimizer for a long time, and most people are aware that it is a (stochastic) <a href="https://math.stackexchange.com/questions/2201384/what-is-the-definition-of-a-first-order-method" rel="external nofollow noopener" target="_blank">first-order optimizer</a>. The benefit of a first-order optimizer is that they are relatively quick and only store extra statistics that is linear in the number of learnable parameters. However, it would be a more accurate estimate to use the second, third, etc. order estimates of our <a href="https://math.stackexchange.com/questions/2957673/second-order-taylor-series-terms-in-gradient-descent" rel="external nofollow noopener" target="_blank">loss function Taylor expansion</a> to approximate the correct update. We motivated Adam based on per-coordinate scaling factors, which is basically just applying a diagonal preconditioner to the gradient! Optimizers like <a href="https://arxiv.org/abs/1802.09568" rel="external nofollow noopener" target="_blank">Shampoo</a> and <a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="external nofollow noopener" target="_blank">Adagrad</a> store preconditioners, but at varying levels of granularity (e.g. block diagonal vs. dense preconditioning matrix). On the <a href="https://arxiv.org/pdf/2306.07179" rel="external nofollow noopener" target="_blank">AlgoPerf</a> benchmark in particular, Shampoo has been shown to converge quicker than all pre-existing optimizers.</p>

<h2 id="part-ii6-pause-how-much-of-this-scale-is-really-necessary">Part II.6: Pause. How much of this scale is really necessary</h2>
<p>If you recall how <code class="language-plaintext highlighter-rouge">std::vector&lt;T&gt;</code> in the <a href="https://en.cppreference.com/w/cpp/standard_library" rel="external nofollow noopener" target="_blank">C++ standard library</a> is implemented under the hood, you’ll remember that we have a capacity that marks allocated memory, and a true array size that is the memory that is actually being “used”. This terrible analogy was thought of at 4am just to say that as we continue to scale, a natural question is whether each parameter in the model is really that important.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/7.png" style="width:90%" alt="LTH and Pruning.">
    <figcaption><b>Figure 7.</b> Iterative pruning cycle for deep learning models. Generally, we train, prune, then re-train while ensuring the model does not collapse <a href="https://towardsdatascience.com/saga-of-the-lottery-ticket-hypothesis-af30091f5cb" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<h3 id="ii6a-model-pruning">II.6.a: Model Pruning</h3>
<p><strong><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf" rel="external nofollow noopener" target="_blank">Learning both Weights and Connections for Efficient Neural Network</a> (Song et al., 2015<d-cite key="han2015learningweightsconnectionsefficient"></d-cite>)</strong>. One of the first successful pruning works in deep learning was done for convolutional models (e.g. <a href="https://arxiv.org/abs/1409.1556" rel="external nofollow noopener" target="_blank">VGG16</a>, LeNet, AlexNet) on ImageNet. The idea was to first train the models, then <strong>zero out weights below a certain norm threshold</strong>, then fine-tune the pruned model to completion. They motivate this simple strategy as an implicit regularizer for overfitting, and show <strong>~10x model compression rates while preserving 99% of the performance</strong>.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/pdf/1803.03635" rel="external nofollow noopener" target="_blank">The lottery ticket hypothesis</a> (Frankle et al., 2018<d-cite key="frankle2019lotterytickethypothesisfinding"></d-cite>)</strong>. The lottery ticket hypothesis (LTH) is a famous theory that states: for every dense, randomly initialized neural network, there <strong>exists a sparse subnetwork</strong> that accounts for a majority of the performance. In the original paper, they prune the lowest $N\%$ of weights after a certain number of training iterations and show on a variety of image tasks and architectures that performance is preserved. The LTH arguably <strong>popularized a lot of derivative works on finding metrics for identifying prunable weights</strong> in a network.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/1810.02340" rel="external nofollow noopener" target="_blank">SNIP</a> (Lee et al. 2018<d-cite key="lee2019snipsingleshotnetworkpruning"></d-cite>)</strong>. The LTH showed that post-training / during-training pruning was more effective than randomly pruning before training, so SNIP proposed a metric to prune “unimportant” weights before training. They first sample a random batch of data $D$ and compute the loss gradients $g_i = \frac{\partial \mathcal{L}(D, \theta)}{\partial \theta_i}$. Then, they compute</p>

<p>
$$
S(\theta_i) = \text{softmax}_{i}\left(|g_i(D, \theta)|\right)
$$
</p>

<p>This metric measures how sensitive each weight is to a loss, so they prune the smallest $S(\theta_i)$. The authors show that they can prune 99% of a network (LeNet) with a 1% increase in error.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2006.05467" rel="external nofollow noopener" target="_blank">SynFlow</a> (Tanaka et al. 2020<d-cite key="tanaka2020pruningneuralnetworksdata"></d-cite>)</strong>. The authors first generalize pruning-at-initialization metrics to what they call “synaptic saliency” as a Hadamard product:</p>

<p>
$$
S(\theta) = \frac{\partial \mathcal{R}}{\partial \theta} \odot \theta
$$
</p>

<p>SynFlow was one of the first works to consider pruning from the perspective of “network flow”, as opposed just aggressively pruning weights with a low metric score. They consider scenarios where an entire layer is pruned, leading to a completely redundant network. Their experiments are mainly image models on <a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="external nofollow noopener" target="_blank">CIFAR-10/100</a>, but they generally showcase good performance on extreme pruning ratios (on the order of $10^{-3}$).</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Are pruned models fast?</strong> From a hardware perspective, randomly pruning weights <em>does not provide a large speed-up</em> because operations like weight matrix multiplication rely on locality and targeting blocks of a matrix at one time. The standard implementation is to apply a $0$-mask to each pruned weight – which clearly provides no speed-ups – but clever implementations of pruning can target sparsity-aware kernels like in <a href="https://docs.nvidia.com/cuda/cusparse/" rel="external nofollow noopener" target="_blank">cuSPARSE</a> and <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/" rel="external nofollow noopener" target="_blank">CUTLASS</a><d-footnote>Check out some of the Han Song lectures like https://www.youtube.com/watch?v=sZzc6tAtTrM&amp;ab_channel=MITHANLab for more information on these topics.</d-footnote>. <a href="https://arxiv.org/pdf/2308.06767" rel="external nofollow noopener" target="_blank">Subsequent works</a> on pruning focus on particular architectures or ensuring hardware-aware speed-ups through structured pruning (e.g. <a href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/" rel="external nofollow noopener" target="_blank">2:4 pruning</a>). Honestly though, model pruning hasn’t seen that much production-success because 1) many companies can afford to use larger models and 2) pruning generally often is not hardware-friendly, i.e. a 50% pruned model is much slower than a model that is just 50% of the number of parameters.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="ii6b-embedding-pruning-or-hashing">II.6.b: Embedding Pruning or Hashing</h3>
<p><a href="https://www.nvidia.com/en-us/glossary/recommendation-system/" rel="external nofollow noopener" target="_blank">Recommendation systems</a> is a practical field where “pruning” is somewhat applicable. In recommendation systems, users and items are typically <strong>represented as an ID that maps to a $O(10)$-dimensional embedding</strong>, meaning for social media companies like Meta and Snapchat, they will have on the <strong>order of millions or billions</strong> of embeddings in their models. For some napkin calculations, a full-precision 1B parameter embedding table with 64-dimensions each is 2 bytes * 64 * 10^9 = 128 GB for the embedding table, which is actually small in production settings! Without going into too much detail about the models themselves (for now, just abstract them as some kind of large transformer model), the <strong>embedding tables take up more than 90% of the memory</strong> load of learnable parameters.</p>

<p>Intuitively, under a <a href="https://princeton-introml.github.io/files/ch20.pdf" rel="external nofollow noopener" target="_blank">vector space</a> and with some assumptions about <a href="https://mbernste.github.io/posts/normed_vector_space/" rel="external nofollow noopener" target="_blank">constraining the norm</a> of each embedding, it is easy to see that we can probably <a href="https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061" rel="external nofollow noopener" target="_blank">cluster these embeddings</a> in some meaningful way, and map multiple IDs to the same embedding without incurring much error. Many ideas in RecSys are not shared publicly, but common techniques like <a href="https://arxiv.org/pdf/2007.14523" rel="external nofollow noopener" target="_blank">double hashing</a> and <a href="https://dl.acm.org/doi/10.5555/645925.671516" rel="external nofollow noopener" target="_blank">locality-sensitive hashing</a> are used in practice.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/8.png" style="width:60%" alt="DHE.">
    <figcaption><b>Figure 8.</b> Using an explicit or implicit hash function (DHE shown here) is often used in practice to reduce the memory requirements of huge embedding tables. <a href="https://arxiv.org/pdf/2010.10784" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Learning to Embed Categorical Features without Embedding Tables for Recommendation (Kang et al. 2020<d-cite key="kang2021learningembedcategoricalfeatures"></d-cite>)</strong>. Deep Hash Embedding (DHE) is a technique to replace an embedding table with a smaller, learnable transformation (e.g. a neural network). In other words, the hashing function is also implicitly learned alongside the embeddings themselves. Surprisingly, computing embeddings on the fly is pretty effective, but the unclear part for me is whether the values of the IDs have some implicit biasing effect on the embeddings produced.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="ii6c-quantization">II.6.c: Quantization</h3>
<figure>
<center>
    <img src="/assets/img/efficient_dl/9.png" style="width:90%" alt="Quantization.">
    <figcaption><b>Figure 9.</b> Quantization schemes involving working at a lower precision (e.g. 16-bit floating point, 8-bit integer) than the standard 32-bit floating point (FP32). <a href="https://medium.com/@lmpo/understanding-model-quantization-for-llms-1573490d44ad" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>Quantization basically means instead of storing and using <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" rel="external nofollow noopener" target="_blank">32-bit floating point values (full-precision)</a>, we can use maybe 16-bit (half-precision), or 8-bit, etc. Doing so reduces the memory footprint of the model, but at the cost of precision. But there are actually many considerations, like whether you want to quantize during training or after training, whether you want to maintain model computations in a lower precision, and how to handle gradients in <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" rel="external nofollow noopener" target="_blank">mixed-precision</a> models.</p>

<p>The concept of quantization is not specific to deep learning and is more of a data compression problem. Generally, we are interested in reducing the memory footprint of our models — i.e. if we quantize a model with FP32 parameters to INT8<d-footnote>FP32 means 32-bit floating point and INT8 means 8-bit integers. We will talk about this a little bit, but they are represented differently in memory. So even INT32 is quite different than FP32.</d-footnote>, we reduce the model size by 4x. However, as we will see later, modern hardware like the <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf" rel="external nofollow noopener" target="_blank">NVIDIA A100</a> introduces specialized instructions to significantly speed up lower precision operations. For now, we introduce some basic quantization strategies for transforming a value $X$.</p>

<p><strong><a href="https://aman.ai/primers/ai/quantization/#absmax-absolute-maximum-quantization" rel="external nofollow noopener" target="_blank">Absmax quantization</a> to INT{b}</strong> will map to the closed interval $[- \max |X|, \max |X|]$ then evenly divide up the interval into $2^b - 1$ sections. Each value will get mapped to its closest point in the above mapping, and the quantization range is clearly symmetric.</p>

<p>
$$
X_{int_b} = \text{round}\left(\frac{2^{b-1} - 1}{\max |X|} \cdot X\right)
$$
</p>

<p><strong><a href="https://aman.ai/primers/ai/quantization/#zero-point-quantization" rel="external nofollow noopener" target="_blank">Zero-point quantization</a> to INT{b}</strong> instead will map to the range $[- \min |X|, \max |X|]$, but again will still uniformly divide the interval.</p>

<p><span>
<center>
$$
\begin{aligned}
z &amp;= \text{round}\left(-\frac{2^b - 1}{\max |X| - \min |X|} \cdot \min |X|\right)  - 2^{b-1}
\\
X_{int_b} &amp;= \text{round}\left(\frac{2^b - 1}{\max |X| - \min |X|} \cdot X + z\right)
\end{aligned}
$$
</center>
</span></p>

<p>The danger of the above methods is the presence of outliers, which cause most quantization bins to be unused while increasing the quantization error. There are many other forms of quantization that do not have to be uniform in any way. <a href="https://speechprocessingbook.aalto.fi/Modelling/Vector_quantization_VQ.html" rel="external nofollow noopener" target="_blank">Codebook quantization</a>, for example, maps pre-defined values to a smaller set of pre-defined values, and the behavior of this map just has to be injective and well-defined<d-footnote>I won’t be going into too much depth about the details of quantizing because it’s not that interesting. I also think that it’s better explained visually. I would recommend reading https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</d-footnote>. The above methods quantize to some integer representation because it is quite intuitive, but quantizing from say FP32 to FP16 is not as obvious because these representations do not uniformly divide the range they represent.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://steve.hollasch.net/cgindex/coding/ieeefloat.html" rel="external nofollow noopener" target="_blank">IEEE754 and floating point representations</a></strong>. Integers are represented with the two’s complement and are evenly spaced; however, floating point values are not evenly spaced fractions. Instead, the IEEE754 standard uses one bit to determine sign, some of the bits as exponents, and some of them as fractions (called the mantissa), giving us<d-footnote>There are also special representations for values like $0$ of $\infty$, but honestly it’s not that important for us to understand.</d-footnote>:</p>

<p>
$$
X_{fp32} = s^{(1)} \cdot  1.m^{(23)} \cdot 2^{b^{(8)}}
$$
</p>

<p>I’ve used the superscript to denote the number of bits used to represent that number. To clarify, the mantissa is the decimal part of the number $1.x$. In other words, in FP32, we have $23$ mantissa bits and $8$ exponent bits. However, other representations also exist to modify the representable range (increase exponent bits) or the precision (increase mantissa bits), which can be beneficial in deep learning applications.</p>
<ul>
  <li>
<strong><a href="https://arxiv.org/pdf/1905.12322" rel="external nofollow noopener" target="_blank">BF16</a></strong>. The IEEE754 standard for FP16 uses 5 exponent bits and 8 mantissa bits. It was discovered by the Google Brain team, however, that using 8 exponent bits, which has the <strong>same dynamic range as FP32</strong>, was more stable than FP16 due to large gradients in LLM training. Furthermore, BF16 has the benefit of being able to <strong>easily convert to and from FP32</strong> — just chop the last 16 bits of the mantissa!</li>
  <li>
<strong>FP8</strong>. NVIDIA’s newest <a href="https://www.nvidia.com/en-us/data-center/h100/" rel="external nofollow noopener" target="_blank">H100s have Tensor Core support for 8-bit floating points</a>, which are represented as either E5M2 or E4M3<d-footnote>E5M2 meaning 5 exponent bits and 2 mantissa bits, and E4M3 meaning 4 exponent bits and 2 mantissa bits. Notice that with 8 bits, we can never do the FP32 → BF16 trick, but we can go from FP16 to E5M2.</d-footnote>. We will talk more about Tensor Cores soon.</li>
</ul>

<p><strong>Automatic Mixed-Precision training (2018)</strong>. In 2018, NVIDIA released the <a href="https://github.com/NVIDIA/apex" rel="external nofollow noopener" target="_blank">Apex extension</a> to PyTorch, which introduced <a href="https://pytorch.org/docs/stable/amp.html" rel="external nofollow noopener" target="_blank">automatic mixed-precision (AMP)</a> training on CUDA devices. The core idea is that lower precision BLAS operations are significantly faster with the introduction of hardware units like NVIDIA Tensor Cores, but not all operations (e.g. logarithms, trig functions) are <a href="https://residentmario.github.io/pytorch-training-performance-guide/mixed-precision.html#how-pytorch-automatic-mixed-precision-works" rel="external nofollow noopener" target="_blank">safe to downcast</a> due to their sensitivity to dynamic ranges / precision. Under the hood, <a href="https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float16" rel="external nofollow noopener" target="_blank">torch.amp has a list of “safe” operations that are downcast to FP16/BF16</a> to provide essentially free speed-ups to the programmer. In most modern training schemes, <strong>you should be using AMP</strong> unless you want full control over your model operations.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="ii6d-the-grandfather-of-efficient-ml-and-tinyml">II.6.d: The Grandfather of Efficient ML and TinyML</h3>
<figure>
<center>
    <img src="/assets/img/efficient_dl/10.png" style="width:90%" alt="Deep compression.">
    <figcaption><b>Figure 10.</b> Deep Compression multi-stage memory reduction scheme, combining most well known methods of model compression at the time (pruning, quantization, compressing codes) to produce an extremely efficient network. <a href="https://arxiv.org/pdf/1510.00149" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/1510.00149" rel="external nofollow noopener" target="_blank">Deep Compression</a>: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding (Han et al. 2015<d-cite key="han2016deepcompressioncompressingdeep"></d-cite>)</strong>. Arguably the most influential work in efficient deep learning for its time, this paper showed that <strong>combining simple magnitude-based weight pruning and codebook quantization was sufficient</strong> for cutting down existing images models like VGG-16 by <strong>~50x</strong> while barely affecting model performance, enabling them to <strong>fit into on-chip SRAM</strong>. I wish there was some more analysis on the properties of these extremely compressed models and how this relates to the data distribution the model was trained on, because we do not see these levels of compression in modern LLMs.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h2 id="part-iix-hardware">Part II.x: Hardware</h2>
<p><em>If you don’t know anything about a GPU except that it’s really good at parallel workloads, this section is a gold mine of information! I think this section motivates a lot of the future work very well, especially as we begin to consider hardware-aware algorithms for scaling our models. Otherwise, I would skip ahead to <a href="#part-iii-the-era-of-scale-till-we-fail-2020-now">Chapter III</a>.</em></p>

<p>You’ve probably noticed that up until this point, a lot of the aforementioned works were interested in improving and tweaking architectural components for the sake of better convergence behavior and ease of scaling. If your model doesn’t fit on a single GPU, find a way to divvy it up on multiple GPUs — we can sort of ignore optimizing for memory accesses, node-to-node latency, and other systems-y lingo because at this scale, it’s probably fast enough<d-footnote>This isn’t entirely true, of course. There were definitely people who cared about these kinds of “engineering” problems, but AI wasn’t really a “product” yet. It was cool, and it had a growing list of applications, but nothing at the scale of a Google search engine or a streaming platform. </d-footnote>. But as the field began to mature, people began thinking more about <strong>hardware-aware algorithms</strong> and how to utilize a lot of the new features offered by the CUDA ecosystem and NVIDIA GPUs. We focus a lot on CUDA because of its strong support in most deep learning applications, but also recognize and discuss other alternatives.</p>

<h3 id="iix1-nvidia-gpus-from-tesla-2006-to-ampere-2020">II.x.1: NVIDIA GPUs from Tesla (2006) to Ampere (2020)</h3>
<figure>
<center>
    <img src="/assets/img/efficient_dl/11.png" style="width:80%" alt="CUDA.">
    <figcaption><b>Figure 11.</b> A comparison of GPU throughput on INT8 tasks over the past 10 years. <a href="https://www.linkedin.com/posts/haiyongw_the-1000-times-increase-in-nvidia-gpu-performance-activity-7110666027297869826-LIMv" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>Let’s continue where we <a href="#i2-compute-unified-device-architecture-cuda-2006">left off</a>. A lot of this section will appear kind of hand-wavy, but don’t worry — it makes a lot of sense to just assume things are a certain way before digging into why. If you ever become interested in the why, you’ll have to start reading denser sources of information. I recommend the <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0124159923" rel="external nofollow noopener" target="_blank">PPMP textbook</a> and the <a href="https://discord.com/invite/Wu4pdW8QqM" rel="external nofollow noopener" target="_blank">GPU MODE Discord</a>!</p>

<p><strong>Compute Structure.</strong> Let’s first talk about why GPUs are so <strong>good at parallelized computations</strong>. CPUs were designed to handle very complicated logic like <a href="https://blog.cloudflare.com/branch-predictor/" rel="external nofollow noopener" target="_blank">branching (think if-else operations)</a>, and a large portion of the <a href="https://superuser.com/questions/324284/what-is-meant-by-the-terms-cpu-core-die-and-package" rel="external nofollow noopener" target="_blank">processor die</a> is dedicated to this. NVIDIA GPUs instead <a href="https://superuser.com/questions/324284/what-is-meant-by-the-terms-cpu-core-die-and-package" rel="external nofollow noopener" target="_blank">trade off this chip space for more cores and specific hardware units</a> that can perform instructions like small matrix multiplications in very few cycles. It’s like having 100 automatic sewing robots (GPU) vs. a human (CPU). Sure, the human being is smarter and more flexible/capable for general tasks, but if the task is to maximize production of clothes, it is much more useful to have the sewing robots. Starting from the <a href="https://en.wikipedia.org/wiki/Nvidia_Tesla" rel="external nofollow noopener" target="_blank">Tesla series GPUs</a>, NVIDIA used many CUDA cores with the <a href="https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/simt_warp" rel="external nofollow noopener" target="_blank">SIMT (single-instruction, multiple threads)</a> abstraction, so effectively a GPU really was just a bunch of small processors running in parallel. To understand how this abstraction works together with actual workloads, however, we need to understand how data is moved from the memory to the actual processors.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/12.png" style="width:70%" alt="CUDA Memory.">
    <figcaption><b>Figure 12.</b> Simplified structure of NVIDIA GPU memory hierarchy. <a href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Hierarchical Memory Structure.</strong> The above is an extremely simplified view of how compute and memory are divided in a GPU starting from the Tesla architecture. Let’s <strong>assume</strong> that performing some kind of memory access from global memory (DRAM) is slow. This design emphasizes data reuse to minimize access to global memory. From <strong>Figure 12</strong>, we can also observe that different hierarchies of memory are shared across different abstractions (e.g. L2 cache is shared among SMs, but L1 cache is per SM), which is extremely important for optimization.</p>

<ul>
  <li>
<strong>SMs</strong> (<a href="https://fabiensanglard.net/cuda/" rel="external nofollow noopener" target="_blank">streaming multiprocessors</a>) are the individual units that run their own processes<d-footnote>This is not entirely true. SMs actually have their own CUDA cores / streaming processors that get assigned the relevant work, but for our abstraction it suffices not to think about them.</d-footnote>, and you generally have on the order of $O(100)$ of these. For now, <strong>assume</strong> that they can run many threads (up to 1024) at the same time.
    <ul>
      <li>Each SM has its own <a href="https://carpentries-incubator.github.io/lesson-gpu-programming/global_local_memory.html" rel="external nofollow noopener" target="_blank">registers</a> (256K per SM on an A100), which are the fastest form of memory to access and write to.</li>
    </ul>
  </li>
  <li>
<strong>L1</strong> and <strong>L2 caches</strong> are a form of fast (roughly 10x faster than DRAM) but small memory — just assume for now that they are a limited but extremely valuable resource.</li>
  <li>
<strong><a href="https://www.tomshardware.com/news/glossary-dram-ram-graphics-cards-gddr-definition,38002.html" rel="external nofollow noopener" target="_blank">DRAM</a></strong> (dynamic random access memory) is the main working memory on a GPU. When you hear the term “A100 40GB”, it means that you are dealing with an A100 GPU with 40GB of DRAM. It is also often labelled as “high-bandwidth memory” (HBM).</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/13.png" style="width:70%" alt="CUDA compute hierarchy.">
    <figcaption><b>Figure 13.</b> Parallel compute hierarchy for modern CUDA devices. <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Streaming Multiprocessors (SMs), Thread Blocks, Warps.</strong> The CUDA programming model is a bit convoluted at first glance, and it’s hard to motivate the design choices without understanding the hardware. Generally, the most important thing to understand is that:</p>
<ol>
  <li>Kernel/device functions operate at the <strong>thread-level</strong>, so we have to specify per-thread behavior in our device functions. Variables defined are implicitly accessed through registers.</li>
  <li>We mentioned earlier that CUDA is SIMT — <strong>groups of threads called warps</strong> <strong>share the same instruction</strong> over different data (typically 32 threads per warp). Starting from the <a href="https://en.wikipedia.org/wiki/Volta_(microarchitecture)" rel="external nofollow noopener" target="_blank">Volta architecture</a>, threads actually have their own <a href="https://personal.utdallas.edu/~dodge/EE2310/lec13.pdf" rel="external nofollow noopener" target="_blank">program counter</a> and <a href="https://www.youtube.com/watch?v=jVzSBkbfdiw&amp;ab_channel=JacobSorber" rel="external nofollow noopener" target="_blank">call stack</a> and can call different instructions.</li>
  <li>Kernels are launched in “grids” of “<a href="https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)" rel="external nofollow noopener" target="_blank">thread blocks</a>”; threads/warps in the <strong>same block can access <a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/" rel="external nofollow noopener" target="_blank">shared fast SRAM memory</a></strong>, which is useful for communicating between threads in operations like <a href="https://www.mathworks.com/help/parallel-computing/stencil-operations-on-a-gpu.html" rel="external nofollow noopener" target="_blank">stencils</a> / <a href="https://ulhpc-tutorials.readthedocs.io/en/latest/cuda/exercises/convolution/" rel="external nofollow noopener" target="_blank">convolutions</a>.</li>
  <li>Each <strong>grid is independent</strong> (and run in parallel), and generally cannot communicate. For example, it is often convenient to launch an independent grid for each batch in the forward pass of a network.</li>
  <li>We <em>usually</em> <strong>launch kernels from the CPU/host</strong>. In PyTorch, it is implicit when we define our model code; in CUDA, it is using the triple bracket notation: <code class="language-plaintext highlighter-rouge">f&lt;&lt;&lt;&lt;a,b&gt;&gt;&gt;&gt;(**kwargs)</code>, where <code class="language-plaintext highlighter-rouge">a</code> is the number of grids, and <code class="language-plaintext highlighter-rouge">b</code> is the number of thread blocks per grid. The hardware is responsible for scheduling these threads on the relevant devices to maximize device usage, or “<a href="https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.htm" rel="external nofollow noopener" target="_blank">occupancy</a>”.</li>
</ol>

<p>An example template of launching a CUDA kernel from the host is below.</p>

<d-code block="" language="python" style="font-size:0.7em">
__global__ void func(float *a, float *b) {
  // Thread ID
  int x = blockIdx.x * blockDim.x + threadIdx.x;
	...
}

int main(int argc, char* argv[]) {
	float* a, b; // with cudaMalloc, these are device pointers.
	// Example of launching a GPU kernel from the CPU.
	func&lt;&lt;&lt;blk_in_grid, thr_per_blk&gt;&gt;&gt;(a, b);
}
</d-code>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Compute-bound vs. Memory-bound workloads.</strong> In parallel GPU workloads, we are concerned with bottlenecks that limit the throughput of the entire workload. At a high-level, this can either be due to our cores being fed lots of operations, or due to blocking operations from data movement in memory. In most language-based deep learning applications, the latter occurs, and we call these programs “<a href="https://nanxiao.gitbooks.io/cuda-little-book/content/posts/compute-bound-and-memory-bound-kernels.html" rel="external nofollow noopener" target="_blank">memory-bound</a>”. Note that being compute-bound on an A100 does not imply that you will reach the <strong>~300 TFLOPS advertised by the A100</strong> — certain compute operations like <a href="https://dublog.net/blog/all-the-activations/" rel="external nofollow noopener" target="_blank">activation functions are slower</a>, as we will soon see. We often estimate these bottlenecks by computing the <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html" rel="external nofollow noopener" target="_blank">arithmetic intensity</a>, which is the number of compute operations divided by the bytes accessed in memory. Finally, the CUDA ecosystem features a variety of profilers for developers to use to understand their programs, which we list in the <a href="#PROFILE">Resources section</a>.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/14.png" style="width:70%" alt="CUDA compute hierarchy.">
    <figcaption><b>Figure 14.</b> Comparison of tensor core operations vs. non-tensor operations on A100 and H100 GPUs. <a href="https://www.cudocompute.com/blog/comparative-analysis-of-nvidia-a100-vs-h100-gpus" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<blockquote>
  <p><strong>GPUs on Steroids: Tensor Cores (2017).</strong> If there was any concern about whether <a href="https://www.intel.com/content/dam/develop/external/us/en/documents/31848-compilerautovectorizationguide.pdf" rel="external nofollow noopener" target="_blank">vectorized CPU operations</a> could compete with GPUs, you can throw that all out the window due to the introduction of <strong>Tensor Cores</strong> with the release of the Volta microarchitecture in 2017. Tensor cores are <strong>specialized hardware units for performing 4x4 floating point matrix multiplications</strong> extremely fast<d-footnote>Certain smaller precision data types like FP16 and FP8 are faster on later editions of Tensor Cores.</d-footnote>. Because matrix multiplication can be re-written as block matrix multiplication and deep learning consists of a small set of operations, Tensor Cores are extremely useful, and optimizing throughput often comes down to <strong>sufficiently feeding the Tensor Cores</strong>. See <strong>Figure 14</strong> for a comparison of Tensor Core speed on A100/H100 GPUs.</p>
</blockquote>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Intra-Device Bandwidth: PCIe vs. SXM &amp; NVLink</strong>. When dealing with larger workloads, another bottleneck to consider is device-device and host-device communication bottlenecks. The standard interface is <a href="https://en.wikipedia.org/wiki/PCI_Express" rel="external nofollow noopener" target="_blank">Peripheral Component Interconnect Express (PCIe)</a>, which can be used to connect devices to other devices or to the host. PCIe lanes connect your devices, and a <strong>larger number of lines provides more (potential) throughput for data movement</strong>. Starting from the <a href="https://en.wikipedia.org/wiki/Pascal_(microarchitecture)" rel="external nofollow noopener" target="_blank">Pascal microarchitecture</a>, NVIDIA also began selling GPUs with the <a href="https://www.arccompute.io/arc-blog/nvidia-h100-pcie-vs-sxm5-form-factors-which-gpu-is-right-for-your-company" rel="external nofollow noopener" target="_blank">SXM form factor</a>, which basically means they have specific ports for SXM interconnects and are connected on a specific SXM board (it still communicates to the CPU through PCIe). The SXM GPUs can also use NVLink, which is a special protocol for larger memory bandwidth. Generally, unless you are dealing with huge workloads, the type of intra-device communication will not even be the bottleneck you are looking for. For example, the H100 PCIe device-to-device bandwidth is 2 TB/s, while the H100 SXM5 device-to-device bandwidth is 3.35 TB/s.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Other relevant optimization details you can just assume exist.</strong> Understanding how to use these often involves profiling kernels and balancing the limited amount of “fast” memory we have. Many of these optimizations are highlighted in this amazing article on optimizing matrix multiplication in raw CUDA: <a href="https://siboehm.com/articles/22/CUDA-MMM" rel="external nofollow noopener" target="_blank">https://siboehm.com/articles/22/CUDA-MMM</a>. Because our GPU compilers aren’t unbeatable, it is useful to know many of the following details:</p>

<ul>
  <li>
<strong>Shared memory:</strong> I didn’t mention this explicitly in the memory hierarchy above, but shared memory<d-footnote>Not to be confused with OS shared memory. The naming here is kind of confusing I’ll admit…</d-footnote> is SRAM that is shared between all threads in a thread block. Many tricks involve using shared memory accesses over HBM accesses.</li>
  <li>
<strong>Thread coarsening</strong>: There is overhead for launching threads (it’s not free!) so sometimes it’s actually better to perform sequential operations on the same thread.</li>
  <li>
<strong>Memory coalescing:</strong> When we access HBM/DRAM, it is faster to access them in “bursts”, or contiguous chunks. In other words, we like structured accesses.</li>
  <li>
<strong>Constant memory:</strong> A small, global read-only memory that is useful when we have to re-use the same data a lot.</li>
  <li>
<strong>Pinned memory:</strong> When transferring between CPU RAM and GPU DRAM, NVIDIA GPUs have a <a href="https://stackoverflow.com/questions/5736968/why-is-cuda-pinned-memory-so-fast" rel="external nofollow noopener" target="_blank">Direct Memory Access (DMA)</a> unit that handles the memory transfer to free up compute. Because the DMA uses physical addresses, the OS paging system can accidentally cause the DMA to transfer the wrong CPU memory — pinned memory is a primitive to ensure a chunk of memory will not be paged out, giving up speed-ups on this transfer.</li>
  <li>
<strong>Streams:</strong> We can avoid “waiting” sequentially for independent blocking operations by telling the device to put them on different streams, so it is safe to run them concurrently.</li>
</ul>

<p><strong>Parallel patterns.</strong> It is also important to understand what types of operations are known to be parallelizable. In deep learning, we understand that matrix multiplications (matmuls) are extremely efficient, but many other operations are also parallelizable and have well-known design patterns:</p>
<ul>
  <li>All BLAS operations</li>
  <li>Convolutions</li>
  <li>Stencil operations</li>
  <li>Reductions (e.g. <code class="language-plaintext highlighter-rouge">torch.sum()</code>)</li>
  <li>Radix Sort</li>
  <li>Merge (e.g. <a href="https://lumetta.web.engr.illinois.edu/408-S20/slide-copies/ece408-lecture16-S20.pdf" rel="external nofollow noopener" target="_blank">Kogge-Stone and Brent-Kung</a>, useful for state-space models)</li>
  <li>Histograms</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p>A comparison of notable GPU specs over the years. We’ll be using PCIe and not SXM numbers for reference here.</p>

<table>
  <tr>
    <th>GPU</th>
    <th>$\mu$-arch</th>
    <th>Year Introduced</th>
    <th>Peak Theoretical TFLOPS</th>
    <th>Peak Theoretical Bandwidth (GB/s)</th>
    <th>Notable inclusion.</th>
  </tr>
  <tr>
    <td>GTX 580 3GB	</td>
    <td>Fermi	</td>
    <td>2010	</td>
    <td>1.58	</td>
    <td>192	</td>
    <td>Used to train AlexNet (2x).</td>
  </tr>
  <tr>
    <td>Tesla P100 16GB	</td>
    <td>Pascal	</td>
    <td>2016	</td>
    <td>21.2	</td>
    <td>732	</td>
    <td>First datacenter GPU.</td>
  </tr>
  <tr>
    <td>V100 16GB	</td>
    <td>Volta	</td>
    <td> 2017	</td>
    <td>28.3 (FP16)	</td>
    <td>897	</td>
    <td>Introduced Tensor Cores.</td>
  </tr>
  <tr>
    <td>RTX 3090 24GB	</td>
    <td>Ampere	</td>
    <td>2020	</td>
    <td>35.6	</td>
    <td>936	</td>
    <td>Popular consumer GPU for deep learning with a lot of VRAM.</td>
  </tr>
  <tr>
    <td>A100 80GB	</td>
    <td>Ampere</td>
    <td>2020	</td>
    <td>312	</td>
    <td>1935	</td>
    <td>Huge DRAM pool and very popular choice for clusters.</td>
  </tr>
  <tr>
    <td>H100 80GB	</td>
    <td>Hopper</td>
    <td>2022	</td>
    <td>1600 (FP8)	</td>
    <td>2040	</td>
    <td>Introduced new components like the TMA for accelerating LLM inference and training.</td>
  </tr>
</table>

<p><strong>Energy costs.</strong> The power consumption of these devices is pretty important to know if you are using your own machines / clusters. I don’t have a strong intuition for these numbers, but generally they float around the $O(100)$ watts range for current high-end GPUs. For example, the A100 80GB consumes 250W when fully utilized, so it would come out to 600 kWh a day, which is roughly 40 USD in electricity bills if you live in the US. Tim Dettmers has a <a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/" rel="external nofollow noopener" target="_blank">useful blog</a> that explains these power considerations when building your own machine.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iix2-googles-tensor-processing-units-tpus">II.x.2: Google’s Tensor Processing Units (TPUs)</h3>
<figure>
<center>
    <img src="/assets/img/efficient_dl/15.png" style="width:70%" alt="TPU">
    <figcaption><b>Figure 15.</b> The architecture for a TPUv1 is actually pretty simple, with a strong emphasis on maximizing matrix multiplication throughput. <a href="https://arxiv.org/pdf/1704.04760" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>The CUDA ecosystem is not the only choice for parallel processing. Google’s in-house <a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit" rel="external nofollow noopener" target="_blank">Tensor Processing Units (TPUs)</a>, first introduced publicly in 2016, are a custom application-specific integrated circuit (<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit" rel="external nofollow noopener" target="_blank">ASIC</a>) designed for deep learning workloads at Google. TensorFlow and Jax have dedicated compilers for TPUs, making them the standard choice for programming these devices (PyTorch support has been added, but it’s not great).</p>

<ul>
  <li>While NVIDIA and <a href="https://www.amd.com/en/products/graphics/desktops/radeon.html" rel="external nofollow noopener" target="_blank">AMD GPUs</a> have features like the <a href="https://fileadmin.cs.lth.se/cs/Personal/Michael_Doggett/pubs/doggett12-tc.pdf" rel="external nofollow noopener" target="_blank">texture cache</a> that are designed for gaming applications, TPUs specialize in high-throughput, low-precision matrix multiplication with low energy usage.</li>
  <li>TPUs use <strong>their own systolic array “Tensor Core”</strong>, which handles <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#chips" rel="external nofollow noopener" target="_blank">128x128 multiply-accumulate operations</a> (compared to the 4x4 for NVIDIA!) in a single instruction cycle. This design favors large matrix computations.</li>
  <li>The TPU features special instructions and hardware for activation functions and a super-fast buffer for moving data.</li>
  <li>Google has since come out <strong>6 generations of TPUs</strong>, with the latest using 256x256 Tensor Cores to accelerate even larger model computations.</li>
  <li>You can’t actually buy your own TPUs, and you have to use cloud-provided TPUs (or work at Google) to use them for your own applications.</li>
  <li>Similar to the design of SXM boards for NVIDIA GPUs, TPUs also have dedicated “<a href="https://cloud.google.com/tpu/docs/training-on-tpu-pods" rel="external nofollow noopener" target="_blank">TPU Pods</a>” to connect multiple devices with high-speed communication.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>II.x.3: Potpourri of other interesting hardware</strong></p>

<p>The popularity of NVIDIA GPUs is in part due to the success of the Transformer and other parallelizable architectures. However, for different memory access patterns, there exist other hardware alternatives that could later play a pivotal role in the field.</p>

<p><strong>Field-gate Programmable Arrays (FPGA).</strong> <a href="https://www.arm.com/glossary/fpga" rel="external nofollow noopener" target="_blank">FPGAs</a> have seen some use in efficient deep learning as a low-cost hardware to target. Because of the availability of GPUs and ASICs like TPUs, it is hard to justify designing and programming these devices for actual workloads. Nevertheless, I wouldn’t write off FPGAs — they have a variety of <a href="https://halverscience.net/fpgas_for_sci_and_eng/" rel="external nofollow noopener" target="_blank">use-cases in the sciences</a> and <a href="https://www.imc.com/us/articles/how-are-fpgas-used-in-trading" rel="external nofollow noopener" target="_blank">low-latency applications</a>, and there is a chance that they will become important in deep learning as well.</p>

<p><strong>Neuromorphic Chips.</strong> We know that the human brain is extraordinarily efficient and powerful (except maybe mine), so a natural question is whether we can design computer hardware around the brain. There are some primitives like <a href="https://en.wikipedia.org/wiki/Spiking_neural_network" rel="external nofollow noopener" target="_blank">Spiking Neural Networks</a> that have been designed in the past, but most of this work has not really taken off in “modern deep learning”. There are also some small neuromorphic chips like <a href="https://research.ibm.com/publications/truenorth-design-and-tool-flow-of-a-65-mw-1-million-neuron-programmable-neurosynaptic-chip" rel="external nofollow noopener" target="_blank">IBM’s TrueNorth</a>, but I haven’t seen significant progress in this area yet. Like quantum computers, however, I am hopeful that people crack this research direction and apply them to AI!</p>

<p><strong>Etched (2024)</strong> [<a href="https://www.etched.com/" rel="external nofollow noopener" target="_blank">site</a>]. Very recently, a startup company came out with a Transformer-specific ASIC called Sohu that they claim accelerates Transformer workloads (not sure if it’s also training?) by an undefined margin. Little information is known about the underlying hardware and how good it actually is, but a Transformer-specific ASIC itself is not a far-fetched idea.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h2 id="part-iii-the-era-of-scale-till-we-fail-2020-now">Part III: The Era of Scale till we Fail (2020-Now)</h2>
<p><strong><a href="https://arxiv.org/abs/2005.14165" rel="external nofollow noopener" target="_blank">GPT-3</a> (OpenAI, 2020<d-cite key="brown2020languagemodelsfewshotlearners"></d-cite>)</strong>. The introduction of GPT-3 was eye-opening for a lot of researchers in the field — <strong>simply scaling a Transformer to 175B parameters</strong> while maintaining the same tricks used in prior works in the field was sufficient to build a syntactically sound and somewhat semantically reasonable model. Furthermore, while most prior works had been task-specific, GPT-3 was flexible enough to perform reasonably on a wide variety of language tasks.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/16.png" style="width:70%" alt="gpt3">
    <figcaption><b>Figure 16.</b> GPT-3, by design, was nothing complex. At its core, it was simply 96 stacked Transformer layers, an embedding layer for the tokens, and small output heads. <a href="https://arxiv.org/pdf/1704.04760" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>Its successor, <a href="https://openai.com/index/chatgpt/" rel="external nofollow noopener" target="_blank">GPT-3.5 / ChatGPT</a> would later blow up the field of AI to the public, but these methods would introduce a combination of new post-training tricks (<a href="https://arxiv.org/pdf/2109.01652" rel="external nofollow noopener" target="_blank">instruction-tuning</a> &amp; <a href="https://arxiv.org/abs/2203.02155" rel="external nofollow noopener" target="_blank">RLHF</a>) and <a href="https://arxiv.org/abs/2306.11644" rel="external nofollow noopener" target="_blank">better data</a> that are not rigorously understood. Scaling these models became a whole new game than all previous works, with the goal of building general-purpose “<a href="https://en.wikipedia.org/wiki/Foundation_model" rel="external nofollow noopener" target="_blank">foundation models</a>” that could be applied to any task. For this reason, the rest of this post will primarily focus on transformer-based architectures or recent alternatives (e.g. state-space models, <a href="https://arxiv.org/abs/2404.19756" rel="external nofollow noopener" target="_blank">Kolmogorov-Arnold Networks</a>). Many of the following ideas certainly apply to existing deep learning methods, and molding these approaches to older algorithms is definitely a useful research direction that may yield meaningful results.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="part-iii0-lets-talk-about-the-h100-gpu">Part III.0: Let’s talk about the H100 GPU</h3>

<figure>
<center>
    <img src="/assets/img/efficient_dl/h100.png" style="width:90%" alt="h100">
</center>
</figure>

<p>NVIDIA’s <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" rel="external nofollow noopener" target="_blank">Hopwell microarchitecture</a> (2022), along with the H100/H200 GPUs, introduced a few notable new features to accelerate deep learning workloads.  In addition to having effectively more/faster memory, higher memory bandwidth, and more CUDA &amp; Tensor Cores than the A100, the H100 also features:</p>
<ul>
  <li>
<strong>Tensor Memory Accelerator</strong> (TMA). The whole concept behind “<a href="https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf" rel="external nofollow noopener" target="_blank">streams</a>” that we introduced before was to ensure non-overlapping operations like memory movement and using the Tensor Cores were done in parallel. The TMA is a new hardware unit that <strong>asynchronously</strong> computes memory addresses (this is not a free operation on older devices and had to be done with registers!) for fetching data between shared memory and global memory. In other words, we no longer need to dedicate threads to perform data transfers and can instead focus on feeding the Tensor Cores.</li>
  <li>
<strong>High-speed low-precision</strong>. Tensor Cores now support the FP8 data type and can theoretically reach <strong>3300 TFLOPS</strong> for FP8 operations.</li>
  <li>
<strong>Thread block clusters.</strong> A new level of the CUDA programming hierarchy sits above the thread block — all threads in a thread block cluster are concurrently scheduled onto SMs, making communicating <strong>between them</strong> more efficient with the CUDA cooperative_groups API.</li>
  <li>
<strong>SM-to-SM shared memory.</strong> They formally call this <strong>distributed shared memory</strong>, but basically a programmer can now access shared memory that sits on other SMs (presumably through a shared virtual address space) without having to move it to the L2 cache / global memory.</li>
  <li>
<strong>DPX instructions.</strong> The promotional material for these instructions keeps claiming that they “accelerate dynamic programming (DP) algorithms”, but I’m pretty sure from the <a href="https://docs.nvidia.com/cuda/pdf/Hopper_Tuning_Guide.pdf" rel="external nofollow noopener" target="_blank">Hopper guide</a> that it’s just specialized instructions for min/max and additions <strong>that are common in DP algorithms</strong> — the actual loop and sequential nature of DP isn’t changed at all.</li>
</ul>

<p>With the release of the H100, a few interesting developments have been made to target these devices, including FlashAttention3, which we will talk about in the coming section.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>WGMMA (Warpgroup Matrix-multiply-accumulate)</strong><d-footnote>This blogpost by Colfax is so good: https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/</d-footnote>.  The <code class="language-plaintext highlighter-rouge">wgmma.mma_async</code> instruction allows threads to launch matrix multiplication on the Tensor Cores as a <strong>non-blocking operation</strong>. In other words, they’re free to handle other tasks like data loading to further increase throughput and hide latency.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>ThunderKittens</strong> (<a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk" rel="external nofollow noopener" target="_blank">Spector et al., 2024</a>). The H100 has a lot of new features that are really annoying to target yourself. <a href="https://github.com/HazyResearch/ThunderKittens" rel="external nofollow noopener" target="_blank">ThunderKittens</a> is a <strong>domain-specific language</strong> (just an extension on top of CUDA C++ basically)  that you can use to abstract away a lot of these features at the warp-level while the compiler handles all of the nitty-gritty details. I haven’t tried it myself because I don’t have an H100, but it looks like a promising library to consider using. I also included the blog in this section because it has some nice details about how they target the H100 that are really well-written!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="part-iii1-the-era-of-scale-on-a-single-gpu">Part III.1: The Era of Scale (on a single GPU)</h3>
<p>By this point, there were clear signs that <strong>scaling up the number of parameters in a model and the amount of data was almost purely beneficial</strong> for improving model capabilities. The obvious solution to scaling networks was to 1) add more compute and 2) wait for longer training runs. But <strong>adding devices is extremely expensive and does not linearly add more memory and training speed</strong> as we will discuss in <a href="#part-iii2-the-era-of-scale-distributed-version">Part III.2</a>, so there was a lot of interest in squeezing out as many FLOPS and bytes out of every GPU as possible. Before it was settled that the attention mechanism was extremely important as is, alternatives with better runtime and memory scaling were first proposed.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii10-early-insights">III.1.0: Early insights</h3>
<p><strong><a href="https://arxiv.org/abs/1604.06174" rel="external nofollow noopener" target="_blank">Activation checkpointing</a> (Chen et al., 2016<d-cite key="chen2016trainingdeepnetssublinear"></d-cite>)</strong>. One widely used technique for trading speed for memory is to re-compute activations during the backwards pass instead of storing them during the forward pass. This idea is also used to speed up overall training in important works like <a href="https://arxiv.org/abs/1910.02054" rel="external nofollow noopener" target="_blank">ZeRO</a> due to the nature of the GPU memory hierarchy, which we will cover in the next section.</p>

<p><strong><a href="https://peterchng.com/blog/2024/06/11/what-is-the-transformer-kv-cache/" rel="external nofollow noopener" target="_blank">KV Caching</a></strong> (2017?).<d-footnote>I actually have no idea when this trick was first introduced — my guess is that it was sort of an obvious engineering trick that people knew about for a while, but didn’t really need to talk about in publications until LLM serving became bigger as a field / after ChatGPT came out in 2022.</d-footnote> For causal Transformers (upper-triangular mask), a well-known trick for next-token prediction is to store the previously computed keys and values in memory, so we only need to compute $K/V/Q $ for the most-recent token. A large number of works we will discuss deal with the growing KV cache, which takes up a large chunk of valuable DRAM and is not a fixed size.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii1a-shaving-complexity-through-approximate-methods">III.1.a: Shaving complexity through Approximate Methods</h3>

<figure>
<center>
    <img src="/assets/img/efficient_dl/17.png" style="width:100%" alt="approximate attention">
    <figcaption><b>Figure 17.</b> Approximate attention through masking patterns introduced in LongFormer. <a href="https://arxiv.org/pdf/2004.05150" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p>A long series of works have proposed approximations to the general attention mechanism in hopes of scaling these methods to sub-quadratic memory and runtime. We list some notable examples in chronological order<d-footnote>Not all of the code examples are the official code repos. I referenced lucidrains a lot because his (PyTorch usually) repos are just a lot easier to digest and are stripped down to the important code.</d-footnote>:</p>

<ul>
  <li>
<strong>Sparse Transformers (Child et al., 2019<d-cite key="child2019generatinglongsequencessparse"></d-cite>)</strong> [<a href="https://arxiv.org/pdf/1904.10509" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/openai/sparse_attention" rel="external nofollow noopener" target="_blank">Code</a>]. Early work on constraining fixed sparse attention patterns (across heads, though this isn’t too relevant anymore) so each query can only attend to $O(\sqrt{N})$ of the keys. They evaluate on a variety of image and audio tasks, although the results aren’t high quality for today’s standards.</li>
  <li>
<strong>Reformer (Kitaev et al., 2020<d-cite key="kitaev2020reformerefficienttransformer"></d-cite>)</strong> [<a href="https://arxiv.org/abs/2001.04451" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/lucidrains/reformer-pytorch" rel="external nofollow noopener" target="_blank">Unofficial Code</a>] This idea is really cute — they posit that attention weights are largely concentrated on a few elements, so they use a locality-sensitive hashing scheme find the $K=\log(N)$ nearest keys for each query and only compute those for the attention mechanism.</li>
  <li>
<strong>Linformer (Wang et al., 2020<d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>)</strong> [<a href="https://arxiv.org/abs/2006.04768" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/lucidrains/linformer" rel="external nofollow noopener" target="_blank">Unofficial Code</a>]. They reason using the <a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma" rel="external nofollow noopener" target="_blank">Johnson-Lindenstrauss</a> lemma<d-footnote>There are many variants, but the core idea is that we can (randomly) project points in a high-dimensional normed space to a lower-dimensional normed space such that distances are preserved up to some error that is a function of the number of points in the space. Basically, it’s used a lot whenever we want to analyze whether moving to lower dimensions is “fine”.</d-footnote> that when computing the attention matrix, they actually just compute it as a product of two low-rank matrices. Their proposed decomposition is extremely simple, and it literally is just projecting down the key and value matrices to a constant dimension.</li>
  <li>
<strong>Longformer (Beltagy et al. 2020<d-cite key="beltagy2020longformerlongdocumenttransformer"></d-cite>)</strong> [<a href="https://arxiv.org/abs/2004.05150" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/allenai/longformer" rel="external nofollow noopener" target="_blank">Code</a>]. Longformer is just an empirically-motivated set of masking patterns over the attention matrix for efficiency. They mainly use a sliding window local attention scheme (see <strong>Figure 17</strong>), but also allow attending sparsely to global positions.</li>
  <li>
<strong>Performer (Choromanski et al., 2021<d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>)</strong> [<a href="https://arxiv.org/abs/2009.14794" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/lucidrains/performer-pytorch" rel="external nofollow noopener" target="_blank">Unofficial Code</a>]. Instead of using a low-rank or sparsity assumption, they observe that the attention operation $A(i,j) = \exp(q_i, k_j^T) = K(q_i, k_i)$ is a kernel, which can be written in the form $\phi(q_i)^T \phi(k_i)$. The choice of $\phi$ is motivated to be an unbiased estimator using random features<d-footnote>See https://gregorygundersen.com/blog/2019/12/23/random-fourier-features/ for background.</d-footnote>, and ultimately the decomposition removes the annoying softmax function and reduces the number of operations.</li>
  <li>
<strong>InfiniAttention (Munkhdalai et al. 2024<d-cite key="munkhdalai2024leavecontextbehindefficient"></d-cite>)</strong> [<a href="https://arxiv.org/abs/2404.07143" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/alexzhang13/InfiniAttention" rel="external nofollow noopener" target="_blank">Unofficial Code</a>]. InfiniAttention avoids sequence-length time/memory complexity by storing a recurrent-style attention matrix that is fixed size, but is updated in memory. They chunk up sequences and sequentially process them, theoretically enabling infinite scaling at the cost of a fixed representation.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p>While many tricks like sparsity, <a href="https://www.ethanepperly.com/index.php/2021/10/26/big-ideas-in-applied-math-low-rank-matrices/" rel="external nofollow noopener" target="_blank">low-rankness</a>, and kernel decomposition were tried, in the end, most of these methods are unused in modern LLMs. Some of the more practical approximations for the attention mechanism are a lot simpler in practice and provide clear memory or runtime improvements over the original.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/18.png" style="width:100%" alt="gqa">
    <figcaption><b>Figure 18.</b> Grouped query attention is a simple approximation method to share keys/values across heads. In the diagram above, each “vector” is a head rather than a single key/query/value vector. <a href="https://arxiv.org/pdf/2305.13245" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2305.13245" rel="external nofollow noopener" target="_blank">Grouped Query Attention</a> (Ainslie et al., 2023<d-cite key="ainslie2023gqatraininggeneralizedmultiquery"></d-cite>)</strong>. A super simple but widely used approximate attention method is to preserve the standard per-head attention, but instead share keys/values across different heads to reduce the memory footprint. The original work (<a href="https://arxiv.org/abs/1911.02150" rel="external nofollow noopener" target="_blank">multi-query attention</a>) re-used the same keys/values across all heads, but in this work they find it better to tune this re-use factor. It turns out that we can get away with this in practice, and from an implementation stand-point there is no hidden drawback to doing this.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii1b-architecture-design">III.1.b: Architecture Design</h3>
<p>Some architecture choices have been motivated by existing bottlenecks in scaling large models. For language models, the naive approach is to just increase the number of attention blocks, but there are other methods that balance memory and capacity tradeoffs differently.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/19.png" style="width:100%" alt="moe">
    <figcaption><b>Figure 19.</b> Mixture-of-Experts layer used in the Switch Transformer to scale LLMs to trillions of parameters without exploding working memory. <a href="https://arxiv.org/abs/2101.03961" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/pdf/1701.06538" rel="external nofollow noopener" target="_blank">Mixture-of-Experts in NLP</a> (Shazeer et al. 2017<d-cite key="shazeer2017outrageouslylargeneuralnetworks"></d-cite>)</strong>. Mixture-of-Experts (MoE) is an older technique<d-footnote>Huggingface has a nice article on the history, which dates back to the 90’s: https://huggingface.co/blog/moe#a-brief-history-of-moes</d-footnote> for scaling deep learning models to extremely high parameter counts without needing to access all the parameters at any time. The first interesting application was done on the LSTM architecture, and generally it consists of a small learnable gating network that activates a subset of the parameters sitting on different devices. As we will see, MoE is particularly useful for LLMs, as it enables scaling model capacity without scaling the resource consumption.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2109.08668" rel="external nofollow noopener" target="_blank">Primer: Searching for Efficient Transformers for Language Modeling</a> (So et al., 2021<d-cite key="so2022primersearchingefficienttransformers"></d-cite>)</strong>. There are plenty of existing works for tweaking and modifying the attention architecture to “scale” better. I’ve referenced this paper because it is quite simple and is one of the earlier works to propose tricks like ReLU^2 and neural architecture search over Transformers. I’m not entirely sure what is done in practice, but as far as I know, there are generally some “good practices” for Transformer blocks, and it is difficult to perform these architecture search algorithms for extremely large models.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2101.03961" rel="external nofollow noopener" target="_blank">Switch Transformers</a> (Fedus et al., 2021<d-cite key="fedus2022switchtransformersscalingtrillion"></d-cite>)</strong>. Their central hypothesis was that <strong>scaling model parameters while keeping FLOPS constant was still a useful dimension of scale</strong>. They replace the FFN MLP layer in the Transformer with an MoE router that routes each token after the attention block to an expert, while also fixing the maximum number of tokens each expert can process. They also add a super simple load-balancing loss that penalizes non-uniform token routing. As it turns out, MoE enables us to scale our models to upwards of trillions of parameters without actually incurring the cost of a trillion parameter model on each forward/backwards pass! It was rumored last year that GPT-4 was a giant 1T MoE model that used tricks like group-query attention and rotary embeddings (<a href="https://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">RoPE</a>).</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii1c-fine-tuning-large-models-efficiently">III.1.c: Fine-tuning Large Models Efficiently</h3>
<p>It is well known that pre-training large foundation models is way out of the budget of a standard researcher<d-footnote>For example, Llama-3 is known to have cost tens of millions of dollars to pre-train.</d-footnote>. Fine-tuning or general post-training (e.g. instruction tuning and RLHF) has become a popular research avenue because it is significantly cheaper and can be task-specific. Researchers began to notice over time that shortcuts could be made to the fine-tuning process to make it feasible for independent researchers to play with.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/1902.00751" rel="external nofollow noopener" target="_blank">Adapters</a> (Houlsby, 2019<d-cite key="houlsby2019parameterefficienttransferlearningnlp"></d-cite>)</strong>. The distinction between fine-tuning and pre-training is actually indistinguishable from a standard machine learning perspective, unless we specifically constrain the optimization problems to be different. To make fine-tuning computationally cheap, Adapters are learnable functions $f_{\hat{\theta}} : \mathbb{R}^n \rightarrow \mathbb{R}^n$ that can be inserted in between the layers of a model. The idea is that we freeze the original model weights $\theta$, and only update the adapter weights $\hat{\theta}$, significantly reducing the memory and fine-tuning time of a model. Intuitively, adapters make sense in the context of language modeling because we believe that fine-tuning should not alter the weights “that much”<d-footnote>”that much” is super hand-wavy. I’m not actually sure if there’s a paper out there that uses norms or other metrics to discuss similarity between a fine-tuned and pre-trained model. If not, could be an interesting research question.</d-footnote> from the base model.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">LoRA</a> (Hu et al., 2021<d-cite key="hu2021loralowrankadaptationlarge"></d-cite>)</strong>. Given a pre-trained model with parameters $\theta$, the central hypothesis of LoRA is that a fine-tuned model weights can be decomposed into $\theta + \Delta \hat{\theta}$, where $\Delta \hat{\theta} \in \mathbb{R}^{m \times n}$ is low-rank and $\theta$ is frozen. In other words, we can factorize $\Delta \hat{\theta} = AB$ where $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$ and $r \ll \min(m,n)$<d-footnote>Strangely, I don’t have a lot of intuition for learned matrix decomposition. This idea is popular in recommendation systems / factorization machines, and is supposedly SVD-esque, but I don’t know what properties you can derive from these factorized matrices. If anyone knows, please tell me! </d-footnote>. Furthermore, unlike adaptors, LoRA adds no extra overhead during inference time!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/20.png" style="width:100%" alt="qlora">
    <figcaption><b>Figure 20.</b> In Q-LoRA, they quantize a Transformer to 4-bit NormalFloat (NF4) and perform LoRA over a larger selection of weights due to the extra allowable memory, which they attribute to its good performance. They demonstrate that fine-tuning a 65B LLM can be done with 48GB of DRAM (on a single device!) with minimal performance degradation. <a href="https://arxiv.org/abs/2305.14314" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2305.14314" rel="external nofollow noopener" target="_blank">Q-LoRA</a> (Dettmers et al., 2023<d-cite key="dettmers2023qloraefficientfinetuningquantized"></d-cite>)</strong>. This landmark paper enabled a lot of future work on fine-tuning LLMs, diffusion models, and other foundation models on a single device. They observe that 1) base models are still huge and need to <strong>fit in memory when using LoRA</strong>, 2) activations/gradients have a large memory footprint in LoRA, which <a href="#CITE%20EARLY%20INSIGHTS">activation/gradient checkpointing</a> can partially solve, and 3) block-wise quantization can have many constants take up significant space in memory.</p>
<ul>
  <li>To solve (1), they introduce the <strong>4-bit NormalFloat</strong> type, which quantizes the weights by evenly dividing the range based on the <a href="https://en.wikipedia.org/wiki/Gaussian_measure" rel="external nofollow noopener" target="_blank">Gaussian measure</a>.</li>
  <li>To solve (2), they introduced a paged optimizer based on <a href="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/" rel="external nofollow noopener" target="_blank">NVIDIA unified memory</a> to move optimizer states between GPU DRAM and CPU RAM when necessary, as they are only used for backpropagation.</li>
  <li>To solve (3), they quantize the quantization constants to a lower precision. Q-LoRA is basically a whole collection of memory reduction techniques for performing LLM fine-tuning on affordable hardware. The LoRA component remains untouched, but the memory reductions allow LoRA to be applied to all layers in a model for better performance.</li>
</ul>

<p>Combined together, a Q-LoRA tuned layer can be written as:</p>

<p>
$$
f(X^{(bf16)}) = X^{(bf16)}\text{dequant}(W^{(NF4)}) + X^{(bf16)}A^{(bf16)}B^{(bf16)}
$$
</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://github.com/huggingface/peft" rel="external nofollow noopener" target="_blank">Huggingface PEFT Library</a></strong>. There are a few other major parameter-efficient fine-tuning (PEFT) works like LoRA and adaptors such as <a href="https://arxiv.org/abs/2101.00190" rel="external nofollow noopener" target="_blank">prefix tuning</a>, <a href="https://arxiv.org/abs/2104.08691" rel="external nofollow noopener" target="_blank">soft prompts</a>, and <a href="https://arxiv.org/abs/2205.05638" rel="external nofollow noopener" target="_blank">$(IA)^3$</a> that all kind of boil down to “I believe that we can fine-tune a model by slightly adding or injecting information to the pre-trained model”. Honestly, PEFT as a whole is extremely hand-wavy, and a lot of the methods are ways to condition or perturb model weights based on the fine-tuning dataset. HuggingFace has a nice wrapper for running different PEFT methods for your models. For details on specific PEFT variants, I’d suggest reading this <a href="https://arxiv.org/abs/2403.14608" rel="external nofollow noopener" target="_blank">survey paper</a>.</p>

<p><strong>Remark</strong>. I couldn’t really fit this work in, but I wanted to mention <a href="https://arxiv.org/abs/2404.03592" rel="external nofollow noopener" target="_blank">ReFT</a>, which I think is a really cute idea that turns out to work well in practice. Based on the <a href="https://www.beren.io/2023-04-04-DL-models-are-secretly-linear/" rel="external nofollow noopener" target="_blank">hypothesis that high-level concepts in language model are directions</a> in some representation space, they fine-tune model generations by learning disjoint “interventions” over the model hidden states (i.e. an adapter motivated by interpretability work). I haven’t fully read into the interpretability work that led to <a href="https://arxiv.org/pdf/2303.02536" rel="external nofollow noopener" target="_blank">DII</a>, but their experiments are pretty convincing.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii1d-fused-kernels-and-the-gpgpu">III.1.d: Fused kernels and the GPGPU</h3>
<p><em>Read <a href="#part-iix-hardware">Part II.x: Hardware</a> before continuing in this section.</em></p>

<p>Eventually, it became clear that cute tricks like sparsity and dimensionality reduction on the attention mechanism were not only hurting model performance, but they weren’t even providing <a href="https://stackoverflow.com/questions/7335920/what-specifically-are-wall-clock-time-user-cpu-time-and-system-cpu-time-in-uni" rel="external nofollow noopener" target="_blank">wall-clock speed</a> improvements to these models. You may have heard the term “<a href="https://stackoverflow.com/questions/56601075/what-is-a-fused-kernel-or-fused-layer-in-deep-learning" rel="external nofollow noopener" target="_blank">fused kernel</a>” used to describe an optimization to a deep learning model. The term kernel is overloaded quite often, but in this instance it just refers to a program run on the GPU. We focused a lot in the earlier sections on building up models as modular, stackable components that we could freely optimize, but allowing this flexibility is not necessarily hardware-friendly. Consider the following example for computing the attention operation in PyTorch:</p>

<p>
$$
\mathbf{O} = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V}
$$
</p>

<d-code block="" language="python" style="font-size:0.7em">
def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -&gt; torch.Tensor:
    d_k = math.sqrt(Q.size(-1))
    scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k
    p_attn = scores.softmax(dim=-1)
    O = torch.matmul(p_attn, V)
    return O
</d-code>

<p>In eager execution mode or without a clever compiler, every assignment $y = f(x_1,x_2,…)$ in the code above will do something like this.</p>

<ol>
  <li>The variable(s) $x_1,x_2,…$ will be sitting in the GPU DRAM/HBM. We first have to <strong>load</strong> it onto the processors/SMs, which is quite slow.</li>
  <li>We perform the transform $f(x_1,x_2,…)$ on device. This operation is relatively fast because the torch functions (e.g. <code class="language-plaintext highlighter-rouge">torch.matmul</code>) are heavily optimized.</li>
  <li>We <strong>store</strong> the result $f(x_1,x_2,…)$ which sits on device registers back into DRAM, and point to it with the variable $y$.</li>
  <li>If $y$ is ever used in subsequent lines, we have to load it back into registers, and repeat.</li>
</ol>

<p>Fused kernel implementations usually aim to remove these intermediate stores and loads to DRAM that Python compilers cannot optimize out. Depending on the level of granularity in the language used (e.g. <a href="https://openai.com/index/triton/" rel="external nofollow noopener" target="_blank">Triton</a> vs. CUDA), we can control data movement at all levels of the GPU memory hierarchy. To get a sense for the relative speeds of each level of the hierarchy, we list some data movement speeds on an NVIDIA H100 GPU found in this <a href="https://arxiv.org/pdf/2402.13499v1" rel="external nofollow noopener" target="_blank">microbenchmarking work</a>. For reference, the H100 runs at roughly 1.5 GHz, or $1.5 \times 10^9$ clock cycles per second.</p>

<table>
  <tr>
    <th>Type of Memory Access</th>
    <th>Number of Clock Cycles</th>
  </tr>
  <tr>
    <td>HBM Access</td>
    <td>~480 clock cycles</td>
  </tr>
  <tr>
    <td>L2 Cache Hit</td>
    <td>~260 clock cycles</td>
  </tr>
  <tr>
    <td>L1 Cache Hit</td>
    <td>~40 clock cycles</td>
  </tr>
  <tr>
    <td>Shared Memory Access</td>
    <td>~30 clock cycles</td>
  </tr>
  <tr>
    <td>Register Access</td>
    <td>~1 clock cycles</td>
  </tr>
</table>

<p>In the following section, we’ll talk a bit about existing fused kernel strategies for attention, followed by some examples in other fields.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/21.png" style="width:100%" alt="flashattention">
    <figcaption><b>Figure 21.</b> Visualization of the original FlashAttention implementation and the associated GPU memory hierarchy that it optimizes. <a href="https://arxiv.org/pdf/2205.14135" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2205.14135" rel="external nofollow noopener" target="_blank">FlashAttention</a> (Dao et al., 2022<d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite>)</strong>. Standard attention implementations, like the PyTorch implementation above, involve several passes to and from GPU DRAM/HBM. The key insight in building the fused attention kernel is computing the softmax block-by-block — however, computing the softmax requires loading all keys into a block, which does not fit into the limited SRAM space. The authors instead minimize accesses to global memory by using a trick called the <a href="https://arxiv.org/abs/1805.02867" rel="external nofollow noopener" target="_blank">online softmax</a> while simultaneously loading in the relevant value blocks. Furthermore, they re-compute the attention matrix in the backwards pass. Launched kernels are parallelized over the batch size and the number of heads.</p>

<p><strong><a href="https://arxiv.org/abs/2307.08691" rel="external nofollow noopener" target="_blank">FlashAttention2</a> (Dao, 2023<d-cite key="dao2023flashattention2fasterattentionbetter"></d-cite>)</strong>. The successor implementation of FlashAttention minimizes non-matrix multiplication operations such as the online softmax scaling term, which are significantly slower due to A100 Tensor Cores — while the <strong>max throughput for FP16 matmuls is 312 TFLOPS</strong>, for <strong>standard FP32 operations it is only 19.5 TFLOPS</strong>. Furthermore, they avoid <a href="https://www.youtube.com/watch?v=g5ZKBH6UQvE&amp;list=PLRRuQYjFhpmubuwx-w8X964ofVkW1T8O4&amp;index=21&amp;ab_channel=ProgrammingMassivelyParallelProcessors" rel="external nofollow noopener" target="_blank">intra-warp synchronization</a><d-footnote>Recall that threads in a warp call the same instructions SIMT-style. However, across warps in the same block, we often will call a block-level synchronization barrier with `__syncthread()` when we need to wait for all previous threads to finish. The authors minimizes these barrier calls in FA2 by changing which warps handle which matrices. If this whole explanation is confusing to you, I totally understand. The original paper has some nice diagrams that explain it better, but it’s definitely a GPU-specific detail.</d-footnote> by switching how they loop over the $\mathbf{Q}$ and $\mathbf{K}/\mathbf{V}$ matrices. One particular limitation of these methods is no support for custom attention masks and attention biases, which is now supported in <a href="https://pytorch.org/blog/flexattention/" rel="external nofollow noopener" target="_blank">FlexAttention</a> as of August 2024 (I also had written a <a href="https://github.com/alexzhang13/flashattention2-custom-mask" rel="external nofollow noopener" target="_blank">Triton implementation for FA2</a>).</p>

<p><strong><a href="https://arxiv.org/abs/2407.08608" rel="external nofollow noopener" target="_blank">FlashAttention3</a> (Shah et al., 2024<d-cite key="shah2024flashattention3fastaccurateattention"></d-cite>)</strong>. The latest version of FlashAttention specifically targets the H100/H200 GPUs, and the focus reads completely differently from v1 and v2. Namely, the new <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tensors" rel="external nofollow noopener" target="_blank">WGMMA instruction</a> we talked about in <a href="#part-iii0-lets-talk-about-the-h100-gpu">Part III.0</a> and the TMA offer essentially free speed-ups. Furthermore, separating data loading (TMA) and computation (WGMMA) in different warps, a technique called <a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization" rel="external nofollow noopener" target="_blank">warp specialization</a>, is also used to <strong>maximize Tensor Core usage</strong>. Finally, the authors observe that <strong>non-matmul operations like exponentiation in softmax are up to 256x slower</strong> than matmuls, so they <a href="https://tridao.me/blog/2024/flash3/#inter-warpgroup-overlapping-with-pingpong-scheduling" rel="external nofollow noopener" target="_blank">manually schedule warpgroups in a pipelined fashion</a> to reduce potential bubbles created by these interleaved operations.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>xFormers (Facebook Research, 2021) [<a href="https://github.com/facebookresearch/xformers/releases" rel="external nofollow noopener" target="_blank">Repo</a>]</strong>. The xFormers repository features a series of CUDA and Triton kernels for various Transformer components like attention, layer norms, dropout, etc. Prior to the release of FlexAttention, the xFormers repo was also the standard for a fast attention algorithm with custom attention biases.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2410.10989" rel="external nofollow noopener" target="_blank">Liger Kernel</a> (Hsu et al., 2024<d-cite key="hsu2024ligerkernelefficienttriton"></d-cite>) [<a href="https://github.com/linkedin/Liger-Kernel/" rel="external nofollow noopener" target="_blank">Repo</a>]</strong>. Recently, a large number of fused kernel implementations for LLM training were released by researchers at Linkedin. In addition to being more memory-efficient and faster than pre-existing Huggingface implementations, they are extremely easy to understand because they were written in Triton.<d-footnote>Some of these kernels were featured in depth in one of the GPU Mode lectures: https://www.youtube.com/watch?v=gWble4FreV4&amp;ab_channel=GPUMODE</d-footnote></p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Other examples.</strong> While fused kernels have seen extensive interest in transformer-based LLM applications, there are other areas where fused kernels were critical to their success. We list a few notable examples below.</p>

<ul>
  <li>
<strong><a href="https://arxiv.org/abs/2311.05908" rel="external nofollow noopener" target="_blank">FlashFFTConv</a> (Fu et al. 2023<d-cite key="fu2023flashfftconvefficientconvolutionslong"></d-cite>).</strong> It is well known that for functions $u(x), v(x)$ with Fourier transforms $\mathcal{F}(u), \mathcal{F}(v)$, the convolution can be written as $ \{u * v \} (x) = \mathcal{F}^{-1} \{\mathcal{F}(u) \cdot \mathcal{F}(v) \} $. It is also well known that the Fast Fourier Transform can be computed in $O(N \log N)$, so we can compute convolutions for state-space models in $O(N \log N)$ where $N$ is the sequence length! However, despite the better runtime complexity than attention, in practice, Transformers are still faster to train on modern hardware. <strong>FlashFFTConv re-writes the FFT into a different decomposition that contains matrix multiplications</strong> to take advantage of Tensor Cores.</li>
  <li>
<strong><a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> (Gu et al., 2023<d-cite key="gu2024mambalineartimesequencemodeling"></d-cite>)</strong>. Prior state-space model methods (e.g. <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">S4</a>) impose a linear-time-invariant (LTI) constraint on the state update matrices so they can be re-written as a convolution to avoid the sequential computation needed for recurrent algorithms. While these models were interesting at the time, Mamba was a huge deal in the community because it removed the LTI constraint and added an input-dependent selection mechanism for its parameters. To remove the LTI constraint, the authors wrote a <strong>kernel to keep the recurrent state in fast shared memory</strong> to keep the computation fast.</li>
  <li>
<strong><a href="https://nvlabs.github.io/instant-ngp/" rel="external nofollow noopener" target="_blank">InstantNGP</a> (Müller et al. 2022<d-cite key="M_ller_2022"></d-cite>)</strong>. The novel view synthesis problem<d-footnote>The novel view synthesis problem is generating unseen views of a scene given a few reference images. With a fine granularity, you can even produce entire videos or interactable scenes from just an image.</d-footnote> has mostly been solved using Neural Radiance Fields (NeRFs), but the computational bottleneck of increasing resolution was large. InstantNGP was a hashing scheme for position-dependent features that was entirely written as a fused kernel, and is widely used as a standard in many subsequent NeRF works as well.</li>
  <li>
<strong><a href="https://github.com/Ligo-Biosciences/AlphaFold3" rel="external nofollow noopener" target="_blank">MSA Pair Weighted Averaging for AlphaFold3</a> (Me!)</strong>. <a href="https://www.nature.com/articles/s41586-024-07487-w" rel="external nofollow noopener" target="_blank">AlphaFold3</a> is a closed-source scientific breakthrough (most notably winning the <a href="https://www.nobelprize.org/prizes/chemistry/2024/summary/" rel="external nofollow noopener" target="_blank">2024 Nobel Prize in Chemistry</a>!) developed by Google DeepMind for predicting generic molecule interactions. While they most likely developed the model in Jax and optimized it for their in-house TPUs, researchers and start-ups outside of Google are interested in using the model for their own biotech use-cases. <a href="https://www.ligo.bio/" rel="external nofollow noopener" target="_blank">Ligo Biosciences</a> is a start-up developing an open-source version of this model, but certain algorithms such as the <a href="https://github.com/lucidrains/triangle-multiplicative-module" rel="external nofollow noopener" target="_blank">Triangular Multiplicative Update</a> and the <a href="https://github.com/alexzhang13/msa" rel="external nofollow noopener" target="_blank">MSA Pair Weighted Averaging</a> algorithm have extreme memory bottlenecks when written naively in PyTorch. I was interested in was writing fast and readable kernels for these algorithms (both forward and backwards passes), which I wrote in Triton<d-footnote>Triton is a programming language (it’s more of a library for Python) that compiles to an intermediate representation (IR) that NVIDIA GPUs can use. Rather than abstract at the thread-level like we’ve discussed for CUDA, it instead operates at the thread block level, and is far easier to prototype with. We will talk about this later, but torch.jit() compiles to Triton code.</d-footnote>. The MSA Pair Weighted Averaging algorithm in particular also has a pesky global softmax operation, and I used tricks similar to FlashAttention2 to minimize HBM accesses. Removing these bottlenecks has helped them feasibly scale their models on more data!</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii1e-deep-learning-compilers">III.1.e: Deep Learning Compilers</h3>
<p>Another parallel thread that the community was interested in was building specialized compilers<d-footnote>I’m going to assume the reader is at least familiar with what a compiler is useful for. A lot of the optimizations done by programming language compilers like constant folding and register assignment are also done by a deep learning compiler, and LLVM itself is used in this setting for compiling to the instruction-level. </d-footnote> for deep learning operations. ML compilers are really annoying to build because 1) there are so many different hardware devices that we can use (e.g. CPU, GPU, TPU, other ASICs), 2) in a standard compiler like gcc, we would normally have access to the entire codebase we are compiling. For ML, a “codebase” is basically the model computation graph, but this isn’t always accessible (i.e. eager mode in PyTorch)<d-footnote>On point (2), you probably don’t need a powerful compiler unless you are running production code, in which case you should not be running your models in eager mode. Regardless, as we will see, the PyTorch team still added an option through torch.jit() for compiling parts of your eager execution code.</d-footnote>.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/22.png" style="width:100%" alt="compiler targeting">
    <figcaption><b>Figure 22.</b> Deep learning compilers are hard to develop because different devices have completely different memory hierarchies and compute primitives. <a href="https://arxiv.org/pdf/1802.04799" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Intermediate representations (IR)</strong> are a critical element of modern compilers — instead of building a compiler for each pair of (language, hardware), we would ideally like to compile each language into a <a href="https://mcyoung.xyz/2023/08/01/llvm-ir/" rel="external nofollow noopener" target="_blank">common intermediate language</a> that can be converted to each specific <a href="https://www.codecademy.com/resources/docs/general/machine-code" rel="external nofollow noopener" target="_blank">machine code</a>. Deep learning applications are typically optimized in two steps, namely <a href="https://uditagarwal.in/ml-compilers-part-2-graph-optimizations/" rel="external nofollow noopener" target="_blank">graph-level optimization</a> of operators, and low-level optimization of the actual device-specific instructions. Below, we discuss some important frameworks and compilers that have evolved throughout the years — the list is not comprehensive (check out <a href="https://github.com/merrymercy/awesome-tensor-compilers" rel="external nofollow noopener" target="_blank">https://github.com/merrymercy/awesome-tensor-compilers</a>!), but focuses mainly on applications that have been popular for a while.<d-footnote>As I was researching this section, I came to the realization that a lot of it starts to bleed into standard compilers research, which is extensive and difficult for me to motivate. I’ve instead decided to just provide some high-level intuition for what these frameworks do, but I won’t be touching on the exact optimizations and design choices for each of these compilers, which was my original intention.</d-footnote></p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>ONNX (PyTorch Team, 2017</strong>). <a href="https://onnx.ai/get-started.html" rel="external nofollow noopener" target="_blank">ONNX</a> is not actually a compiler, but an open-source standard format and inference engine (<a href="https://onnxruntime.ai/" rel="external nofollow noopener" target="_blank">ONNX Runtime</a>) for model computation graphs across different libraries. Most libraries allow you to export your models to ONNX, allowing you to use their optimized runtime engine, as well as convert models easily between libraries. Many of the libraries listed below accept or expect a packaged ONNX model as input.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/23.png" style="width:100%" alt="compilers">
    <figcaption><b>Figure 23.</b> Most DL compilers first optimize over the compute graph, then target specific devices for a second pass of optimizations. <a href="https://arxiv.org/pdf/2002.03794" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong>Agnostic stacks</strong>. These frameworks are designed for developers to be able to modify and add certain parts of their compilers stack (e.g. targeting your own edge devices), and are widely used as general purpose compilers. They often features multiple IR conversion and optimization steps, and provide functionality for targeting your own hardware.</p>

<ul>
  <li>
<strong>TVM (Chen et al., 2018<d-cite key="chen2018tvmautomatedendtoendoptimizing"></d-cite>)</strong>. <a href="https://arxiv.org/abs/1802.04799" rel="external nofollow noopener" target="_blank">TVM</a> is an <strong>open-source</strong> <strong>end-to-end</strong> compiler stack for common deep learning platforms and targets a wide range of hardware. TVM first converts compute graphs into a <a href="https://www.cs.princeton.edu/~wayne/kleinberg-tardos/pearson/03Graphs.pdf" rel="external nofollow noopener" target="_blank">directed-acyclic graph</a> (DAG) IR called <a href="https://docs.calyxir.org/frontends/tvm-relay.html" rel="external nofollow noopener" target="_blank">Relay</a> — you may have learned about <a href="https://www.cs.princeton.edu/courses/archive/spring19/cos320/lectures/lecture4.pdf" rel="external nofollow noopener" target="_blank">let-based IRs</a> in your undergraduate compilers class that allow for optimizations like <a href="https://en.wikipedia.org/wiki/Dead-code_elimination" rel="external nofollow noopener" target="_blank">dead-code elimination</a>, and a DAG IR is basically just the equivalent for a graph. The individual tensor operators have a separate optimization step, and TVM uses functional “<a href="https://tvm.apache.org/docs/tutorial/tensor_expr_get_started.html" rel="external nofollow noopener" target="_blank">tensor expressions</a>” to define these operators. TVM has a ton of other really cool features like auto-tuning for specific data/hardware formats that are beyond the scope of what I really understand, but I would highly recommend reading the <a href="https://arxiv.org/pdf/2002.03794" rel="external nofollow noopener" target="_blank">DL compilers survey</a> for a high-level overview of TVM and other compilers.</li>
  <li>
<strong>MLIR</strong>. The <a href="https://mlir.llvm.org/getting_started/" rel="external nofollow noopener" target="_blank">Multi-Level Intermediate Representation (MLIR)</a> is an extension to LLVM<d-footnote>Again, sort of assuming you know what it is. In case you don’t, LLVM is a really powerful and language-independent library that features the LLVM IR. You would generally convert your language of choice into the LLVM IR, LLVM would perform a bunch of optimizations, then it would convert the IR into your hardware of choice. Before LLVM, dealing with compilers across different hardware was a pain in the ass. LLVM is also used for these deep learning compilers as well.</d-footnote> that essentially allows you to define your own IR / dialect based on existing MLIR dialects — in other words, you don’t have to define an IR completely from scratch. MLIR is extremely useful in the context of deep learning compilers, because we <strong>often care about multiple optimization passes at different abstractions</strong>, which MLIR gives you the flexibility to define. MLIR works well with a lot of the compilers / tools we will list below — to get started, I found this post pretty helpful: <a href="http://lastweek.io/notes/MLIR/" rel="external nofollow noopener" target="_blank">http://lastweek.io/notes/MLIR/</a>.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Examples of prominent specific-compilers.</strong> These compilers are technically general-use, but are mostly used to target specific devices or specific libraries. Unlike the frameworks above, they are highly optimized for specific use-cases and are much more useful as tools rather than personal development. If you’re not very interested in compilers, it is nice to know some of the stuff listed below.</p>

<ul>
  <li>
<strong>nvcc (NVIDIA, 2007).</strong> nvcc is NVIDIA’s compiler for CUDA to PTX (NVIDIA GPU’s assembly code). As far as I’m aware, a lot of the details about how what the compiler does under the hood are proprietary.</li>
  <li>
<strong>XLA (Google, 2017)</strong>. The accelerated linear algebra (XLA) compiler is mainly for linear algebra workloads in TensorFlow/Jax.  It also features a just-in-time (JIT) compiler and operates at the computation graph-level. The OpenXLA project designed it to be able to target other non-TPU hardware as well.</li>
  <li>
<strong>TensorRT (NVIDIA, 2019).</strong> <a href="https://github.com/NVIDIA/TensorRT" rel="external nofollow noopener" target="_blank">TensorRT</a> (and now <a href="https://github.com/NVIDIA/TensorRT-LLM" rel="external nofollow noopener" target="_blank">TensorRT-LLM</a>) are inference engines that target NVIDIA devices. Given a computational graph in PyTorch/Tensorflow or ONNX, these libraries apply a set of optimizations (e.g. layer fusion, quantization, kernel selection) on CUDA devices for low-latency inference.</li>
  <li>
<strong>PyTorch’s Compilers over the Years.</strong> PyTorch supports both eager execution and graph execution, and it compiles these separately. Recently, PyTorch 2.0 introduced the <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="external nofollow noopener" target="_blank">torch.compile()</a> decorator for easily applying JIT compilation to your code (with some restrictions of course). The PyTorch umbrella includes several different compilers such as the two-phase IR <a href="https://github.com/pytorch/glow" rel="external nofollow noopener" target="_blank">Glow</a> (2018), <a href="https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/" rel="external nofollow noopener" target="_blank">nvFuser</a> (2022), and the JIT compiler <a href="https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html" rel="external nofollow noopener" target="_blank">TorchDynamo</a> + <a href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747" rel="external nofollow noopener" target="_blank">TorchInductor</a>.</li>
  <li>
<strong>Triton IR (Philippe Tillet / OpenAI, 2021).</strong> Triton is a domain-specific language for programming NVIDIA GPU kernels in Python<d-footnote>If you’ve used Triton, you’ll notice the compute hierarchy is less granular than CUDA. Kernels operates at the block level, and there are specific functions for loading memory into threads. The other downside is the reliance on the Triton compiler when new hardware comes out, e.g. targeting H100 features like the TMA.</d-footnote>. By default, the <code class="language-plaintext highlighter-rouge">torch.compile()</code> function generates Triton code using TorchInductor. Triton has its own compiler, which converts Triton code into the MLIR-based Triton IR. The Triton-JIT compiler then optimizes this code and generates PTX code. I have found <a href="https://github.com/srush/Triton-Puzzles" rel="external nofollow noopener" target="_blank">Sasha Rush’s GPU Puzzles</a> to be quite useful (<a href="https://github.com/alexzhang13/Triton-Puzzles-Solutions" rel="external nofollow noopener" target="_blank">my solutions</a>). I also found the <a href="https://github.com/linkedin/Liger-Kernel" rel="external nofollow noopener" target="_blank">Liger Kernel</a> repository, which we talked about earlier, to be a well-written set of examples for learning Triton.</li>
</ul>

<p><strong>Remark.</strong> There is honestly a lot more to talk about regarding deep learning compilers, and compilers in general, but it is hard to motivate it at a high-level without going into details. There’s also a lot that goes into the design choices for specific optimizations, and I’m really not an expert on this stuff. I linked this earlier, but I did find <a href="https://arxiv.org/pdf/2002.03794" rel="external nofollow noopener" target="_blank">The Deep Learning Compiler: A Comprehensive Survey</a> to be extremely informative on the design choices of these compilers.</p>

<h2 id="part-iii2-the-era-of-scale-distributed-version">Part III.2: The Era of Scale (distributed version)</h2>
<p>Imagine that you are a {insert big tech company or unicorn startup} in 2020, and you are now a big believer in scale — you want to build, say, a trillion parameter model, but you now have a whole suite of new problems in the distributed setting. I previously mentioned adding more GPUs as an “obvious” solution to scaling models, but doing this is a lot harder than it sounds — a lot of work goes into <strong>minimizing various overheads</strong>, circumventing <strong>communication errors</strong>, and building <strong>fault-tolerant and stable</strong> algorithms for distributed workloads.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/24.png" style="width:90%" alt="compilers">
    <figcaption><b>Figure 24.</b> Differences between model parallelism and data parallelism. Model parallelism partitions a model across devices, and has potentially blocking operations. Data parallelism partitions along the batch dimension. <a href="https://medium.com/@minhanh.dongnguyen/megatron-lm-how-model-parallelism-is-pushing-language-models-to-new-heights-c21a5343e06a" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<h3 id="iii2a-data-parallelism">III.2.a: Data parallelism</h3>
<p>Suppose I have a <strong>1B parameter (~2GB)</strong> language model that I want to train on the <a href="https://huggingface.co/datasets/allenai/c4" rel="external nofollow noopener" target="_blank">C4 dataset</a> (~750 GB). One common approach to accelerating training is to increase the batch size to increase the training throughput by taking advantage of the GPU’s parallelism (e.g. <a href="https://arxiv.org/pdf/2407.21783" rel="external nofollow noopener" target="_blank">Llama 3</a> uses a <strong>batch size of each least 250K</strong>). Because we know that models make updates after batches of training, the naive approach is to put a copy of the model on each GPU and distribute the batch across multiple GPUs so it can fit in memory. Certain libraries like PyTorch have wrappers that handle distributing and gathering gradients across GPUs to make sure the model copies on each device are in sync<d-footnote>See the DistributedDataParallel module: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html</d-footnote>. The most common data parallel scheme is to distribute a large batch of samples $B$ across many devices, compute the forward and backwards passes, then sum and broadcast all gradients to all devices in an <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html" rel="external nofollow noopener" target="_blank">MPI-allreduce</a><d-footnote>There are a bunch of collective operations like allreduce that are used to communicate effectively across multiple nodes. For example, the NCCL operations can be found here: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</d-footnote> operation.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/25.png" style="width:90%" alt="minima">
    <figcaption><b>Figure 25.</b> Visual example of how sharp minima can be problematic when minimizing the training loss function — these issues can be attributed to a slight mismatch between the testing loss function and training loss function. <a href="https://arxiv.org/pdf/1609.04836" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/1609.04836" rel="external nofollow noopener" target="_blank">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> (Keskar et al., 2016<d-cite key="keskar2017largebatchtrainingdeeplearning"></d-cite>)</strong>. Data parallelism effectively provides a linear relationship between the number of available GPUs and the allowable batch size during training. This work empirically show that as we increase the batch size $B$, their models (applied to <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" rel="external nofollow noopener" target="_blank">MNIST</a> and CIFAR-10, both old and relatively small by today’s standards) begin converging to non-general solutions, which they attribute to large-batch solutions converging to sharp minima (e.g. areas of the loss landscape where the eigenvalues of $\nabla^2 f$ are large), and <strong>not</strong> to overfitting (see <strong>Figure 25</strong> above)<d-footnote>Computing the eigenvalues of the Hessian is hard, so actually in the original paper they approximate the sharpness measure, which you can read about in the original paper.</d-footnote>. So even if your model can fit on a single GPU, the effectiveness of data parallelism saturates as we scale. It has therefore become more interesting to cleverly parallelize our model computations across multiple devices.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii2b-model-parallelism">III.2.b: Model parallelism</h3>
<p>Like data parallelism, model parallelism is not necessarily that technically novel or interesting, but it is still extremely important and relevant today. Data parallelism relies on the model (+ optimizer states and data) fitting on a single GPU, but for large models this may not be possible (e.g. 400B parameter full-precision model is ~800GB just for model weights, far too big to fit on any GPU). <a href="#part-ii1-the-first-breakthrough-on-images">AlexNet</a>, for example, split the model across two GPUs in the original implementation, as they only had 3GB of RAM. Model parallelism is far more complex than data parallelism in that there are “blocking” steps — if we have a model with layer A which goes into layer B and we put the layers on different devices, we have to wait for layer A to finish before starting computation in layer B.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/1811.02084" rel="external nofollow noopener" target="_blank">Mesh-Tensorflow</a> (Shazeer et al., 2018<d-cite key="shazeer2018meshtensorflowdeeplearningsupercomputers"></d-cite>)</strong>. The core idea behind data parallelism is to split tensor computations along the batch dimension, which is free because model operations over these dimensions are entirely independent. In Mesh-Tensorflow, they propose an automatic strategy for <strong>splitting tensors along arbitrary dimensions</strong> (hence generalizing data &amp; model parallelism) and scheduling them across multiple devices. The idea is that we can define a meshgrid of processors to handle tensor transformations in parallel, so this method does not reduce waiting times due to causal sequences of operations.</p>

<p>Another similar term you will probably see a lot is “<strong>tensor parallelism</strong>”, and it’s basically a form of model parallelism where we partition the weights of a layer along a particular dimension and place them on different devices. <a href="https://arxiv.org/abs/1909.08053" rel="external nofollow noopener" target="_blank">Megatron-LM</a>, which we talk about in <a href="#iii2d-architecture-specific-parallelism">III.2.d</a>, relies heavily on tensor parallelism.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii2c-pipeline-parallelism">III.2.c: Pipeline parallelism</h3>

<p>Model parallelism &amp; Mesh TensorFlow suffer from significant downtime when dependencies are involved. For example, if we split a model into layer A and B, where the output of A is the input of B, the devices holding layer B are blocked until layer A is finished. Pipeline parallelism is basically like pipelining in computer architecture — we pass partially computed tensors to satisfy dependencies, while also keeping GPU utilization high.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/26.png" style="width:100%" alt="pipeline">
    <figcaption><b>Figure 26.</b> Pipeline parallelism increases GPU utilization for model parallelism schemes and reduces bubbling. <a href="https://arxiv.org/pdf/1811.06965" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/1811.06965" rel="external nofollow noopener" target="_blank">G-Pipe</a> (Huang et al., 2018<d-cite key="huang2019gpipeefficienttraininggiant"></d-cite>)</strong>. One of the first open-source pipeline parallelism works for deep learning, G-Pipe is extremely simple and intuitive. To avoid stalls, they simply schedule sequential “micro-batches” on each device, so if device B has to wait for device A, it can process an earlier micro-batch while waiting for device A to finish its micro-batch. Like pipelining, there are bubbles that occur in this simple process, but compared to model parallelism, it significantly increases GPU utilization. They naively handle the backwards pass by waiting for all forward pass micro-batches to finish, due to the reverse layer-order dependency of the backwards pass. Finally, they perform a synchronous model update across all devices.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://people.eecs.berkeley.edu/~matei/papers/2019/sosp_pipedream.pdf" rel="external nofollow noopener" target="_blank">PipeDream</a> (Narayanan et al., 2018<d-cite key="10.1145/3341301.3359646"></d-cite>)</strong>. PipeDream was a concurrent work that developed a slightly different strategy. In addition to adding pipeline stages, they also interleave available backwards computations (e.g. when the first microbatch finishes its forward pass) with scheduled forwards passes to reduce bubbling caused by the reverse layer-order dependencies of backpropagation. PipeDream also features an automatic work partitioner for roughly dividing each pipeline stage to be equal in computation time. I didn’t talk about this in the context of GPipe, but uneven pipeline stages causes bottlenecks and therefore extra stall time.</p>

<p>Some other follow-up works like <a href="https://arxiv.org/pdf/2006.09503" rel="external nofollow noopener" target="_blank">PipeDream-2BW (Narayanan, 2020)</a> and <a href="https://openreview.net/pdf?id=cw-EmNq5zfD" rel="external nofollow noopener" target="_blank">WPipe (Yang et al., 2022)</a> essentially minimize the stall / bubble time of the above methods, but are far more specific and still use the core idea that G-Pipe and Pipedream proposed.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii2d-architecture-specific-parallelism">III.2.d: Architecture-specific Parallelism</h3>
<p>This mini-section is somewhat overlapping with the previous two, as model and pipeline parallelism are not necessarily architecture-agnostic. It should be pretty clear that there are certain considerations like load balancing and how to partition the model that are difficult to optimize when the model architecture is unknown. There are many recent works that focus on parallelizing specific architectures for scale, especially transformers.</p>

<p><strong><a href="https://arxiv.org/abs/1909.08053" rel="external nofollow noopener" target="_blank">Megatron-LM</a> (Shoeybi et al., 2020<d-cite key="shoeybi2020megatronlmtrainingmultibillionparameter"></d-cite>)</strong>. The aforementioned distributed training frameworks have pretty complicated implementations, and have evolved over time to include extra optimizations as well. The core thesis of Megatron-LM is to reduce overhead communication costs and assign operators in a Transformer models purely by intuition, and they identify synchronization points (basically where the devices will stall) that they can remove. Since then, Megatron-LM has changed significantly to be a framework for scaling languages, with two subsequent works <a href="https://arxiv.org/abs/2104.04473" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2104.04473</a> and <a href="https://arxiv.org/abs/2205.05198" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2205.05198</a>, as well as a library called <a href="https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-core" rel="external nofollow noopener" target="_blank">Megatron-Core</a> for handling large-scale training.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii2e-multi-node-distributed-training">III.2.e: Multi-node distributed training</h3>
<p>We can generally get away with multi-GPU workloads on a single node (e.g. a <a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf" rel="external nofollow noopener" target="_blank">DGX A100 8x80GB</a> server) without having to deal with a scheduling algorithm or factoring node-to-node network bandwidth as a bottleneck, but as we <strong>start scaling even further to pre-training foundation models, we have to consider multi-node multi-GPU</strong> training frameworks.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/27.png" style="width:100%" alt="zero">
    <figcaption><b>Figure 27.</b> ZeRO-DP features 3 different stages of optimization, each of which partition more data across the devices. The base version of data parallelism makes copies of everything on every device, and each stage of ZeRO-DP partitions different types of data to reduce the overall memory footprint. <a href="https://arxiv.org/pdf/1910.02054" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/1910.02054" rel="external nofollow noopener" target="_blank">ZeRO</a> (Rajbhandari et al., 2020<d-cite key="rajbhandari2020zeromemoryoptimizationstraining"></d-cite>)</strong>. ZeRO cleans up most of the redundant memory footprint in data-parallel training schemes <strong>by partitioning across multiple devices / nodes</strong>. ZeRO is a family of optimizations separated into two classes: ZeRO-DP for “states”, and ZeRO-R for “residual memory”<d-footnote>The paper introduces residual memory as activations, temporary buffers, and fragmented memory, but this is basically like the constantly changing / temporary data.</d-footnote>.</p>

<ul>
  <li>
<strong>ZeRO-DP</strong> targets various types of memory such as optimizer states (stage 1), gradients (stage 2), and the actual parameters of the model (stage 3). The general strategy is for each device to be responsible for holding and updating a partition of these components in memory, while requesting certain partitions only when needed (updates are made with a final all-gather or reduce-scatter). For example, when partitioning the model parameters, instead of performing model parallelism, where layer A sits on device 1 and sends its outputs to layer B on device 2, device 1 will instead grab layer B from device 2 and compute it all on device.</li>
  <li>
<strong>ZeRO-R</strong> also centers around the partitioning strategy, but instead patches up a lot of the potential redundancies caused by ZeRO-DP. ZeRO-R handles activation checkpointing with a partitioning strategy similar to those found in ZeRO-DP (basically request it when you need it), but also uses a buffer to ensure requests are sufficiently sized while also handling memory fragmentation by pre-allocating contiguous memory chunks as needed.</li>
</ul>

<p>There are a lot of rich details regarding how each optimization is ZeRO is implemented using node communication primitives that can be found in the original paper. ZeRO has since evolved into a family of optimizations for multi-device deep learning workloads and is directly usable with multi-device deep learning libraries like <a href="https://www.deepspeed.ai/tutorials/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">deepspeed</code></a>.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/28.png" style="width:90%" alt="ringattention">
    <figcaption><b>Figure 28.</b> An example of how RingAttention partitions query and key/value blocks on different hosts and also how this can result in redundancies with a causal mask. <a href="https://arxiv.org/pdf/2311.09431" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2310.01889" rel="external nofollow noopener" target="_blank">RingAttention</a> (Liu et al., 2023<d-cite key="liu2023ringattentionblockwisetransformers"></d-cite>)</strong> [<a href="https://github.com/lucidrains/ring-attention-pytorch" rel="external nofollow noopener" target="_blank">unofficial code</a>]. When we increase the effective context window of a model, we start getting to the regime where a single attention operation has to be split across multiple devices. Recall from our <a href="#iii1d-fused-kernels-and-the-gpgpu">discussion of FlashAttention</a> that we can compute attention by splitting $Q$  and $K,V$ into blocks. The <a href="https://arxiv.org/abs/2305.19370" rel="external nofollow noopener" target="_blank">Blockwise Parallel Transformer (BPT)</a> takes this further by also fusing the subsequent feedforward layer with the attention layer, which operates independently on each $Q$ block<d-footnote>Allow me to clarify. When you look at FlashAttention, you’ll notice that the output block $O$ is computed by taking a block $Q$ with all the keys and values. In other words, $Q$ and $O$ are synced, and $K$ and $V$ are synced. Each FFN layer gets applied independently along the sequence dimension of the $O$ block, so we can apply it immediately when any $O$ block is computed.</d-footnote>. RingAttention uses the intuition from BPT with one more observation: for every output block $O$, we compute it using a query block $Q$ and all key/value blocks, and the order that we load $K/V$ blocks is entirely permutation invariant! Thus, we can form a “ring” of host devices that each handle one query block, while we move each $K/V$ block from host to host to compute the $O$ block so each query block will see each $K/V$ block exactly once in some arbitrary order. This scheme overlaps the communication cost of moving around the $K/V$ blocks with the BPT computation, effectively hiding most of the latency that a naive distributed Transformer would have.</p>

<p><strong><a href="https://arxiv.org/abs/2311.09431" rel="external nofollow noopener" target="_blank">StripedAttention</a> (Brandon et al., 2023<d-cite key="brandon2023stripedattentionfasterring"></d-cite>)</strong>
StripedAttention is an extension of RingAttention that avoids redundancies caused by causal attention masks — instead of placing contiguous $K/V$ blocks on each device, they shuffle the keys/values to avoid completely masked out blocks (see Figure 28).</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii2f-libraries-for-distributed-deep-learning-workloads">III.2.f: Libraries for distributed deep learning workloads.</h3>
<p>Multi-GPU and multi-node algorithms like ZeRO have been integrated into libraries for developers to use. The community has moved extraordinarily fast on producing libraries for multi-device training and inference, making it <strong>possible for people with no knowledge to use multiple devices</strong>. In this section, I want to talk a little bit about those libraries, as well as provide some context for what is going on under the hood. We begin with a simple example of how to run basic distributed training in PyTorch.</p>

<p><strong><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" rel="external nofollow noopener" target="_blank">PyTorch example</a>.</strong>  In PyTorch, we start by initializing a process group on each device that defines a <strong>master address/port</strong>, its <strong>device rank</strong>, the <strong>world size</strong>, and a communication <strong>backend</strong>.</p>

<ul>
  <li>The <strong>master address</strong> and <strong>port</strong> from the master node, which generally controls the whole distributed system, is set across all nodes.</li>
  <li>The <strong>device rank</strong> or world rank is a unique identifier in $\mathbb{N}$ for each device in the distributed network. The <strong>local rank</strong> is the identifier of a process within a node (e.g. gpu:0), and the <strong>world size</strong> is the total number of devices.</li>
  <li>The communication <strong>backend</strong> is the protocol that defines how messages are sent and received across nodes and devices, as well as the available communication collectives (e.g. <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html" rel="external nofollow noopener" target="_blank">send, recv, all_reduce, all_to_all, reduce_scatter</a>, etc.).</li>
</ul>

<d-code block="" language="python" style="font-size:0.7em">
# Define node-specific constants
os.environ['MASTER_ADDR']= '127.0.0.1'
os.environ['MASTER_PORT']= '01134'
torch.distributed.init_process_group(backend, rank=rank, world_size=size)
</d-code>

<p>Modern libraries like <code class="language-plaintext highlighter-rouge">deepspeed</code> will make these primitives a lot easier for you, and will even make launching these applications with their <a href="https://aws.amazon.com/what-is/cli/" rel="external nofollow noopener" target="_blank">CLI tools</a> a lot simpler (you’ll probably just have to run <code class="language-plaintext highlighter-rouge">deepspeed program.py ...</code>).  If you were to manually run a distributed workload (e.g. with <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="external nofollow noopener" target="_blank">PyTorch’s DistributedDataParallel</a> or by defining your own sends and receives), you would typically have to run the program on each separate node while specifying their individual ranks.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Node communication backends.</strong> Under the hood, multi-node and multi-GPU workloads need to communicate and send data. Most libraries take care of this for you, but they will often have you define a communication backend — each of these serves a slightly different purpose and have various tradeoffs.</p>

<ul>
  <li>
<strong><a href="https://github.com/NVIDIA/nccl" rel="external nofollow noopener" target="_blank">nccl</a></strong>. NCCL is NVIDIA’s communication protocol specifically designed for inter-(NVIDIA)GPU communication. It is the recommended backend for most deep learning applications on NVIDIA devices.</li>
  <li>
<strong><a href="https://github.com/facebookincubator/gloo" rel="external nofollow noopener" target="_blank">gloo</a></strong>. Gloo is more flexible for supporting CPU-GPU communication as well as GPU-GPU communication, and is often noted to be more useful for CPU-intensive distributed workloads.</li>
  <li>
<strong><a href="https://en.wikipedia.org/wiki/Message_Passing_Interface" rel="external nofollow noopener" target="_blank">mpi</a></strong>. MPI has been the standard backend for most high-performance computing (HPC) applications.</li>
</ul>

<p><strong>Some relevant modern libraries.</strong> You can definitely code up a multi-GPU or multi-node job in PyTorch or TensorFlow, and most experienced developers choose to do this in favor of flexibility. However, there are many choices for libraries / CLI tools that handle multi-device training for you, and we list some in <a href="#a2-large-training-and-finetuning-frameworks">A.2: Large training / finetuning frameworks</a>.</p>

<h2 id="part-iii3-scaling-laws">Part III.3: Scaling Laws</h2>
<p>Characterizing model performance as a function of scale is a useful signal for whether any advances in efficient deep learning are even important. There are even works that look into predicting training curves, but in this section we mainly focus on observed empirical scaling laws and what they imply. All of the following scaling laws focus on characterizing the <strong>generalization / test loss in (nats/token)</strong>, which is just the average negative log-likelihood with respect to the evaluation set. To keep this post focused on efficiency, I will mainly be glossing over results and leaving it to the reader to learn more about specific constant ranges or empirical findings.</p>

<p><strong><a href="https://arxiv.org/abs/1712.00409" rel="external nofollow noopener" target="_blank">Deep learning Scaling is Predictable, Empirically</a> (Hestness et al., 2017<d-cite key="hestness2017deeplearningscalingpredictable"></d-cite>)</strong>. One of the first papers to present empirical scaling laws on a wide range of tasks (image, language, machine translation, speech) as a function of the training set size. They model test loss as a function of dataset size:</p>

<p>
$$
\mathcal{L}(D) = C \cdot D^{\alpha} + \gamma 
$$
</p>

<p>and find that existing theoretical works estimate these constants incorrectly — prior works estimate $\alpha \sim -0.5$, while the empirical ranges they found were in $[-0.35, -0.07]$. Interestingly, they find in their experiments that the power law exponent $\alpha$ changes across tasks, while $C$ changes based on model architecture and choice of optimizers.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/29.png" style="width:90%" alt="power law">
    <figcaption><b>Figure 29.</b> Single-variable power-law functions of the test loss align closely with the empirical results in (Kaplan et al., 2020).<a href="https://arxiv.org/pdf/2001.08361" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2001.08361" rel="external nofollow noopener" target="_blank">Scaling Laws for Neural Language Models</a> (Kaplan et al., 2020<d-cite key="kaplan2020scalinglawsneurallanguage"></d-cite>)</strong>. This paper proposes scaling laws across dataset size $D$, model size $N \in [768, 10^9]$, and compute budget $C \in [10^{12}, 10^{21}]$ FLOPS. They focus mainly on Transformer decoders on trained on <a href="https://openwebtext2.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">WebText2</a>, and they first analyze single-variable scaling laws by fixing $2/3$ of the above variables at a “sufficient level” and analyzing the third. They estimate in these models that each parameter costs roughly 6 FLOPS per token in the forward + backwards pass. These scaling laws are a power-law function of the test loss:</p>

<p>
$$
\mathcal{L}(X) = \left(\frac{X_0}{X}\right)^{\alpha} + \gamma, \quad X \in \{D,N,C\}
$$
</p>

<p>They notably discover through experimentation that:</p>
<ul>
  <li>Counting embedding parameters for $N$ does not result in the nice power-law relationship we would expect, but excluding them does.</li>
  <li>Performance depends strongly on model scale and weakly on model shape, which is consistent with the findings of (<strong>Hestness et al., 2017<d-cite key="hestness2017deeplearningscalingpredictable"></d-cite>)</strong>.</li>
  <li>Increasing $N$ and $D$ at a fixed rate $N^{\beta} / D$ is necessary to observe performance gains in the scaling laws.</li>
</ul>

<p>They also derive test loss as a function of multiple variables, which are consistent analytically when you take the limit of one variable (think of it as a form of <a href="https://statproofbook.github.io/D/prob-marg.html" rel="external nofollow noopener" target="_blank">marginalization</a>). Using this function, they propose an optimal allocation of resources given a fixed compute budget. For specific coefficients and rationale for their fitting functions, I would highly recommend reading the original paper — they have a lot more conclusions and experiments than what I’ve discussed above!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2203.15556" rel="external nofollow noopener" target="_blank">Chinchilla</a> (Hoffmann et al., 2022<d-cite key="hoffmann2022trainingcomputeoptimallargelanguage"></d-cite>)</strong>. This landmark paper in neural scaling laws for LLMs is a collection of over 400 large-scale experiments (all Transformers with a cosine schedule learning rate and trained on one epoch), entering the <strong>foundation model range (70M - 16B parameters, 5B - 500B tokens)</strong> that <strong>(Kaplan et al., 2020<d-cite key="kaplan2020scalinglawsneurallanguage"></d-cite>)</strong> does not touch on. From an efficiency perspective, they are interested in optimal cost budgets, i.e.</p>

<p>
$$
N^*, D^* = \underset{\text{FLOPS}(N,D)\leq C}{\text{argmin}} \mathcal{L}(N,D)
$$
</p>

<p>In their experiments, they vary both the number of training examples for a fixed model size and the model size for a fixed FLOP budget, and fit (+ motivate) the scaling law according to the function (with fitting parameters $A_0, A_1, \alpha, \beta, \gamma$).</p>

<p>
$$
\mathcal{L}(N, D) = \frac{A_0}{N^{\alpha}} + \frac{A_1}{D^{\beta}} + \gamma 
$$
</p>

<p>Under their fitted power law, they set a constraint budget $6ND \leq C$ for their proposed compute-optimal Chinchilla model. In the domain of large language models, scaling law papers are hard to come by because of the sheer cost of running experiments. Other works like <a href="https://arxiv.org/abs/2206.14486" rel="external nofollow noopener" target="_blank">Beyond neural scaling laws</a>, <a href="https://arxiv.org/abs/2210.11399" rel="external nofollow noopener" target="_blank">Transcending Scaling Laws with 0.1% Extra Compute</a>, and <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d89448b63ce1e2e8dc7af72c984c196-Abstract-Conference.html" rel="external nofollow noopener" target="_blank">Scaling Data-Constrained Language Models</a> explore how scaling law constants change under different datasets, constraints, and model assumptions. From an efficiency standpoint, there will always be interest in deriving the upper-bound of power law constants $\alpha,\beta$.</p>

<h2 id="part-iii4-revisiting-downwards-scaling">Part III.4: Revisiting downwards scaling</h2>
<p>A natural analogue for neural scaling laws is the lower bound of compute necessary to achieve some level of model performance. In the era of foundation models and Transformers, model compression methods have evolved to deal with the challenges of large-scale models trained on huge datasets.</p>

<h3 id="iii4a-small-language-models-slms">III.4.a: Small Language Models (SLMs)</h3>
<p>With foundation models getting too large to fit on affordable hardware, there has been a growing interest in how to train a small language model that performs the same as a large language model. A lot of the subsequent sections are relevant to training SLMs from scratch and from an LLM.</p>

<p><strong>The Phi models.</strong> The Phi models are a series of open-source SLMs from Microsoft Research designed to emphasize the value of high-quality training data. This idea may contradict the scaling laws we discussed in <a href="#part-iii3-scaling-laws">the previous section</a>, but actually the scaling laws bake in assumptions such as properties of the data distribution, types of models used, etc. that aren’t universally covered.</p>

<ul>
  <li>
<strong><a href="https://arxiv.org/abs/2306.11644" rel="external nofollow noopener" target="_blank">Phi-1</a> (Gunasekar et al., 2023<d-cite key="gunasekar2023textbooksneed"></d-cite>)</strong>. phi-1 is a 1.3B parameter model trained on <strong>6B tokens of high-quality scientific/textbook material</strong> for coding. Compared to other models at the time, which were generally 10x bigger and trained on 100x more tokens, it displayed near-SOTA performance on <a href="https://github.com/openai/human-eval" rel="external nofollow noopener" target="_blank">HumanEval</a> (Pass@1) and <a href="https://arxiv.org/abs/2108.07732" rel="external nofollow noopener" target="_blank">MBPP</a> (Pass@1), which were the primary coding benchmarks at the time.</li>
  <li>
<strong><a href="https://arxiv.org/abs/2309.05463" rel="external nofollow noopener" target="_blank">Phi-1.5</a> (Li et al., 2023<d-cite key="li2023textbooksneediiphi15"></d-cite>)</strong>. As a follow up, they build more 1.3B parameter models trained on “textbook-quality” data generated by LLMs and show near-SOTA performance on reasoning tasks beyond coding! It’s unclear how the learned distribution is affected by this synthetic training data trick, but for Phi-1.5 it seems to work fairly well.</li>
  <li>
<strong><a href="https://arxiv.org/abs/2404.14219" rel="external nofollow noopener" target="_blank">Phi-2, Phi 3, Phi-3.5</a> (Microsoft, 2024<d-cite key="abdin2024phi3technicalreporthighly"></d-cite>)</strong>. Subsequent iterations of the Phi models were larger (~2-3B parameters) and trained on significantly more high-quality filtered &amp; synthetic “textbook” data. They demonstrate the capabilities of these models across language, vision, and multi-modal tasks, and also introduce a mixture-of-experts version (~6B params) to compete against models of a similar size like LLaMA 3.1, <a href="https://mistral.ai/news/mixtral-of-experts/" rel="external nofollow noopener" target="_blank">Mixtral</a>, <a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" rel="external nofollow noopener" target="_blank">GPT-4o-mini</a>, and <a href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" rel="external nofollow noopener" target="_blank">Gemini-1.5-Flash</a>.</li>
</ul>

<p>Similarly sized models follow the same training recipe (i.e. really “high quality” data seems to affect the power law constants positively), but not all of them are open-source.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/30.png" style="width:100%" alt="sheared llama">
    <figcaption><b>Figure 30.</b> In Sheared LLaMa, they preserve locality and dense matmuls while pruning large language models by pruning at a higher abstraction. The diagram above shows an example of pruning out attention heads and hidden dimensions without the need for sparse kernels. <a href="https://arxiv.org/pdf/2310.06694" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2310.06694" rel="external nofollow noopener" target="_blank">Sheared LLaMA</a> (Xia et al., 2023<d-cite key="xia2024shearedllamaacceleratinglanguage"></d-cite>)</strong>.  Existing pruning techniques mentioned in <a href="#ii6a-model-pruning">II.6.a: Model Pruning</a> have found little success in the large language model space due to the lack of hardware-aware structure. Instead, in this work they prune at a higher abstraction such as the <strong>number of layers, attention heads, and hidden dimensions</strong> to enable hardware-aware structured pruning for language models. They also introduce “dynamic batch loading”, which is an online optimization-style problem for adjusting the proportion of data from each domain that is added to the training batch. I am hopeful that more theoretically motivated versions of this technique will be useful for faster convergence.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Knowledge Distillation (KD).</strong><d-footnote>I apologize for keeping this section brief. I think KD is a very rich field, but there just isn’t much to talk about except how to improve matching the distribution of one model to another. In terms of “efficiency”, I don’t have that much to say about the topic.</d-footnote> Knowledge distillation emerged around the time when other methods like pruning and quantization were popularized, but the more interesting ideas like black-box knowledge distillation came about due to the closed nature of a lot of large models. Generally, the idea is to start with a large language model (teacher) and produce a small language model (student) by “distilling” the behavior of the large language model into the small language model.</p>

<ul>
  <li>
<strong>White-box KD</strong> means we have access to the <strong>logits/distribution</strong> of the large teacher model, and our optimization objective is to align the distribution of the student to the distribution of the teacher (e.g. through <a href="https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf" rel="external nofollow noopener" target="_blank">KL divergence</a>). <a href="https://openreview.net/forum?id=5h0qf7IBZZ" rel="external nofollow noopener" target="_blank">MiniLM</a> claims that KL is not the right optimization objective for language, but works like <a href="https://arxiv.org/abs/2308.02019" rel="external nofollow noopener" target="_blank">Baby LLaMA</a> have shown that standard white-box KD can yield good results.</li>
  <li>
<strong>Black-box KD</strong> is interesting in the era of large models because many SOTA LLMs are available through APIs. One of the more interesting techniques is <a href="https://arxiv.org/abs/2310.16944" rel="external nofollow noopener" target="_blank">Zephyr</a>, where they fine-tune a small open-source model with <a href="https://huggingface.co/docs/trl/main/en/sft_trainer" rel="external nofollow noopener" target="_blank">SFT</a> + <a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">DPO</a> by generating <code class="language-plaintext highlighter-rouge">(instruction, response)</code> pairs from a larger closed-source model. Given the fact that people train their models on synthetic model-generated content (e.g. GPT-4), it is not that surprising that black-box KD works in this way<d-footnote>As a side note, I wonder what this implies about the distribution of “language” we are learning and what kind of space it lies on.</d-footnote>.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii4b-modern-quantization-techniques">III.4.b: Modern quantization techniques</h3>
<p>We revisit the topic of quantization and some popular research directions related to it. Quantization is especially interesting for language models because it can be made efficient for modern hardware without affecting the architecture of the model. However, <strong>unless the entire model is quantized, quantization methods still suffer from a lack of hardware support for speed-ups</strong>.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/31.png" style="width:70%" alt="llm.int8()">
    <figcaption><b>Figure 31.</b> In LLM.int8(), the authors observe that when scaling OPT beyond 2.7B, naive post-training 8-bit quantization collapses. They attribute model collapse to outlier features that cause large quantization errors to propagate throughout the model. <a href="https://arxiv.org/abs/2208.07339" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2208.07339" rel="external nofollow noopener" target="_blank">LLM.int8()</a> (Dettmers et al., 2022<d-cite key="dettmers2022llmint88bitmatrixmultiplication"></d-cite>)</strong>. While FP32 and FP16 (+ mixed precision training) have been shown to work fairly well with LLMs, this work was the first to perform 8-bit (INT) quantization for large-scale LLMs. The authors discover that <strong>&lt;1% of input features have a high variance/magnitude, which causes a large quantization error</strong> when going down to 8-bit representations. They also find that this “outlier” phenomenon occurs along the same hidden dimensions across most sequences, so they separate these outliers out using an outer-product notation for matrix multiplication. More formally, for outlier dimensions $O$,</p>

<p>
$$
XW = \sum_{o \in O} X_{:,o}^{fp16}W^{fp16}_{o,:} + \sum_{k \notin O} X_{:,k}^{int8}W^{int8}_{k,:}
$$
</p>

<p>Lastly, they assign quantization constants to each <strong>row</strong> of the input and each <strong>column</strong> of the weight matrix (vector-wise quantization). Interestingly, they attribute model collapse in <strong>Figure 31</strong> to quantization errors propagating across all layers in larger models.<d-footnote>The author of LLM.int8() and QLoRA (Tim Dettmers) has also built the [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library for quantizing / fine-tuning LLMs using these techniques. It is an extremely simple and popular wrapper around Huggingface transformers for quantizing your models!</d-footnote></p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2210.17323" rel="external nofollow noopener" target="_blank">GPT-Q</a> (Frantar et al., 2022<d-cite key="frantar2023gptqaccurateposttrainingquantization"></d-cite>)</strong>. Following the same trend as before, GPT-Q quantizes LLMs to the <strong>4-bit regime</strong> while stably reducing generalization perplexity for large models. They focus on layer-wise quantization, meaning they isolate each layer and do not consider layer-to-layer effects. Following the <a href="https://arxiv.org/abs/2208.11580" rel="external nofollow noopener" target="_blank">Optimal Brain quantization framework</a>, they minimize the following quantization error:</p>

<p>
$$
\text{argmin}_{W^q} \| WX - W^q X \|_2^2
$$
</p>

<p>Intuitively, what we’re doing here is quantizing a row of the weights, then adjusting the full precision weights to minimize the error, and iteratively performing this update. There are closed form solutions to this iterative update by Taylor expanding the error above that were originally derived in <a href="https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html" rel="external nofollow noopener" target="_blank">(Optimal Brain Surgeon, 1992)</a>, but <strong>GPT-Q modifies/approximates the algorithm to maximize GPU utilization</strong>. Like other quantization methods at the time, quantizing the rows at different granularity did not enable any speed-ups on GPUs.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/32.png" style="width:100%" alt="AWQ">
    <figcaption><b>Figure 32.</b> Activation-aware weight quantization (AWQ) is far more effective than naive weight quantization without any fancy machinery.<a href="https://arxiv.org/abs/2306.00978" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2306.00978" rel="external nofollow noopener" target="_blank">Activation-Aware Weight Quantization (AWQ)</a> (Lin et al., 2023<d-cite key="lin2024awqactivationawareweightquantization"></d-cite>)</strong>. The authors observe in experiments that only a <strong>small percentage of weights in an LLM are “salient”</strong>, meaning they are extremely sensitive to quantization. Furthermore, they observe that the saliency metric is dependent on the input data (i.e. the activation resulting from the weight times the input). Thus, prior to the post-quantization step, they sample a subset of the original data distribution and find the high-variance activations to determine salient weights (see <strong>Figure 32.b</strong>). Instead of keeping these salient weights at full precision<d-footnote>From a hardware perspective it’s always hard to intermix weights at different precisions because 1) you need kernels that can handle this and 2) the way it’s stored in memory is not convenient — imagine implementing a C-array that can take multiple types. Indexing into the array with pointers would be a pain.</d-footnote>, they find in theory that adding a computed scaling factor can reduce quantization error without affecting the range of representable values (see <strong>Figure 32.c</strong>). They demonstrate their method by using 4-bit AWQ on LLaMA-2 70B to deploy on a single <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/" rel="external nofollow noopener" target="_blank">NVIDIA Jetson Orin 64GB</a>.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/33.png" style="width:90%" alt="AWQ">
    <figcaption><b>Figure 33.</b> The fact that 1-bit/ternary weights have been shown to work for LLMs is cool, but it also features a significantly simplified relationship between the input and the weights — no scalar multiplication! <a href="https://arxiv.org/abs/2310.11453" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2310.11453" rel="external nofollow noopener" target="_blank">BitNet</a> (Wang et al., 2023<d-cite key="wang2023bitnetscaling1bittransformers"></d-cite>)</strong>. Another research direction that emerged was <a href="https://pytorch.org/blog/quantization-aware-training/" rel="external nofollow noopener" target="_blank">quantization-aware training</a>, where a model stores weights and gradient updates in full precision, but computes the forward pass in the quantized regime (a straight-through estimator is used for gradient computation). BitNet replace all linear layers (e.g. <code class="language-plaintext highlighter-rouge">nn.Linear</code> or just the $W$ matrices) with rounded 1-bit variants (i.e. $W_{i,j} \in $ { $-1,1$ }) and quantize the activations to 8-bits with absmax quantization. While BitNet was a cool experiment, the <strong>results were subpar compared to other existing quantization techniques</strong>, and the <strong>produced model was not any faster on existing hardware</strong> than a standard LLM.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2402.17764" rel="external nofollow noopener" target="_blank">BitNet b1.58</a> (Ma et al., 2024<d-cite key="ma2024era1bitllmslarge"></d-cite>)</strong>. A follow-up and <strong>arguably more successful variant of BitNet</strong> was the <strong>ternary</strong> version (i.e. $W_{i,j} \in$ { $-1,0,1$ }). The recipe is basically the same, except they compare to a half-precision LLaMA {1.3B, 3B, 7B, 13B, 70B}, and demonstrate better / comparable performance on a wide range of language reasoning tasks, as well as <strong>significantly faster throughput (up to 10x)</strong> and <strong>less active memory usage (up to 4x reduction)</strong><d-footnote>I have very little intuition as to why this paper does so much better than BitNet (maybe something key about LLaMA models?) and I think this paper should do a better job of explaining it as well. As much as I want to believe something like this works, it seems almost too good to be true in most practical settings. I hope some more follow ups investigate the “whys” that this paper leaves open.</d-footnote>!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2211.10438" rel="external nofollow noopener" target="_blank">SmoothQuant</a> (Xiao et al., 2023<d-cite key="xiao2024smoothquantaccurateefficientposttraining"></d-cite>)</strong>. Most of the <strong>aforementioned methods focused more on memory reductions rather than speed improvements</strong>. Like other papers, the authors observe that outliers cause a lot of problems for quantization schemes. SmoothQuant chooses to work entirely in the <strong>quantized regime for both weights and activations</strong> without needing to dequantize anything. Because we can’t control outliers in the activations (this is input dependent) but we can control the initial distribution of weights, SmoothQuant adds a per-channel scaling factor based on a calibration set<d-footnote>Similar to what AWQ does (from the same lab), they use calibration sets as an approximation of the data distribution.</d-footnote> to scale down outlier activation channels, and a corresponding inverse per-channel scaling factor to the weight matrix. Effectively, they squash outliers in the inputs by introducing some outliers to the weights, which they argue is better than large outliers.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/smoothquant.png" style="width:60%" alt="smoothquant">
</center>
</figure>

<p>Under this scheme, we never need to go back and forth between different precisions, so we can directly apply low-precision kernels that enable speed-ups!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii4c-sparse-parameters">III.4.c: Sparse Parameters</h3>
<p>One research direction that hasn’t really taken off for the past few years is <strong>introducing sparsity or sparse decompositions to model parameters</strong>. We mentioned in <a href="#iii1a-shaving-complexity-through-approximate-methods">III.1.a: Shaving complexity: Approximate Methods</a> that sparse attention methods were just not efficient on modern parallel processors, which is not entirely true(-ish).</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/34.png" style="width:90%" alt="sparsity masks">
    <figcaption><b>Figure 34.</b> Different attention masks (dense and sparse) that can be written in a fused FlashAttention-like CUDA kernel on parallel processors. <a href="https://hanlab.mit.edu/blog/block-sparse-attention" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2309.17453" rel="external nofollow noopener" target="_blank">StreamingLLMs with Attention Sinks</a> (Xiao et al., 2023<d-cite key="xiao2024efficientstreaminglanguagemodels"></d-cite>)</strong>. It should be somewhat clear by now that pre-defined sparsity patterns can be made efficient on GPUs. In this work, they return to <a href="https://paperswithcode.com/method/sliding-window-attention" rel="external nofollow noopener" target="_blank">window attention</a> masks (causal mask that can only attend back a certain length) and add the ability to attend to a fix (set of) “attention sink” tokens, which they hypothesize contain global information due to the inherent structure of the attention mechanism. Furthermore, the authors develop efficient fused kernels in <a href="https://hanlab.mit.edu/blog/block-sparse-attention" rel="external nofollow noopener" target="_blank">https://hanlab.mit.edu/blog/block-sparse-attention</a> for efficiently handling these sparse patterns.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong>Sparse factorizations.</strong> One really interesting direction a few years back was factorizing model layers into sparse parameters, but ultimately it didn’t really take off. I am hopeful that people continue working on this direction, because derivable factorizations can say something about how our models work.</p>

<ul>
  <li>
<strong><a href="https://arxiv.org/abs/1903.05895" rel="external nofollow noopener" target="_blank">Butterfly Matrices</a> (Dao et al., 2019<d-cite key="dao2020learningfastalgorithmslinear"></d-cite>)</strong>. In this work, they show that a large class of structured matrices (e.g. <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" rel="external nofollow noopener" target="_blank">FFT</a>, <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform" rel="external nofollow noopener" target="_blank">DFT</a> live in this family) can be recursively factorized into sparse matrices with a nice <a href="https://linear.axler.net/BlockDiagonal.pdf" rel="external nofollow noopener" target="_blank">block diagonal</a> structure. While the implementations are not hardware friendly, these factorizations theoretically lead to a reduced number of operations and memory-footprint.</li>
  <li>
<strong><a href="https://proceedings.mlr.press/v162/dao22a.html" rel="external nofollow noopener" target="_blank">Monarch Matrices</a> (Dao et al., 2022<d-cite key="pmlr-v162-dao22a"></d-cite>)</strong>. As a follow-up, they derive a less-expressive class of matrices with hardware-friendly factorizations. Despite now being practically interesting, I haven’t seen much follow up work in this area in recently.</li>
</ul>

<h2 id="part-iii5-what-about-model-inference">Part III.5: What about model inference?</h2>
<p>The introduction of ChatGPT (2022) made it clear that building infrastructure to support querying large models ( i.e. model serving) was a necessary research direction. In addition to the compiler optimizations offered by inference engines like TensorRT for speeding up model code, people also began thinking about how to handle and schedule batches of user requests. The primary considerations were <strong>minimizing the latency of each user request</strong>, and <strong>maximizing the throughput of processing all user requests</strong>. Furthermore, due to the nature of KV-caching that we discussed in <a href="#iii10-early-insights">III.1.0: Early Insights</a>, these systems generally have to distinguish between the <a href="https://quic.github.io/cloud-ai-sdk-pages/1.12/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#prefill-stage" rel="external nofollow noopener" target="_blank">pre-filling</a> stage, where an initial prompt is fed into the model and all keys/queries/values are computed, and the <a href="https://quic.github.io/cloud-ai-sdk-pages/1.12/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#decode-stage" rel="external nofollow noopener" target="_blank">decoding phase</a>, where cached KVs can be re-used, and only one new query token is considered.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://github.com/ggerganov/llama.cpp" rel="external nofollow noopener" target="_blank">llama.cpp (Gerganov, 2022)</a></strong>. One of the coolest solo projects by <a href="https://github.com/ggerganov" rel="external nofollow noopener" target="_blank">Georgi Gerganov</a> is a pure C++ implementation of the <a href="https://en.wikipedia.org/wiki/Llama_(language_model)" rel="external nofollow noopener" target="_blank">LLaMA family</a> that optimizes for non-GPU devices (it now supports GPUs). It has since become a standard tool for running model inference on a variety of language models, and is extremely simple to use with its CLI. The downside is that adapting this code for custom LLMs is difficult without a strong understanding of the underlying implementation.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii5a-generative-model-serving">III.5.a: Generative model serving</h3>
<p>The most naive form of model serving for generative Transformers is to batch a bunch of requests, process them, then distribute the results back to each user. There are a lot of annoying considerations like <strong>non-uniform length prompts</strong>, <strong>non-uniform length generations</strong>, and how to <strong>handle the KV cache in memory</strong> (which is not small!) that people quickly began figuring out in the past two years.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/35.png" style="width:70%" alt="orca">
    <figcaption><b>Figure 35.</b> In order to optimize token-level scheduling, Orca exploits an observation that linear layers can be arbitrarily batched, while attention operations cannot, so they selectively batch operations to enable scheduling requests of different lengths. <a href="https://www.usenix.org/system/files/osdi22-yu.pdf" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://www.usenix.org/conference/osdi22/presentation/yu" rel="external nofollow noopener" target="_blank">Orca</a> (Yu et al., 2022<d-cite key="280922"></d-cite>)</strong>. One of the <strong>first open-source engines for model serving optimized for throughput</strong>. Given a batch of requests, their <strong>scheduler works at the token-level (they call it iteration-level)</strong>, meaning it doesn’t care if two requests were launched at different times. Furthermore, they notice that certain operations in a Transformer in non-batchable requests (e.g. they’re in different stages or of different lengths) can actually be batched — any linear transforms can be batched regardless of length (see <strong>Figure 35</strong>).</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/36.png" style="width:70%" alt="sparsity masks">
    <figcaption><b>Figure 36.</b> Prior to vLLM, most serving engines would pre-allocate a buffer in DRAM for the KV cache to live on, resulting in several forms of memory fragmentation and insufficient memory usage. Reserved inefficiency is when a smaller batch request could be using memory that a larger request will later use, but can’t because it’s pre-allocated. Internal fragmentation occurs when memory was pre-allocated but is never used. External fragmentation is your typical malloc memory fragmentation, where small pockets of contiguous memory are free but inaccessible because the KV cache is always larger. <a href="https://arxiv.org/pdf/2309.06180" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2309.06180" rel="external nofollow noopener" target="_blank">vLLM and PagedAttention</a> (Kwon et al., 2023<d-cite key="kwon2023efficientmemorymanagementlarge"></d-cite>)</strong>. Keeping the KV cache on the same device is critical for keeping model throughput high, as it avoids overhead communication costs. However, prior works like Orca handle the KV cache naively — they generally pre-allocate a fixed length memory buffer for the KV cache, which causes several memory inefficiencies highlighted in <strong>Figure 36</strong>. Furthermore, these methods have no way of sharing KV caches for shared prefixes across multiple requests. PagedAttention mitigates these issues by introducing ideas from virtual memory in an operating system — they <strong>block up the KV cache into equal and fixed size chunks and use a translation table to map them to physical DRAM</strong>. Equivalent chunks in different requests get mapped to the same physical memory, enabling memory sharing. While the KV blocks are not contiguous in physical memory, the elements in the block are locally contiguous and internal and external fragmentation are significantly reduced. <strong>vLLM is a serving engine on top of PagedAttention that operates at the request level</strong> (batches according to request arrival) and handles the virtual and physical KV cache for a variety of common decoding methods on single and distributed hardware.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<p><strong><a href="https://arxiv.org/abs/2403.02310" rel="external nofollow noopener" target="_blank">Sarathi-serve</a> (Agrawal et al., 2024<d-cite key="agrawal2024tamingthroughputlatencytradeoffllm"></d-cite>)</strong>. Prefilling (high latency, high GPU utilization) and decoding (low latency, low GPU utilization) are difficult to schedule together, but serving systems will often having many concurrent requests at either stage. The authors observe that when optimizing for throughput or greedily scheduling prefills first, there is a tradeoff between the <a href="https://arxiv.org/html/2407.07000v1#:~:text=TBT%20%3A%20Time%20Between%20Tokens%20(TBT,of%20the%20model%20by%20users.)%20and%20the%20overall%20throughput%20of%20the%20model.%20Furthermore,%20certain%20**scheduling%20behavior%20can%20cause%20requests%20to%20get%20stalled%20because%20they%20are%20forced%20to%20wait%20for%20other%20requests%20to%20finish**%20first.%20Sarathi-serve%20walks%20in%20the%20middle%20by%201" rel="external nofollow noopener" target="_blank">time-between-token (TBT)</a> chunking prefills to interleave requests at a finer granularity and 2) interleaving ongoing decodes with other requests to prevent stalling ongoing requests. <strong>tldr;</strong> <em>if you optimize too much for throughput, you’re inevitably going to make some requests really slow. Sarathi-serve tries to make sure no request gets stalled for too long while still maximizing throughput.</em></p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="iii5b-fast-decoding-strategies">III.5.b: Fast decoding strategies</h3>
<p>We have mentioned over and over again that many Transformer computations are memory-bound, and this is <strong>especially true for model inference.</strong> While we can increase inference throughput using the methods in the previous section, the latency is lower-bounded. A new research direction on fast decoding strategies has emerged to push this lower bound down.</p>

<p><strong><a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Speculative decoding</a> (Leviathan et al., 2022<d-cite key="leviathan2023fastinferencetransformersspeculative"></d-cite>)</strong>. The core idea is to sample tokens from a cheaper “draft” model $q(x_{&lt;t})$, and use a cute probability trick to make sure the distribution we sample from is actually the large model $p(x_{&lt;t})$<d-footnote>Up until now I’ve tried to avoid writing out math because I always recommend reading the original paper if you’re more curious about the “why”, and in this case the original paper is really simple, so I think it’s much easier to just let the math speak.</d-footnote>. The savings comes from the fact that we can actually sample multiple sequential tokens from $q(x_{&lt;t})$ while simultaneously computing tokens and the actual distribution from $p(x_{&lt;t})$. We can then perform rejection sampling based on the likelihood of the generated token, and choose up to the first token that was rejected. By using more compute resources, we can speed up decoding by up to how much faster the smaller model is than the larger model. This work was critical for future ideas on using smaller models for faster decoding.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/37.png" style="width:60%" alt="sparsity masks">
    <figcaption><b>Figure 37.</b> The Medusa heads are small learnable projections that, like the draft models in speculative decoding, are allowed to generate sequences of tokens rather than just a single token. <a href="https://arxiv.org/pdf/2401.10774" rel="external nofollow noopener" target="_blank">[Image Source]</a> </figcaption>
</center>
</figure>

<p><strong><a href="https://arxiv.org/abs/2401.10774" rel="external nofollow noopener" target="_blank">Medusa: Multiple Decoding Heads</a> (Cai et al., 2024<d-cite key="cai2024medusasimplellminference"></d-cite>)</strong>. Instead of using a smaller draft model, which is hard to fit into the GPU memory hierarchy without being slow, Medusa uses <strong>multiple prediction heads</strong> (each head is just a <a href="https://medium.com/image-processing-with-python/the-feedforward-network-ffn-in-the-transformer-model-6bb6e0ff18db" rel="external nofollow noopener" target="_blank">FFN</a> with a residual connection) at the last hidden state and a sparsely structured attention mask over the predictions (basically to make sure they only attend heads can’t attend to tokens they didn’t generate) which they call “tree attention”. Unlike speculative decoding, the authors argue that matching the original model distribution is unnecessary, as long as the outputs are “reasonable” (they define a rule based on <a href="https://arxiv.org/abs/2210.15191" rel="external nofollow noopener" target="_blank">truncated sampling</a>).</p>

<h2 id="part-n-modern-day-and-beyond">Part N: Modern Day and Beyond</h2>
<p>We are still in the era of scale. However, in my opinion (not necessarily shared by the community), I don’t find the recent results of “scaling” to be particularly impressive (e.g. in a lot of the domains like <a href="https://minerl.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">decision-making game environments</a>, <a href="https://www.swebench.com/multimodal.html" rel="external nofollow noopener" target="_blank">software engineering</a> tasks, etc. LLMs are still pretty bad). A lot of the prior directions in <a href="#part-iii-the-era-of-scale-till-we-fail-2020-now">Part III</a> are still being tackled to this day, so this section will feel a bit all over the place.  Here, I will list some interesting on-going threads without a strong answer.</p>

<h3 id="n1-whats-up-with-these-superclusters">N.1: What’s up with these superclusters?</h3>
<p>I recently listened to this <a href="https://www.youtube.com/c/DwarkeshPatel" rel="external nofollow noopener" target="_blank">Dwarkesh podcast</a> with Leopold Aschenbrenner where they talk in the beginning about the huge cost of building compute clusters that can support scaling model training. They talk about the natural progression of scaling these data centers beyond to <a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" rel="external nofollow noopener" target="_blank">100K H100s, ~150 MW</a>, and then to 1 GW, and beyond. GPT-4, for reference, was rumored to be trained on ≥20k A100s with 13T tokens, or roughly 2e25 FLOPS. It’s also been rumored recently that <a href="https://www.reuters.com/technology/microsoft-openai-planning-100-billion-data-center-project-information-reports-2024-03-29/" rel="external nofollow noopener" target="_blank">Microsoft wants to build a 100 billion dollar data center/supercluster</a> for their AI applications.</p>

<p>Obviously we haven’t observed the ceiling of the “scale to model performance” relationship, but I’ve always been a bit irked by the rush to continue scaling to uncharted territory, where the superclusters in AI are finally surpassing the existing institutional superclusters. I get that it has been “working” for a few years, but in some sense it reached a level of performance that I don’t find particularly surprising. LLMs model the distribution of language in its data distribution quite well, and they “generalize” to novel tasks (what does generalization even mean? We can barely characterize the distribution we are feeding as training data so what we think is generalization could be trivial when the model optimizes with respect to the entire Internet). Even more concerning, how did we extrapolate to the idea that these newer models will be superintelligent<d-footnote>I’m not claiming AI cannot be dangerous. In fact, existing AI applications are already dangerous in not-so-sci-fi-esque ways. I also am not denying that safety / doomsday preventative research is important. But for “scale-pilled” individuals, the argument for burning billions of dollars seems a bit weak. I wonder if there is some strong prior about the equations or models we’ve been using that people have been seeing.</d-footnote>, or even that much more useful for that matter? Why is a GPT-7 that much more useful than a GPT-4?</p>

<p><strong>Remark</strong>. I’m genuinely just curious what the rationale is, and I wonder if someone has a good answer for me. I would love to see a supercluster get built because I think it’s cool, but realistically there’s a high probability that it turns out to be a massive waste of resources.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="n2-how-much-bigger-are-industry-resources-than-academia">N.2: How much bigger are industry resources than academia?</h3>
<p>So I graduated from Princeton this past May, and during my undergrad I was part of the Princeton NLP group — now rebranded as Princeton Language and Intelligence (PLI). At the tail end of my time there, it was announced that PLI had purchased <a href="https://ai.princeton.edu/news/2024/princeton-invests-new-300-gpu-cluster-academic-ai-research" rel="external nofollow noopener" target="_blank">300 H100 GPUs</a>, positioning itself as one of the largest academic clusters for deep learning. The only other comparable academic cluster is UT Austin’s <a href="https://baxtel.com/news/university-of-texas-to-host-cluster-of-600-nvidia-h100-gpus" rel="external nofollow noopener" target="_blank">600 H100 cluster</a>, which most research labs would love to have.</p>

<p>I got curious about these numbers, because Meta’s <a href="https://arxiv.org/abs/2407.21783" rel="external nofollow noopener" target="_blank">LLaMA 3.1 family was reportedly trained on</a> <strong>16k GPUs on their 24k GPU cluster</strong> (I wonder what kind of monstrous network topology they’ve built…) — in this <a href="https://www.factorialfunds.com/blog/thoughts-on-llama-3" rel="external nofollow noopener" target="_blank">blog</a>, they estimate training to take ~100 days on this cluster (not sure how accurate this estimate is but this ballpark seems somewhat reasonable given the FLOPs range). And this is just on Meta’s LLaMA team — I’m sure they have more compute spread out across the company. In other words, my academic lab doesn’t seem so grand in comparison. That’s not to say that you cannot do good research in academia, but it is pretty funny to me just how much more compute and money these industry labs have over some of the most prestigious academic labs in the world.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="n3-how-fast-can-we-train-old-models-with-modern-techniques">N.3: How fast can we train old models with modern techniques?</h3>
<p>I’ve always been curious how fast we can train older algorithms on new hardware with all the new fancy tricks we’ve learned throughout the years. Here is a thread of some interesting works in this direction.</p>

<figure>
<center>
    <img src="/assets/img/efficient_dl/38.png" style="width:80%" alt="sparsity masks">
    <figcaption><b>Figure 38.</b> Comparison of convergence rates of different iterations of GPT-2, plot taken from [https://github.com/KellerJordan/modded-nanogpt](https://github.com/KellerJordan/modded-nanogpt) </figcaption>
</center>
</figure>

<p><strong><a href="https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history" rel="external nofollow noopener" target="_blank">llm.c to speedrunning NanoGPT</a> (Keller Jordan, 2024 - )</strong>.
Andrej Karpathy’s super efficient implementation of GPT-2 (124M parameters) called <a href="https://github.com/karpathy/llm.c" rel="external nofollow noopener" target="_blank">llm.c</a> achieves a validation loss of 3.28 on <a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1" rel="external nofollow noopener" target="_blank">FineWeb</a> in 45 minutes on an 8xH100. This feat was further pushed by an ongoing Twitter thread on applying modern training techniques to tweak the NanoGPT model to converge faster.</p>

<ul>
  <li>The <a href="https://x.com/kellerjordan0/status/1798863559243513937" rel="external nofollow noopener" target="_blank">original thread</a> adding rotary embeddings and an increasing LR. <strong>31.4 min.</strong>
</li>
  <li>Using new <strong>muon optimizer</strong>, although I don’t fully understand the intuition or what exactly it does (some kind of fast orthogonalization trick applied to the Nesterov momentum update). It does use less memory than AdamW though and is slightly faster! <strong>24.9 min.</strong>
</li>
  <li>The rest of the changes are in the repo/on Twitter, but it’s of the flavor of 1) tuning muon, 2) tweaking activations and layers 3) hardware-aware tricks. Current record: <strong>12.03 min.</strong>
</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/39.png" style="width:90%" alt="mosaic">
    <figcaption><b>Figure 39.</b> Table 1 in <a href="https://www.databricks.com/blog/mosaicbert" rel="external nofollow noopener" target="_blank">https://www.databricks.com/blog/mosaicbert</a>, the original BERT average GLUE score is 79.6, which they reach in 1.13 hours on an 8xA100. </figcaption>
</center>
</figure>

<p><strong><a href="https://www.databricks.com/blog/mosaicbert" rel="external nofollow noopener" target="_blank">Pre-training BERT for under $20</a> (Mosaic AI, 2024)</strong>. I really like this blog, as it showcases how far we’ve come in deep learning efficiency. BERT and <a href="https://arxiv.org/abs/1907.11692" rel="external nofollow noopener" target="_blank">RoBERTa</a> were some of my first introductions to the field of deep learning, and they were known at the time to be some of the biggest training jobs, costing upwards of $300 and <a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank">taking &gt;4 days on 16 TPUs</a>! They use a suite of tricks like <a href="https://github.com/Dao-AILab/flash-attention" rel="external nofollow noopener" target="_blank">FlashAttention</a>, <a href="https://arxiv.org/abs/2108.12409" rel="external nofollow noopener" target="_blank">ALiBi</a>, and <a href="https://arxiv.org/pdf/2208.08124" rel="external nofollow noopener" target="_blank">unpadding</a>, as well as the popular <a href="https://huggingface.co/datasets/allenai/c4" rel="external nofollow noopener" target="_blank">C4</a> corpus for pre-training. Basically, this paper takes the original BERT model and trains it entirely differently while using modern hardware and libraries, and it turns out to work extremely well. I’m excited to see how fast we can train LLaMA 3.1 405B in the future!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="n4-recent-efforts-to-scale-hybrid-or-non-transformer">N.4: Recent efforts to scale hybrid or non-Transformer.</h3>
<p>I sort of briefly mentioned alternatives to Transformers like SSMs and relevant algorithms like <a href="https://github.com/HazyResearch/flash-fft-conv" rel="external nofollow noopener" target="_blank">FlashFFTConv</a> that are used to accelerate them. Given the existing constraints of Transformers and the attention mechanism, I wanted to discuss some alternatives and roughly why people have been interested in them.</p>

<ul>
  <li>
<strong>Transformer-SSM Hybrids</strong> (e.g. <a href="https://www.ai21.com/jamba" rel="external nofollow noopener" target="_blank">Jamba</a>, <a href="https://github.com/togethercomputer/stripedhyena" rel="external nofollow noopener" target="_blank">Striped Hyena</a>). These models attempt to combine SSM blocks with Transformer blocks to improve long context reasoning capabilities. These models are still in the early stages of research without a key production-level model, but I wouldn’t be surprised if something interesting emerged from them in the future.</li>
  <li>
<a href="https://arxiv.org/abs/2305.13048" rel="external nofollow noopener" target="_blank"><strong>RWKW</strong></a>. An open-source effort (led by <a href="https://x.com/blinkdl_ai" rel="external nofollow noopener" target="_blank">BlinkDL</a>) to build an RNN that can be trained with parallel algorithms like a Transformer while maintaining constant memory / compute complexity during inference.</li>
  <li>
<a href="https://arxiv.org/abs/2307.08621" rel="external nofollow noopener" target="_blank"><strong>RetNet</strong></a> (Sun et al., 2023). Reformulating the attention mechanism with a recurrent formulating to get the benefits of a Transformer-like architecture with constant compute complexity during inference. It aims for similar guarantees to RWKV but the approach is entirely different.</li>
  <li>
<strong>Linearizing Transformers</strong> (e.g. <a href="https://arxiv.org/abs/2408.15237" rel="external nofollow noopener" target="_blank">Distilling Transformers into RNNs</a>, <a href="https://hazyresearch.stanford.edu/blog/2024-10-14-lolcats-p1" rel="external nofollow noopener" target="_blank">Linearizing LLaMA 3</a>). These methods attempt to take pre-trained Transformers and somehow distill or convert them into a different model with better inference-time guarantees. Unfortunately, the performance hit seems to be pretty significant in a lot of these.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="n5-model-efficiency-benchmarks">N.5: Model efficiency Benchmarks</h3>
<p>We have a lot of benchmarks for evaluating model performance, but not as many for evaluating efficiency. The most comprehensive benchmark available is the <a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/" rel="external nofollow noopener" target="_blank">MLPerf</a> benchmarks, which features inference, training, and HPC tasks across a wide range of modalities. In most instances, we can directly just compare algorithms on specific hardware, but I would be interested in more rigorous benchmarking in the future.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="n6-startups-in-the-efficient-deep-learning-space">N.6: Startups in the Efficient Deep Learning Space</h3>
<p>I have no affiliation to any of these startups — I just came across these at some point in the last year and felt they were interesting enough to save in my Notes app. I have no way of verifying if the work they’re doing is legit or even useful, so take it all with a grain of salt.</p>

<ul>
  <li>
<a href="https://www.etched.com/" rel="external nofollow noopener" target="_blank"><strong>Etched</strong></a>. An ASIC specialized for Transformers. At the time of writing, little information is known about their chip.</li>
  <li>
<a href="https://cerebras.ai/" rel="external nofollow noopener" target="_blank"><strong>Cerebras</strong></a>. Develop specialized chips for AI applications, with gimmicks like lots of on-device memory and super-fast inference for large models.</li>
  <li>
<a href="https://www.together.ai/" rel="external nofollow noopener" target="_blank"><strong>Together.ai</strong></a>. They’re pretty well known for their open-source work and research on fast inference methods and Transformer-SSM hybrids, but they also have a cloud platform for fine-tuning and using a wide variety of large models.</li>
  <li>
<a href="https://groq.com/" rel="external nofollow noopener" target="_blank"><strong>Groq</strong></a>. An ASIC specialized for language (basically big model) AI applications. They remove a lot of the complexity with CUDA’s hierarchy, and instead focus on being super low-latency and energy efficient. As far as I understand, they’ve mostly been used for model inference, like a lot of the other ASICs mentioned.</li>
  <li>
<a href="https://tenstorrent.com/" rel="external nofollow noopener" target="_blank"><strong>Tenstorrent</strong></a>. They develop a lot of custom hardware from chips to workstations specifically for AI applications. From what I can tell, they’re trying to build out a whole CUDA-like ecosystem, but I’m guessing they’ll need some kind of breakthrough performance to attract more interest.</li>
</ul>

<h2 id="resources">Resources</h2>
<h3 id="a1-where-to-access-free-gpus">A.1: Where to access “free” GPUs?</h3>
<p>There are plenty of services like Amazon AWS, Google GCP, Microsoft Azure, etc. that offer cloud GPUs, but if you’re not rich like me, you may also be interested in what free options are currently available<d-footnote>Gradient by Paperspace used to be my go-to, but I can’t seem to find what happened to it.</d-footnote>.</p>

<ul>
  <li>
<a href="https://colab.google/" rel="external nofollow noopener" target="_blank"><strong>Google Colab</strong></a>. You can get access to a free <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/" rel="external nofollow noopener" target="_blank">Tesla T4 16GB</a> when using their notebooks, but the time limits are not consistent and you’ll have to use multiple emails to get consistent usage.</li>
  <li>
<a href="https://studiolab.sagemaker.aws/" rel="external nofollow noopener" target="_blank"><strong>Amazon SageMaker Studio Lab</strong></a>. Another notebook service with a <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/" rel="external nofollow noopener" target="_blank">Tesla T4 16GB</a> available with just an email! The time limits are also not great on this one.</li>
  <li>
<a href="https://lightning.ai/" rel="external nofollow noopener" target="_blank"><strong>Lightning.ai</strong></a>. You get 22 free GPU hours every month without needing to put in a credit card, and you also get a vscode-like interface so you can just plop in your codebase and run what you need.</li>
  <li>
<a href="https://www.kaggle.com/" rel="external nofollow noopener" target="_blank"><strong>Kaggle</strong></a>. Kaggle gives access to free <a href="https://www.nvidia.com/en-gb/data-center/tesla-k80/" rel="external nofollow noopener" target="_blank">NVIDIA K80s</a> with a weekly limit. I’m not sure what it is anymore, but it used to be <a href="https://www.kaggle.com/discussions/general/108481" rel="external nofollow noopener" target="_blank">30 hours/week</a>.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a2-large-training-and-finetuning-frameworks">A.2: Large training and finetuning frameworks.</h3>
<p>There are many options for handling large-scale training jobs other than PyTorch/TensorFlow’s in-house wrappers and distributed modules. A lot of these examples use config files or YAML file configurations for defining your desired job. We list some useful libraries below.</p>

<ul>
  <li>
<a href="https://pytorch.org/torchtune/stable/index.html" rel="external nofollow noopener" target="_blank"><strong>torchtune</strong></a>. Torchtune is PyTorch’s newest module for fine-tuning large language models. They’ve heavily modularized their code and have some nice recent examples with LLaMA 3.</li>
  <li>
<a href="https://github.com/huggingface/peft" rel="external nofollow noopener" target="_blank"><strong>HuggingFace PEFT</strong></a>. PEFT integrates most of the existing parameter-efficient fine tuning (recall from <a href="#iii1c-fine-tuning-large-models-efficiently">III.1.c</a>) methods to work with models loaded from the Huggingface <code class="language-plaintext highlighter-rouge">transformers</code> library.</li>
  <li>
<a href="https://github.com/huggingface/accelerate" rel="external nofollow noopener" target="_blank"><strong>accelerate</strong></a>. A super-thin wrapper around your PyTorch models, dataloader, and optimizer for launching multi-GPU jobs without a lot of extra code.</li>
  <li>
<a href="https://github.com/microsoft/DeepSpeed" rel="external nofollow noopener" target="_blank"><strong>deepspeed</strong></a>. A library around PyTorch for reducing multi-GPU workloads automatically. It notably integrates ZeRO optimizations, and works really well with / similarly to accelerate.</li>
  <li>
<a href="https://github.com/axolotl-ai-cloud/axolotl" rel="external nofollow noopener" target="_blank"><strong>axolotl</strong></a>. A fine-tuning library that sits on top of libraries like <code class="language-plaintext highlighter-rouge">deepspeed</code> and <code class="language-plaintext highlighter-rouge">accelerate</code>. It’s basically like a code-free tool and works entirely in config files and the CLI.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a3-model-compression-frameworks">A.3: Model compression frameworks.</h3>
<p>A lot of model inference libraries like TensorRT do auto-tuned quantization under the hood, but for research purposes, there are other frameworks where you have better control over the weight / activation quantization.</p>

<ul>
  <li>
<a href="https://pytorch.org/docs/stable/quantization-support.html" rel="external nofollow noopener" target="_blank"><strong>torch.ao.quantization</strong></a> <strong>(2022)</strong>. Quantization used to be quite annoying to implement because it modifies how we represent our data in memory. The PyTorch team has done a lot of work</li>
  <li>
<a href="https://github.com/bitsandbytes-foundation/bitsandbytes" rel="external nofollow noopener" target="_blank"><strong>bitsandbytes</strong></a> <strong>(2023)</strong>. A wrapper around your optimizers that allows you to use llm.int8() and Q-LoRA. It works very well with HuggingFace and PyTorch.</li>
  <li>
<a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer" rel="external nofollow noopener" target="_blank"><strong>TensorRT Model Optimizer</strong></a>. This library is like an intermediate step between converting from PyTorch / ONNX and TensorRT. It runs a bunch of optimizations like pruning, quantization, and distillation to your model to prepare it for inference, but it works at the computational graph level.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a4-profiling-tools">A.4: Profiling Tools.</h3>
<p>For any kind of efficient deep learning work, it is always important to profile your models at all levels of the compute hierarchy. Check out the <a href="https://www.youtube.com/watch?v=LuhJEEJQgUM&amp;ab_channel=GPUMODE" rel="external nofollow noopener" target="_blank">GPU Mode lecture on profiling</a>, which is a nice introduction to profiling in PyTorch, Triton, and CUDA. Here, we provide some useful tools for profiling your code.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/nvitop.png" style="width:90%" alt="nvitop">
</center>
</figure>

<p><a href="https://nvitop.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">nvitop</a>. You can always just use <code class="language-plaintext highlighter-rouge">nvidia-smi</code> to view the memory / power usage of your GPUs, but there are some cool alternatives that are prettier and more customizable. I pretty much use nvitop as an nvidia-smi replacement, but there are a lot of other features they have in their GitHub that you can play with.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/pytorch_profiler.png" style="width:100%" alt="pytorch profiler">
</center>
</figure>

<p><a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" rel="external nofollow noopener" target="_blank">Pytorch Profiler</a>. PyTorch has a simple profiler that you can wrap around your code for viewing the individual kernels / CPU calls. It also has peak memory usage / compute time statistics that it prints out for you, and is relatively simple to insert into your code for debugging.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<figure>
<center>
    <img src="/assets/img/efficient_dl/nsight_compute.png" style="width:80%" alt="ncu">
</center>
</figure>
<p><a href="https://developer.nvidia.com/nsight-compute" rel="external nofollow noopener" target="_blank">Nsight Compute</a> and the <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html" rel="external nofollow noopener" target="_blank">Nsight Compute CLI (ncu)</a> are excellent profiling tools for your CUDA kernels. It provides analysis on potential bottlenecks, as well thread, memory, and kernel call information at a very fine granularity. It also provides thorough analysis and recommendations for fixing bottlenecks in your kernels.</p>

<p><a href="https://developer.nvidia.com/nsight-systems" rel="external nofollow noopener" target="_blank">Nsight Systems</a> is designed for profiling entire workloads (CPU, GPU), and is more similar to the PyTorch profiler tool.</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a5-from-scratch-style-tutorials">A.5: “From scratch”-style tutorials.</h3>
<p>It was always nice to get your hands dirty when learning a new topic. The machine learning community has made a lot of nice libraries for practitioners to use that lets you load and use a powerful LLM with a few lines of code. However, because the field moves so fast, it is a valuable skill to know what’s going on under the hood. Here, we list many useful resources for learning from the ground up (many of which come from Andrej Karpathy).</p>

<ul>
  <li>
<a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank">Karpathy’s Neural Networks: Zero to Hero Playlist</a>.  Probably one of the most information-dense tutorials for how LLMs are coded from the ground up. I find his teaching style quite fun, and I think these are worth following in your free time.</li>
  <li>
<a href="https://www.youtube.com/watch?v=4pkbXmE4POc&amp;list=PLRRuQYjFhpmubuwx-w8X964ofVkW1T8O4" rel="external nofollow noopener" target="_blank">Programming Massively Parallel Processors Lectures</a>. The PMPP book is one of the most iconic for understanding common GPU programming primitives. The lectures are from one of the authors, and they’re extremely well-made. Most of the examples are in CUDA, which is perfect for getting into efficient deep learning.</li>
  <li>
<a href="http://blog.ezyang.com/2019/05/pytorch-internals/" rel="external nofollow noopener" target="_blank">PyTorch internals</a>. I’m not sure how much PyTorch has changed since this blog came out (there’s slides out there for PyTorch 2.0), but this blog has a lot of nice visuals that explains how PyTorch implements tensors, autodifferentiation, and kernel dispatches.</li>
  <li>
<a href="https://siboehm.com/articles/22/CUDA-MMM" rel="external nofollow noopener" target="_blank">Optimizing CUDA Matmul from Scratch</a>. I love this blog — the goal is to get to CuBLAS-level performance with raw CUDA, and they use a lot of the tricks and primitives you learn from the PMPP book. I found this blog to be one of the most helpful hands-on tutorials for getting started with CUDA.</li>
  <li>
<a href="https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/" rel="external nofollow noopener" target="_blank">Colfax CUTLASS</a>. Colfax has a bunch of nice blogs on GPUs, but their CUTLASS GEMM series is extremely new and well-made. This resource is probably the most up-to-date out of the ones listed so far.</li>
  <li>
<a href="https://www.youtube.com/watch?v=RgUl6BlyaF4&amp;list=PL80kAHvQbh-qGtNc54A6KW4i4bkTPjiRF" rel="external nofollow noopener" target="_blank">Han Song’s Efficient ML Lectures</a>. Professor Han Song is one of the leading figures in efficient ML, and his course is freely available on YouTube. I watched the 2023 iteration of the course, but a lot of the topics center around his research which is pretty cool!</li>
  <li>
<a href="https://residentmario.github.io/pytorch-training-performance-guide/intro.html" rel="external nofollow noopener" target="_blank">PyTorch Performance Guide</a>. High-level overview of common training techniques for PyTorch workloads.</li>
</ul>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a6-designing-deep-learning-clusters-and-network-topology">A.6: Designing deep learning clusters and network topology.</h3>
<p>We can design all the algorithms we want for working with multiple nodes, but if our cluster is poorly designed, we are strictly bottlenecked by speed. Ideally, we would want every device and node to share the same pair-wise communication latency, but in practice this is almost impossible.</p>

<p><strong><a href="https://www.nvidia.com/en-gb/data-center/dgx-systems/" rel="external nofollow noopener" target="_blank">NVIDIA DGX servers</a>.</strong> NVIDIA has packaged up their GPUs nicely into these super expensive multi-GPU servers that you can plug into your cluster. They handle stuff like optimizing the inter-GPU interconnects and attaching a host processor for you<d-footnote>A lot more details about each generation of these servers can be found here: https://training.continuumlabs.ai/infrastructure/servers-and-chips/nvidia-dgx-2</d-footnote>. While researching this topic (e.g. when someone says they’re using a 8xH100, what else is there other than the H100s), I came across a bunch of other bundled up servers like <a href="https://www.arccompute.io/solutions/hardware/gpu-servers" rel="external nofollow noopener" target="_blank">Arc Compute</a> and <a href="https://lambdalabs.com/deep-learning/servers/hyperplane" rel="external nofollow noopener" target="_blank">Lambda Hyperplane</a> from third-party distributors.</p>

<p><strong>Network topology.</strong> I heard this work thrown around a lot, and it sort of confused me what the relation was to say point-set topology. But network topology is literally the physical connections between nodes and devices within a cluster. Unfortunately, I know little about the design decisions here other than something of the form “node A has a limited number of lanes/ports, so we can’t just jam all the nodes together”. I hope to expand this section and add it to <a href="#part-iii-the-era-of-scale-till-we-fail-2020-now">Part III</a>!</p>

<hr style="margin-bottom: 20px;margin-top: 20px">

<h3 id="a7-useful-surveys-on-efficiency">A.7: Useful surveys on efficiency.</h3>
<p>Part of the difficulty of research in this field is sifting through the sheer number of different papers. This post hopefully serves as a strong filter for many of these works, but perhaps for some readers it is <em>too</em> strong of a filter. Below, I list some comprehensive surveys to find more interesting works related to efficiency.</p>

<ul>
  <li>
<strong>[2020] Efficient Transformers: A Survey</strong>: <a href="https://arxiv.org/abs/2009.06732" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2009.06732</a>.</li>
  <li>
<strong>[2020] The Deep Learning Compiler: A Comprehensive Survey</strong>: <a href="https://arxiv.org/pdf/2002.03794" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2002.03794</a>.</li>
  <li>
<strong>[2021]</strong> <strong>Efficient Deep Learning</strong>: <a href="https://arxiv.org/abs/2106.08962" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2106.08962</a>.</li>
  <li>
<strong>[2023] Deep Learning Accelerators</strong>: <a href="https://arxiv.org/abs/2306.15552" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2306.15552</a>.</li>
  <li>
<strong>[2023] Deep Learning Pruning</strong>: <a href="https://arxiv.org/abs/2308.06767" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2308.06767</a>.</li>
  <li>
<strong>[2023] Efficient Large Language Models: A Survey</strong>: <a href="https://arxiv.org/abs/2312.03863" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2312.03863</a>.</li>
  <li>
<strong>[2023]</strong> <strong>Survey on TinyML</strong>: <a href="https://ieeexplore.ieee.org/document/10177729" rel="external nofollow noopener" target="_blank">https://ieeexplore.ieee.org/document/10177729</a>.</li>
  <li>
<strong>Lil’log.</strong> (<a href="https://lilianweng.github.io/posts/2020-08-06-nas/" rel="external nofollow noopener" target="_blank">https://lilianweng.github.io</a>/). Just the absolute GOAT with lots of topics on deep learning in general.</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>I am open to suggestions and edits, even those that are critical. I want to log these edits and changes made over time in this section to give credit where credit is due!</p>

<ul>
  <li>
<strong>Eddy Wu</strong> for finding typos in the quantization and sparsity sections.</li>
</ul>

<h2 id="citation">Citation</h2>
<p>Just as a formality, if you want to cite this for whatever reason, use the BibTeX below.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2024efficientdl,
  title   = "A Meticulous Guide to Advances in Deep Learning Efficiency over the Years",
  author  = "Zhang, Alex",
  year    = "2024",
  month   = "October",
  url     = "https://alexzhang13.github.io/blog/2024/efficient-dl/"
}
</code></pre></div></div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/efficientdl2024.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    

    <div class="container">
      © Copyright 2025
      Alex
      L.
      Zhang. 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-K2L3VESDMP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-K2L3VESDMP');
  </script>




    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


  
</body>
</html>
