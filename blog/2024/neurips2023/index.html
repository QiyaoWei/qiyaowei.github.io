<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Highlights of NeurIPS 2023 from Reading All 3584 Abstracts | Alex L. Zhang
    
  
</title>
<meta name="author" content="Alex L. Zhang">
<meta name="description" content="Just me reading through every paper abstract...">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">






  <!-- OpenGraph -->
  <meta property="og:site_name" content="Alex L. Zhang">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Alex L. Zhang | Highlights of NeurIPS 2023 from Reading All 3584 Abstracts">
  <meta property="og:url" content="https://alexzhang13.github.io/blog/2024/neurips2023/">
  <meta property="og:description" content="Just me reading through every paper abstract...">
  
    <meta property="og:image" content="https://alexzhang13.github.io/assets/img/preview/default.png">
  
  <meta property="og:locale" content="en">

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Highlights of NeurIPS 2023 from Reading All 3584 Abstracts">
  <meta name="twitter:description" content="Just me reading through every paper abstract...">
  
    <meta name="twitter:image" content="https://alexzhang13.github.io/assets/img/preview/default.png">
  
  
    <meta name="twitter:site" content="@a1zhang">
    <meta name="twitter:creator" content="@a1zhang">
  



  <!-- Schema.org -->
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Alex L. Zhang"
        },
        "url": "https://alexzhang13.github.io/blog/2024/neurips2023/",
        "@type": "BlogPosting",
        "description": "Just me reading through every paper abstract...",
        "headline": "Highlights of NeurIPS 2023 from Reading All 3584 Abstracts",
        
        "sameAs": ["https://scholar.google.com/citations?user=rtCr0q4AAAAJ", "https://github.com/alexzhang13", "https://www.linkedin.com/in/alexzhang13", "https://twitter.com/a1zhang"],
        
        "name": "Alex L. Zhang",
        "@context": "https://schema.org"
    }
  </script>



<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AB%A1&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://alexzhang13.github.io/blog/2024/neurips2023/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Highlights of NeurIPS 2023 from Reading All 3584 Abstracts",
            "description": "Just me reading through every paper abstract...",
            "published": "January 09, 2024",
            "authors": [
              
              {
                "author": "Alex Zhang",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Undergraduate at Princeton University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Alex</span>
            
            L.
            Zhang
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/publications/">publications and pre-prints
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/projects/">projects
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/academics/">academics
                    
                  </a>
                </li>
              
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Highlights of NeurIPS 2023 from Reading All 3584 Abstracts</h1>
        <p>Just me reading through every paper abstract...</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#introduction">Introduction</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#the-overall-vibes">The Overall Vibes</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#large-language-models">Large Language Models</a>
                </div>
                
              
                <div>
                  <a href="#multi-modal-learning">Multi-modal Learning</a>
                </div>
                
              
                <div>
                  <a href="#transformers">Transformers</a>
                </div>
                
              
                <div>
                  <a href="#reinforcement-learning">Reinforcement Learning</a>
                </div>
                
              
                <div>
                  <a href="#generative-ai">Generative AI</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#diffusion-models">Diffusion Models</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#computer-vision">Computer Vision</a>
                </div>
                
              
                <div>
                  <a href="#adversarial-attacks-and-model-poisoning">Adversarial Attacks and Model Poisoning</a>
                </div>
                
              
                <div>
                  <a href="#knowledge-distillation-and-memory-reduction-schemes">Knowledge Distillation and Memory Reduction Schemes</a>
                </div>
                
              
                <div>
                  <a href="#graph-neural-networks">Graph Neural Networks</a>
                </div>
                
              
                <div>
                  <a href="#privacy-and-federated-learning">Privacy and Federated Learning</a>
                </div>
                
              
                <div>
                  <a href="#datasets-benchmarks-challenges">Datasets, Benchmarks, Challenges</a>
                </div>
                
              
                <div>
                  <a href="#other-topics">Other Topics</a>
                </div>
                
              
                <div>
                  <a href="#"></a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#interpretability-and-explainable-ai">Interpretability and Explainable AI</a>
                      </li>
                    
                      <li>
                        <a href="#training-dynamics">Training Dynamics</a>
                      </li>
                    
                      <li>
                        <a href="#implicit-bias">Implicit Bias</a>
                      </li>
                    
                      <li>
                        <a href="#embodied-ai">Embodied AI</a>
                      </li>
                    
                      <li>
                        <a href="#neural-architecture-search">Neural Architecture Search</a>
                      </li>
                    
                      <li>
                        <a href="#neural-operators">Neural Operators</a>
                      </li>
                    
                      <li>
                        <a href="#variational-inference-methods">Variational Inference Methods</a>
                      </li>
                    
                      <li>
                        <a href="#quantum-information-theory">Quantum Information Theory</a>
                      </li>
                    
                      <li>
                        <a href="#energy-based-models">Energy-Based Models</a>
                      </li>
                    
                      <li>
                        <a href="#curriculum-learning">Curriculum Learning</a>
                      </li>
                    
                      <li>
                        <a href="#anomaly-detection">Anomaly Detection</a>
                      </li>
                    
                      <li>
                        <a href="#class-imbalance-approaches">Class Imbalance Approaches</a>
                      </li>
                    
                      <li>
                        <a href="#continual-learning">Continual Learning</a>
                      </li>
                    
                      <li>
                        <a href="#deep-learning-theory">Deep Learning Theory</a>
                      </li>
                    
                      <li>
                        <a href="#bio-inspired-ai">Bio-inspired AI</a>
                      </li>
                    
                      <li>
                        <a href="#domain-specific-applications-of-ai">Domain-specific applications of AI</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#other-papers-i-liked">Other Papers I Liked</a>
                </div>
                
              
                <div>
                  <a href="#footnotes">Footnotes</a>
                </div>
                
              
                <div>
                  <a href="#references">References</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <h1 id="introduction">Introduction</h1>

<p>To celebrate the end of my graduate school application deadlines and finals week disaster, I decided
to spend my winter break <b>going through and reading every single abstract of the accepted papers
in the NeurIPS 2023</b> (which I unfortunately couldn’t attend). It was a long and sometimes mind-numbing process (especially since the TeX wasn’t rendering on the <a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">neurips.cc</a> website), but it was really cool to see all these works and ideas that I had no idea were being done. Luckily, I am somewhat familiar with quite a number of these papers because they popped off on <a href="https://arxiv.org/list/cs.AI/recent" rel="external nofollow noopener" target="_blank">arXiv</a> or
<a href="https://twitter.com/home" rel="external nofollow noopener" target="_blank">Twitter</a> when they were first announced, so I wasn’t discovering every
single paper for the first time. Here is just a highlight of what I found interesting and the
general vibes I had while reading over the last two weeks, but keep in mind that I am an
undergraduate student that has not worked with or spent a lot of time with a variety of popular
topics (e.g. <a href="https://en.wikipedia.org/wiki/Federated_learning" rel="external nofollow noopener" target="_blank">federated learning</a>, <a href="https://en.wikipedia.org/wiki/Differential_privacy" rel="external nofollow noopener" target="_blank">differential
privacy</a>, <a href="https://en.wikipedia.org/wiki/Causal_inference" rel="external nofollow noopener" target="_blank">causal
inference</a>). I’ve structured this post into a
<strong>high-level overview for each topic</strong> of what I observed, followed by <strong>short discussions on papers I
found interesting</strong>. Each discussion is loaded with references to the relevant NeurIPS papers, and I’ve tried to ensure that almost every citations in this post are from NeurIPS 2023.
If you want to read the abstracts on your own, they’re available publicly at
<a href="https://neurips.cc/virtual/2023/papers.html" rel="external nofollow noopener" target="_blank">https://neurips.cc/virtual/2023/papers.html</a>. Finally,
I ended up searching up so many terms, I eventually started keeping track of them in case the
curious reader wants to know at the bottom of this post.</p>

<h2 id="the-overall-vibes">The Overall Vibes</h2>
<figure>
    <img src="/assets/img/neurips2023.png" width="400" alt="Visualization of NeurIPS 2023">
    <figcaption>Visualization generated from <a href="https://neurips2023.vizhub.ai/?ref=blog.roboflow.com&amp;brushed=%255B%255B14.600000381469727%252C-10%255D%252C%255B798.7249755859375%252C796.664306640625%255D%255D" rel="external nofollow noopener" target="_blank">here.</a></figcaption>
</figure>

<p>2023 was a great year for AI (generative models especially), and the number of submissions to
NeurIPS 2023 reflects that. This year, there were <b>3584</b> accepted papers out of an astonishing <b>12345</b> submissions. Honestly, I expected to go into this process marking only my top 10 or 20 favorite papers, but I discovered so many absolutely fascinating works on the most random things like applying 3D neural rendering methods to Unreal Engine. By the end of this process, I ended up reading or skimming probably 50 or so of the papers (excluding the ones that I had seen before). Of course, from the abstract alone, it is not possible to grasp the quality or impact of a work, and in an ideal world I would have read each paper in their entirety and tried their code as well. Regardless, after reading through, these are some things that stuck out to me this year:</p>
<ol>
  <li>
<strong>Multimodal is the new “thing”.</strong> Rather unsurprisingly, combining vision and language is a hot topic <d-cite key="huang2023language,wang2023connecting,luo2023cheap,gadre2023datacomp,zhu2023multimodal,mizrahi2023m,yin2023vlattack,cheng2023metaadapter,dai2023instructblip,wortsman2023stable,wu2023parameter"></d-cite>. With LLMs demonstrating impressive performance in the past few years, leveraging these models to reason over visual data is a natural progression of the technology.</li>
  <li>
<strong>In-context Learning (ICL) belongs everywhere.</strong> Anything that even resembles a transformer should exhibit in-context learning, whether its a diffusion model <d-cite key="wang2023incontext"></d-cite>, for RL <d-cite key="brooks2023large"></d-cite>, or even for generic regression <d-cite key="raventós2023pretraining"></d-cite>. I wonder what’ll happen when more gradient-free learning methods like hyperdimensional computing <d-footnote>https://arxiv.org/abs/2111.06077</d-footnote> take off…</li>
  <li>
<strong>Diffusion models are really good at generating things.</strong> Diffusion models are getting so good at generating stuff, it seems like people are confident that they can be used for synthetic data<d-cite key="yang2023freemask,pronovost2023scenario,ntavelis2023autodecoding,zhu2023genimage"></d-cite>. There is a lot of exploration into using diffusion for more than just de-noising and text-to-image generation <d-cite key="saxena2023surprising,mo2023dit3d"></d-cite>.</li>
  <li>
<strong>Leveraging model priors.</strong> Foundation models are a blessing because they contain so much information about a specific domain. Leveraging these models beyond just in-domain downstream tasks is going to be extremely important to understand for the next few years, especially concerning data multiple modalities <d-cite key="wang2023connecting"></d-cite>. We also want to understand how to use the information present in these models in an interpretable way.</li>
  <li>
<strong>Model robustness.</strong> We want to be able to trust our models, and this means that our models behave according to our expectations. These models should also be robust to malicious actors and data, and should not leak our information, especially if we continue to feed them with our data. There are many tradeoffs and metrics for model robustness, so identifying the upper bounds both theoretically and empirically are important.</li>
  <li>
<strong>What does it mean to be similar?</strong> How do we measure similarity being two “concepts”? Or even just semantics? An intuitive way is to use similarity metrics like inner products over an embedding space, but how do we know that this embedding space is truly representative of the data <d-cite key="oh2023geodesic"></d-cite>? Do we measure similarity in terms of probability distributions, or do we use mutual information <d-cite key="dunion2023conditional,wang2023mutualinformation"></d-cite>?</li>
  <li>
<strong>When are inductive biases useful?</strong> We don’t necessarily always have the luxury of being able to scale our models and datasets for every domain, and we don’t know whether or not our models should not use inductive biases <d-cite key="bachmann2023scaling"></d-cite>. We want to understand whether we can leverage our own understanding of problems to introduce effective inductive biases like symmetry to solve problems <d-cite key="wu2023equivariant,lengyel2023color"></d-cite>.</li>
</ol>

<h2 id="large-language-models">Large Language Models</h2>
<p>This is the section of papers I was most familiar with before reading through the abstracts.
Generally, there have been a lot of works exploring the reasoning capabilities of LLMs through
prompting or fine-tuning. I think the LLM papers at this conference reflect the
interests of the NLP community for the past year even though a lot of these works can be considered
<strong>at least a year old at this point</strong> (which in this field is old!). One general gripe I have with these works is that they often like to make claims about general LLM behavior, but often evaluate on an arbitrary LLM of their choosing. It would be nicer if there was some consistency here, but it’s probably because the field is so fast-moving combined with the fact that many models are just inaccessible to 95% of labs. There is also this interesting interplay of whether data purity or extra machinery is the driving factor towards improving performance. These are the general themes that I’ve noticed:</p>

<ol>
  <li>
<strong>Reasoning.</strong> How can we apply algorithms or intuitions to fine-tune
LLMs to be better planners, reasoners, etc. like in <d-cite key="phan2023training"></d-cite>? Furthermore, how can we explicitly improve their reasoning capabilities without gradient updates like in <d-cite key="yao2023tree"></d-cite>?</li>
  <li>
<strong>Logical Reasoning.</strong> Can LLMs be used for logical tasks like mathematics <d-cite key="zhang2023evaluating,frieder2023mathematical"></d-cite>  and coding <d-cite key="yang2023intercode"></d-cite>? What are the pitfalls and potential solutions? We are interested in analyzing the limits of their abilities, as well as understand why and when they fail <d-cite key="hanna2023does"></d-cite>.</li>
  <li>
<strong>Agents.</strong> Can LLMs be considered cognitive agents? That is, can we
equip them with capabilities (tools, externals APIs <d-cite key="schick2023toolformer"></d-cite>, etc.) such that they can interact with environments through text generation? Furthermore, can they interact, think, and reflect in an anthropomorphic way? <d-cite key="madaan2023selfrefine,shinn2023reflexion"></d-cite>
</li>
  <li>
<strong>Efficiency.</strong> A key bottleneck in autoregressive LLM inference on GPUs is data movement, so research into circumventing this issue by exploiting parallelism and cached data is key (e.g. speculative decoding) <d-cite key="jin2023s3,kim2023speculative,sun2023spectr,zhu2023optimal"></d-cite>. Furthermore, can we improve the speed and cost of both training and inference on LLMs at both a systems-level (e.g. take advantage of properties of GPU memory footprints and throughput speeds) and model-level (low-rank fine-tuning, adaptors, sparsity, etc.) <d-cite key="portes2023mosaicbert,pagliardini2023faster,dettmers2023qlora"></d-cite>?</li>
  <li>
<strong>Scaling and Fine-tuning Models.</strong> What advances can we make to push the capabilities of foundation models (e.g. MoE) <d-cite key="xue2023repeat"></d-cite>? What procedures can we use to efficiently fine-tune <d-cite key="malladi2023finetuning,dubois2023alpacafarm"></d-cite> these models for downstream tasks or direct their generations towards user-aligned behavior (also can we do this without reinforcement learning (RL) <d-cite key="rafailov2023direct,zhou2023lima"></d-cite>) ? Or, what techniques can we use to bridge the gap between smaller models and huge models? Also, what kind of scaling laws can we identify between model parameters and data <d-cite key="muennighoff2023scaling"></d-cite>?</li>
  <li>
<strong>Memory and Context Windows.</strong> The context window of an LLM is inherently limited by memory constraints and the quadratic runtime of the standard attention mechanism. How do we increase the size of the input context window without degrading performance? The primary methods that are being investigated are external memory stores in LLMs to process huge chunks of texts like a book <d-cite key="wang2023augmenting,bertsch2023unlimiformer,mohtashami2023landmark"></d-cite> and summarization tokens <d-cite key="mu2023learning"></d-cite>.</li>
  <li>
<strong>Emergent Capabilities.</strong> It has been observed that LLMs seem to be suddenly able to perform tasks after scaling up to a certain point. Can we characterize these emergent abilities of LLMs effectively and why we observe them <d-cite key="schaeffer2023emergent"></d-cite>?</li>
  <li>
<strong>Controlling Generation.</strong> Can we 1) hard constrain the outputs and 2) steer LLM generations to be what we want? Typically (2) has been done with instruction-tuning and RLHF<d-footnote>Reinforcement Learning with Human Feedback, an application of reinforcement learning to further fine-tune language models to better reflect our human preferences. Recommend reading this source: https://huggingface.co/blog/rlhf</d-footnote>, but methods like <d-cite key="li2023inferencetime"> </d-cite> modify activation patterns in the model during inference while <d-cite key="li2023guiding"></d-cite> learns a smaller auxiliary model to edit the prompt of the larger model, which we can effectively treat as a set of parameters. Furthermore, what methods can we come up with to ensure preference-based alignment is accurate and robust <d-cite key="wang2023aligning"></d-cite>
</li>
  <li>
<strong>LLMs can do [insert task]</strong>. How far can we go with zero-shot, few-shot, and in-context
learning (ICL) to mimic known algorithmic procedures <d-cite key="chen2023evoprompting"></d-cite> entirely through prompting? For example, policy iteration from RL <d-cite key="brooks2023large"> </d-cite> or time-series forecasting <d-cite key="gruver2023large"> </d-cite>.</li>
  <li>
<strong>Evaluating Language Models.</strong> What kind of metrics and
   benchmarks are needed to effectively evaluate the abilities of different LMs? Is it by using a strong LLM as a judge <d-cite key="zheng2023judging"></d-cite>? Also, we generally just want more benchmarks for evaluating abilities like factuality <d-cite key="chen2023felm"></d-cite>, coding <d-cite key="yang2023intercode"></d-cite>, and domain-specific information <d-cite key="liu2023benchmarking,guha2023legalbench,guo2023large"></d-cite>.</li>
</ol>

<h3 id="llm-interesting-papers">[<span style="color:orange">LLM</span>] Interesting Papers</h3>
<p>Below is a list of papers I particularly liked and think are worth reading in their entirety. Of course, there were plenty of other extremely interesting and useful works at this conference, so please do not take this as some kind of definitive ranking of papers. Also, I’m mainly going to be giving a brief sentence about why I think each paper is cool/important and not a <em>tldr</em>, as I think you can get a lot more out of just reading it yourself. I have included this type of subsection at the end of every topic, so enjoy!</p>

<ol>
  <li>
<strong>Tree-of-Thoughts</strong> <d-cite key="yao2023tree"></d-cite>: A simple yet highly useful idea,
tree-of-thoughts (ToT) is simply an extension of chain-of-thoughts where a model can now traverse
through its own “thought” chains as a tree. This simple application of graph traversal to CoT
(which also gives beam search vibes) has been used extensively for prompting in the last year. This paper actually came from my PI’s lab :)</li>
  <li>
<strong>Toolformer</strong> <d-cite key="schick2023toolformer"></d-cite>: Another well-known and simple
approach, Toolformer is an extremely general framework for fine-tuning a model to be able to use
user-specified APIs like a calculator or a search engine. It’s really practical and their
experiments use GPT-J <d-footnote>https://huggingface.co/docs/transformers/model_doc/gptj</d-footnote>, so it’s quite easy to replicate for your own use cases.</li>
  <li>
<strong>Are Emergent Abilities of Large Language Models a Mirage?</strong> <d-cite key="schaeffer2023emergent"></d-cite>: I remember this paper being widely debated when it came out, with many arguing that the author’s conclusion was not indicative of anything useful. I think this paper makes a great point, in that emergent capabilities are a consequence of the evaluation metrics we choose to measure abilities, even if these evaluation metrics are natural (e.g. % of math questions answered). The problem with relying on intuitive of pre-existing metrics is that they don’t tell the full picture about scale vs. performance. I’ll put it like this. Suppose our LLM has never been able to solve task A. No matter how we have scaled it so far, it can never solve A. Scaling is expensive, so an important question is whether scaling will lead to emergent capabilities on A. Instead, if we have an auxiliary task B that is indicative of solving A, we can measure the relationship between scale and performance on A by looking at performance on B. The usefulness of reframing is to hopefully find metrics that show linear relationships between things like scale and downstream task performance, but this is still hard to do.</li>
  <li>
<strong>SPRING: Studying Papers and Reasoning to Play Games</strong> <d-cite key="wu2023spring"></d-cite>: This paper is exciting to me because I am very interested in models that can utilize external sources of information in an intuitive way. Outside of their reasoning module, they read information using an LLM from a game manual (honestly I don’t really get why they use the LaTeX source though, maybe for equations?) and store it as context for their agent (also an LLM). The point is that an LLM can act as a retriever and can intuit about what information is relevant. I think that this work is in a very preliminary stage though, and there is a lot of future research to be done in generalizing this type of framework.</li>
  <li>
<strong>RapidBERT: Pretraining BERT from Scratch for $20</strong> <d-cite key="portes2023mosaicbert"></d-cite>: This paper is kind of crazy… With lots of modern tricks (FlashAttention<d-cite key="dao2022flashattention"></d-cite>, ALiBi<d-cite key="press2022train"></d-cite>, low-precision layernorm, etc.) they can pre-train a BERT with the same performance as the original in just 9 hours on an A100. I think it goes to show far how we’ve gone in optimizing our language models. It is also important to note that they use a more modern scraped dataset in this paper (C4 <d-footnote>https://github.com/allenai/allennlp/discussions/5056</d-footnote>) for pre-training.</li>
  <li>
<strong>Scaling Data-Constrained Language Models</strong> <d-cite key="muennighoff2023scaling"></d-cite>: Understanding the relationship between model scale and data scale is important, and this paper looks into scaling laws under limited amounts of unique data. In their experiments, they found that repeating data during training works but diminishes in performance compared to using completely unique data. As an aside, their results focus on cross-entropy loss, but with any work that focuses on CE loss, it is important to distinguish inherent entropy in the data and the actual performance gap you want to mitigate<d-footnote>https://arxiv.org/abs/2307.15936</d-footnote>.</li>
  <li>
<strong>Collaborative Alignment of NLP Models</strong><d-cite key="khani2023collaborative"></d-cite>: We have been conditioned to use single, huge foundation models because they work well in practice, but this paper looks into whether the learning of several, concept-aligned models with some meta-chooser on top actually works just as well. The primary benefit is the ability to modularize and parallelize LLMs, making them more flexible and also potentially faster.</li>
  <li>
<strong>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.</strong>   <d-cite key="wen2023hard"></d-cite>: You can sort of think of (hard) prompts as parameters to an LLM that transferrable to other LLMs (e.g. If I prompt GPT-4 with “Explain Newton’s laws like I’m 5”, I expect LLAMA 2 to answer similarly). In the case of text-to-image models, the images we get are often not exactly what we want (there are various reasons for this we discuss in the multi-modal section!). This work is basically a way to steer text prompts with gradient-based optimization so they generate the images you want.</li>
  <li>
<strong>OpenAssistant Conversations - Democratizing Large Language Model Alignment</strong> <d-cite key="kopf2023openassistant"></d-cite>: So I actually followed this project on YouTube through Yannic Kilcher’s channel <d-footnote>https://youtube.com/yannickilcher</d-footnote>, and the central idea is to open-source the data required for RLHF because its extremely expensive to curate 60k+ human preference samples. I believe that open-source communities are extremely important for AI, and it’s exciting to see them produce extremely useful projects like this one. Because it is open-source, a large chunk of the paper discusses quality control and reducing “bad” or “toxic” data.</li>
  <li>
<strong>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</strong> <d-cite key="rafailov2023direct"></d-cite>: For a while, the standard preference-alignment approach was to learn a reward model over preference data and fine-tune an LLM with RL on this reward model (RLHF). The issue with this approach is that RL is generally quite unstable and hard to work with, so this paper first motivates re-parameterizing the RLHF objective into a new objective that we can directly minimize. I’m not actually sure how well this works relative to RLHF in practice because I’ve never had access to these tools, but it is an RL-free alternative to preference-based alignment.</li>
</ol>

<h2 id="multi-modal-learning">Multi-modal Learning</h2>
<p>This year had a heavy focus on multimodal (mostly vision + language) models, with many companies/labs introducing their own shiny foundation models and associated datasets to compete with GPT-4. A lot of the desirable features in large multimodal models parallel that of large language models, so many of the research questions and themes are quite similar. In my mind, a core difference is the combinatorially larger amount of paired or associated data required to build a model that can interchangebly handle two different modalities. The obvious direction is to continue to scale the size of visual-language datasets with the size of new models, but I suspect that fundamentally answering how to “ground” two different modalities from a representation learning perspective may be able to reduce the necessary scale. At the end of the day though, this is still an open research question which I don’t know the answer to. The general themes I observed were</p>

<ol>
  <li>
    <p><strong>New Foundation Models.</strong> Large multimodal models, specifically vision-language, are the logical next progression to LLMs. Thus, it’s rather unsurprising that many research groups are racing to build the next big model <d-cite key="huang2023language,mizrahi2023m"></d-cite> with the same capabilities of LLMs like being instruction-tuned <d-cite key="dai2023instructblip"></d-cite> and in-context learning. However, it looks like the training mechanisms for making these models robust are still pretty elementary. For example, <d-cite key="huang2023language"></d-cite>, they simply treat everything as a token, but use a special embedding token for images and attentive pooling to reduce the complexity of these embeddings, then use the interleaved text and image data for standard log-likelihood optimization. At this conference, I didn’t see many actual models being showcased (although I’ve seen them over time on Twitter), but the large number of datasets seem to indicate that this is a growing direction.</p>
  </li>
  <li>
    <p><strong>Datasets and Benchmarks.</strong> We’ve observed several instances of “good” data being key to getting LLMs to work better. The same logic applies to multi-modal models, except because there generally is no 1-1 mapping between tokens in each modality, this is quite hard. Regardless, there are lots of multi-modal datasets and benchmarks being curated, either by scraping and filtering data on the internet <d-cite key="zhu2023multimodal,gadre2023datacomp"></d-cite> or by curating the data <d-cite key="pătrăucean2023perception,zhang2023m3exam"></d-cite>.</p>
  </li>
  <li>
    <p><strong>The Shared Representation.</strong> As far as I’m aware, the two main ways of building a multi-modal learning model are to tokenize each modality and train it as a decoder model on negative-log-likelihood loss <d-cite key="huang2023language"></d-cite> or train it CLIP-style<d-footnote>CLIP or Contrastive Language-Image Pre-Training is an contrastive learning-based encoding method for embedding images and language into the same embedding space. The benefit is that we can query into this embedding space using text or images, and query from this embedding space to generate either text or images. The idea was popularized from its used in DALL-E, and is the standard for text-to-image models. Read more here: https://openai.com/research/clip </d-footnote> as an encoder model. <d-cite key="oldfield2023parts,qiu2023controlling,samuel2023normguided,oh2023geodesic"></d-cite> In both cases, we want to understand this latent representation embedded either in the model layers or in the embedding space to see if we can exploit its properties <d-cite key="samuel2023normguided,oh2023geodesic"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Complex text prompts.</strong> It is known that text-to-image models often exhibit <em>bag-of-words behavior</em>, which means it lacks a strong understanding of syntax and logical quantifiers in a sentence. If you’ve ever played with Midjourney <d-footnote>A popular text-to-image generator: https://www.midjourney.com/home?callbackUrl=%2Fexplore</d-footnote> or DALL-E2 <d-footnote>A popular text-to-image generator by OpenAI: https://openai.com/dall-e-2</d-footnote>. Several works attempt to inject compositional reasoning in these models to solve this <d-cite key="doveh2023dense,zhao2023unicontrolnet"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Multi-modal video understanding.</strong> Processing videos adds a temporal dimension to these models that is quite difficult. Even standard video understanding models have been difficult to get right for a while now. The naive approach is to concatenate frames and pass them into a image-language model, so there has been some interest in making the language component temporally aware <d-cite key="yu2023selfchained"></d-cite>.</p>
  </li>
  <li>
    <p><strong>All the same questions for LLMs.</strong> Fundamentally, these models are just huge transformers. Even the vision components are basically just LLMs where the vocabulary is image patches (although the vocabulary is much bigger and not fixed I suppose). Regardless, any open problem for an LLM (efficiency <d-cite key="luo2023cheap,wortsman2023stable"></d-cite>, robustness<d-cite key="zhao2023evaluating,yin2023vlattack"></d-cite>, fine-tuning algorithms <d-cite key="wu2023parameter,cheng2023metaadapter"></d-cite>) is essentially also a problem for multi-modal models. The difference though, is that we can assume that we are given a model that is good at each modality, so “bridging” the modalities is what we need to actually solve.</p>
  </li>
</ol>

<h3 id="multimodal-interesting-papers">[<span style="color:orange">Multimodal</span>] Interesting Papers</h3>
<p>There are a lot of what I like to call <em>low-hanging fruit</em> in multi-modal models that have been solved, and while they are still interesting and definitely more applicable to most problems, I wanted to focus on some works that I thought were cool.</p>

<ol>
  <li>
<strong>4M: Massively Multimodal Masked Modeling</strong> <d-cite key="mizrahi2023m"></d-cite>: Apple doesn’t usually publish in machine learning conferences (e.g. they didn’t let me publish or extensively discuss my work when I was there), but I have to say, I thought this paper was pretty cool. I mentioned in point (3) that a lot multi-modal models are either decoder-based (token-style) or encoder-based (embedding-style), but the authors of this work discretize the shared embedding space and embed using tokens instead of a continuous embedding.</li>
  <li>
<strong>Connecting Multi-modal Contrastive Representations</strong> <d-cite key="wang2023connecting"></d-cite>: The idea here is that as we continue adding modalities to a shared representation space, we ideally want to use as little paired data as possible (for $N$ modalities, you would need $\binom{N}{2}$ sets of paired data). So we want to leverage pre-existing multimodal models, say a visual-language and a language-audio, and combine their representations without the need for visual-audio data. They effectively learn the projection function from both pre-trained models to the shared representation space, and motivate the loss required to align semantically similar embeddings.</li>
  <li>
<strong>A Theory of Multimodal Learning</strong> <d-cite key="lu2023theory"></d-cite>: This is an interesting paper on trying to formalize multimodal learning, although I don’t exactly understand how this differs from standard unimodal training. My understanding is that the primary limitation is data, and we can basically treat two modalities as distinct subspaces in some larger “unimodal” space. But regardless, they provide a formal differentiation between multimodal and unimodal learning and prove some standard ML theory bounds for an empirical risk minimization (ERM) algorithm.</li>
  <li>
<strong>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</strong> <d-cite key="yin2023vlattack"></d-cite>: They use some cute tricks for developing black-box adversarial attacks on vision-language models by considering perturbations for both modalities in isolation, as well as for image-text pairs. Similar to above, it’s not clear exactly why we need a distinct “multimodal” strategy for this kind of stuff, but perhaps more works into this area will provide more insight.</li>
  <li>
<strong>Geodesic Multi-Modal Mixup for Robust Fine-Tuning</strong> <d-cite key="oh2023geodesic"></d-cite>: We
want to understand the landscape of multi-modal embeddings and see if we can impose nice
properties of this space like making it isotropic. In this paper, they first propose that CLIP
embeddings (ZS) and naively fine-tuned embeddings (FT) have an inherent uniformity issue that
distinctly separates “text” and “images” into different subspaces. Ideally though, they argue
that we want the distribution over the space (they constrain it to be a hypersphere) to be based
on the semantics. Their method proposes to mold this space by generating “mixed” hard-negative
samples to use with the standard contrastive loss during fine-tuning.</li>
</ol>
<figure>
<center>
   <img src="/assets/img/oh2023geodesic.png" style="width:60%" alt="Visualization of CLIP embedding space.">
</center>
</figure>
<p>I’m curious because in the past, isotropic properties of “word embeddings” was thought to be a
   necessary thing, but it turns out we don’t really care, and a lot of methods that try to
   constrain this didn’t turn out to be that useful. I wonder how that applies here.</p>

<h2 id="transformers">Transformers</h2>
<p>With the Transformer module being the standard building block for scalable models, it is important that progress is made on improving their usage as a whole. Just as a side note, I think generally the term “transformer” is now overloaded to mean any structure using positional encodings plus an attention-mechanism, feedforward layers, and normalization in some repeated fashion, and does not necessarily refer to the original Transformer architecture. Interestingly, one thing I didn’t really find at this conference was investigating the enforcement of constraints or inductive biases like equivariance to Transformers, which may be an indicator of Sutton’s Bitter Lesson <d-footnote>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</d-footnote>. The general themes were</p>
<ol>
  <li>
<strong>Studying Transformer Models.</strong> Are there provable or empirically well-understood limitations of Transformers that may be severely limiting for future research directions? We know that Transformers seem to excel at reasoning but also frequently fail in simple cases, so <d-cite key="dziri2023faith"></d-cite> argue through a series of compositional tasks that these models (GPT-3,4) tend to <em>pattern match reasoning chains</em>. Furthermore, in <d-cite key="sanford2023representational"></d-cite>, they try to examine the function classes that Transformers can efficiently approximate, which is especially important for upper bounding the representational capacity of models as they scale. Finally, there are works that examine/ablate features of the Transformer <d-cite key="kazemnejad2023impact"></d-cite> to study their impact.</li>
  <li>
<strong>Efficiency.</strong> How do we modify parts of the transformer to be more efficient to 1) scale them for bigger models and 2) use them on low-compute devices? In <d-cite key="baykal2023alternating"></d-cite>, they selectively act on a fixed block of the embeddings at any layer to increase model capacity while keeping inference latency fixed. In <d-cite key="anagnostidis2023dynamic"></d-cite>, they train layers to selectively drop tokens by imposing a sparsity constraint that affects their modified attention mechanism. Meanwhile, works like <d-cite key="xi2023training"></d-cite> focus on preserving performance for quantized transformers and <d-cite key="liang2023mcuformer"></d-cite> focus on deploying transformers on microcontrollers.</li>
  <li>
<strong>Modifications to Attention.</strong> Can we make attention mechanisms more efficient <d-cite key="chen2023primalattention"></d-cite>? As in sub-quadratic runtime <d-cite key="anagnostidis2023dynamic,yu2023megabyte"></d-cite>? Sparse <d-cite key="pagliardini2023faster"></d-cite>? Or can we even replace attention <d-cite key="fu2023monarch"></d-cite>?</li>
  <li>
<strong>Memory.</strong> Can we increase context-window lengths or add external memory for transformers? This question is tied heavily to LLMs, and hence the methods (external memory source or summarization) are similar. Other than the works tied to LLMs, I only really found <d-cite key="zeng2023vcc"></d-cite>, which basically learns to compress token sequences into “VIP”-tokens that represent what’s most important. My only concern is whether these tokens are domain-specific and how a fully-trained model fairs for transfer learning to other modalities.</li>
</ol>

<h3 id="transformers-interesting-papers">[<span style="color:orange">Transformers</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>Faith and Fate: Limits of Transformers on Compositionality</strong>  <d-cite key="dziri2023faith"> </d-cite>: It has always unclear how good Transformers are at compositional reasoning, and this paper tries to uncover this question in a systematic way. As in, they literally break down each task (they are mostly computational tasks like multiplication and dynamic programming puzzles) into a computation graph and train their models in a bunch of different ways, ultimately concluding that Transformers are good at pattern matching reasoning chains, but not necessarily extrapolating reasoning itself.</li>
  <li>
<strong>Geometric Algebra Transformer</strong> <d-cite key="brehmer2023geometric"> </d-cite>: This paper is super cool and really well written. It’s a fairly non-traditional work that enforces equivariance with respect E(3) <d-footnote>https://en.wikipedia.org/wiki/Euclidean_group</d-footnote>, which is all linear combinations of translations, rotations, and reflections of 3D Euclidean space. This is particularly useful for learning representations of geometric data, and they apply their Transformer to a downstream planning task and demonstrate that it still works even when we don’t want to enforce this constraint.</li>
  <li>
<strong>Pretraining Task Diversity and The Emergence of Non-Bayesian In-context Learning for Regression</strong><d-cite key="raventós2023pretraining"> </d-cite>: We’ve always been curious how well models generalize to information not present in the training data, but this paper takes this question with a bit more abstraction, examining the effectiveness of in-context learning on tasks not present in the training data. They propose a “task diversity threshold” and claim that in-context learning emerges if the pre-training data is sufficiently diverse.</li>
  <li>
<strong>Fast Attention Requires Bounded Entries</strong><d-cite key="alman2023fast"> </d-cite>: Theoretically-motivated result on why we want entries in the attention matrix to be relatively small with respect to the matrix size if we want to speed up computations through approximation algorithms. It’s a pretty neat work, and I don’t necessarily think it’s intuitively obvious why this holds.</li>
  <li>
<strong>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment</strong><d-cite key="ni2023transformers"> </d-cite>: Since Transformers are now appearing in almost every non-compute-sensitive task, it’s interesting to understand why they work so well. This paper looks into the performance model-free RL agents on specific tasks designed to evaluate long-term memory and the efficiency of credit assignment. They find that (rather unsurprisingly), Transformers are useful for storing in-episode memory, but they do not solve the long-standing credit assignment problem, which means they are not the key to solving RL.</li>
  <li>
<strong>MotionGPT: Human Motion as a Foreign Language</strong><d-cite key="jiang2023motiongpt"> </d-cite>: We have generally settled on the notion that discrete tokenized representations are quite nice for Transformers, and this paper takes that one-step further by tokenizing human motion frames. The use-case is interesting, but I’m also interested in extending this kind of tricks for arbitrary modalities. Maybe we can see Transformers used for just about any kind of prediction!</li>
</ol>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>Reinforcement Learning was a huge topic this year, with many papers discussing RL in the context of other works like diffusion models <d-cite key="he2023diffusion,kang2023efficient"> </d-cite> or in-context learning <d-cite key="lee2023supervised"> </d-cite>. There is a distinction between classical RL works and deep RL works, the former of which are primarily theoretical, and the latter of which are primarily empirical. The main difference is the use of a neural network to approximate tabular mappings, especially in settings involving an infinite or combinatorially large state and/or action space. I’m not entirely sure what direction the field has been in the past few years because it is so broad, but I have noticed an emphasis on sample efficiency and the utilization of priors to accelerate RL exploration. This is probably because earlier successes in deep RL have primarily been through OpenAI or DeepMind brute forcing domain-specific trajectories into a model, so we have proven that deep RL works and can focus on efficiency. Lastly, I noticed a lot of papers related to offline RL <d-cite key="chen2023conservative,hong2023beyond"></d-cite>, where models make updates without interacting with the environment.</p>
<ol>
  <li>
<strong>Robustness.</strong> I’m specifically referring to the robustness of RL training algorithms and preventing failure modes. It is fairly well-known that RL is quite delicate and requires a lot of tricks <d-cite key="rlblogpost"> </d-cite> to get working in practice, so it is unsurprising that a lot of work goes into improving the robustness of these algorithms. There are many failure modes of RL that are addressed in this conference, and I highlight them below:
    <ol>
      <li>Balancing the ratio of updates to timesteps is important for trading off convergence and sample efficiency. To prevent <em>primacy bias</em> (favoring early explorations), deep RL methods often perform a “reset” of their weights while storing the transition data in a replay buffer. Doing this can cause the model to diverge on reset, so <d-cite key="kim2023sampleefficient"></d-cite> attempt to circumvent this issue by using an ensemble of agents and perform random resets so at least one agent is not reset.</li>
      <li>We often want RL agents to act <em>conservatively</em>, so when they reach an unseen state, they do not act wild. In <d-cite key="chen2023conservative"></d-cite>, they add penalties to out-of-distribution states and prove that in an offline RL setting, they achieve a conservative estimation of the expected value function.</li>
      <li>Deploying RL in realistic settings often implies the need for safety constraints to avoid exploring unsafe states. Works like <d-cite key="wachi2023safe,kim2023sampleefficient"></d-cite> look into imposing these constraints with high probability.</li>
      <li>There are many tricks involved in RL training, many of which are problem-dependent. For example, importance weighting over the training dataset in offline RL to prevent being influenced by low-reward trajectories <d-cite key="hong2023beyond"></d-cite>. Or reducing variance in the learning process with multi-step surrogate rewards <d-cite key="zhong2022long"></d-cite>. In <d-cite key="ma2023learning"></d-cite>, they analyze why augmenting visual observations leads to better sample efficiency. Some works even investigate tricks from other models like reward scaling and regularization schemes and apply them more generally <d-cite key="sullivan2023reward"></d-cite>.</li>
    </ol>
  </li>
  <li>
<strong>Improving Exploration by Leveraging Priors.</strong> Reward functions play a huge role in the convergence of RL training. In most settings, the only “true” reward is a sparse reward given for achieving a goal (e.g. winning a chess game). However, propagating this sparse reward through a combinatorially large state and transition function space is fundamentally difficult, so people often design intrinsic reward functions to better guide an agent towards maximizing the true reward. Some methods attempt to generally identify good transitions and states to explore <d-cite key="lin2023mimex,jain2023maximum"></d-cite> by looking for state diversity and orthogonality, while other methods focus on automating a reward designer’s intent by conditioning on images, videos, or language through some pre-trained embedding space and push exploration in a certain direction using these exploration “hints” <d-cite key="kim2023guide,escontrela2023video,wu2023read,gupta2023behavior"></d-cite>. Meanwhile, <d-cite key="nikishin2023deep"></d-cite> propose that even with sufficient exploration, a model may not learn a good policy, so dynamically modifying the model during training may fix that!</li>
  <li>
<strong>Learning World Models for Model-based RL (MBRL).</strong> If you’re familiar with the basic formulation for the dynamic programming update functions used in RL, you’ll know that knowing the <em>transition function</em>, or the model of the environment dynamics, is an extremely powerful guarantee that you generally do not have. However, it is possible to try and “learn” this model to apply MBRL methods. One of my research interests as of late is learning and editing world models using language, so it is pretty exciting to see these works. A central theme in the use of world models is learning and acting “in imagination”, which means treating the world model itself as an environment that the agent can interact in, which is especially useful for environments where interacting is costly or dangerous. In <d-cite key="chung2023thinker"></d-cite>, they use the world model as a way for the model to “think” before it generates an action. In <d-cite key="guan2023leveraging"></d-cite>, they learn strictly formatted world models in a standardized language <d-footnote>https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language</d-footnote> that models can interact with. Furthermore, many world models use an recurrent neural network (RNN) as the backbone for historical reason, so in <d-cite key="deng2023facing"></d-cite>, they experiment with different models for the world model backbone and propose their own with better empirical performance.</li>
  <li>
<strong>Multi-agent RL.</strong> Multi-agent RL (MARL) involves multiple learning agents in the same environment, with most papers I’ve found generally focusing on cooperative or mixed-sum games. A lot of similar research themes in RL apply here as well, but there is also room for game-theoretic analysis. A core limitation in prior MARL work is the assumption of a fixed task and fixed environment, so works like <d-cite key="mao2023multiagent,wang2023mutualinformation"> </d-cite> extend standard frameworks to a multi-task learning setting. There are also works regarding how to leverage reward signals that apply to the team of agents versus a singular agent <d-cite key="yang2023hierarchical,hu2023differ"></d-cite> which is not obvious and directly influences the learned policy of each agent. Finally, because MARL is inherently expensive (you’re launching several agents at once!), creating environments that are compute-friendly is a topic of interest <d-cite key="lechner2023gigastep,suarez2023neural"></d-cite>.</li>
  <li>
<strong>Goal-conditioned RL.</strong> Goal-conditioned RL (GCRL) is a class of algorithms or problems where decisions are conditioned on both a state and a goal. There were only a few GCRL papers at this conference with an emphasis on offline RL strategies such as using sequence modeling over trajectories as a goal <d-cite key="zeng2023goalconditioned"></d-cite>, but I did find an interesting work on using the distance (they define their own metric) between the goal distribution and state visitation distribution as an extra reward signal for exploration <d-cite key="agarwal2023fpolicy"></d-cite>.</li>
  <li>
<strong>Theoretical Analysis.</strong> There were a lot of papers on bandit problems (especially adversarial or contextual bandits) <d-cite key="olkhovskaya2023first"></d-cite> and provable regret/convergence bounds <d-cite key="whitehouse2023on"></d-cite>, most of which I was not really able to understand. While I think following the math itself and reading through it is quite fun, I’m just not familiar with what problems people are interested in and what is unsolved, so it’s hard to gather from the abstracts or even a quick skim of the papers what the immediate impact is. That’s not to say that these works are not interesting or useful, but I am going to be careful not to say something false about their results. However, I did find two works that prove convergence guarantees for <em>deep RL</em>!. In <d-cite key="zhang2023convergence"></d-cite>, they prove convergence guarantees for deep Q learning using $\epsilon$-greedy exploration under accelerated gradient descent (momentum) under some pretty minimal assumptions. In <d-cite key="gaur2023global,zhong2023theoretical"></d-cite>, they limit their analysis to linear MDPs and simplified neural networks, but show convergence guarantees for actor-critic and proximal policy optimization (PPO) methods respectively.</li>
</ol>

<h3 id="rl-interesting-papers">[<span style="color:orange">RL</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>Conditional Mutual Information for Disentangled Representations in Reinforcement Learning</strong> <d-cite key="dunion2023conditional"> </d-cite>: I haven’t really seen prior works in RL that try to tackle disentanglement in feature representations, but the motivating factor here is that the exploring RL agent does not sufficiently capture the environment dynamics and instead learns spurrious feature correlations. I’m not sure if the technique they used was done for image/video works in the past, but it makes sense to me that this is not an RL or agent-specific technique.</li>
  <li>
<strong>Creating Multi-Level Skill Hierarchies in Reinforcement Learning</strong> <d-cite key="evans2023creating"></d-cite>: When I was playing around with PySC2 (Starcraft II RL environment), I used to always be confused how an RL agent would feasibly learn these complex chains of actions (turns out the answer was tons of data). Another approach outlined in this paper is to explicitly map out hierarchical skill trees, where the lowest levels are explicit actions and higher levels are more abstract, learnable skills. I’ve seen a similar idea applied to LLMs where you can explicitly query the LLM to reason about what it should do, but in RL its more robust but less interpretable.</li>
  <li>
<strong>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</strong><d-cite key="chen2023efficient"></d-cite>: An interesting question in RL is how much “error” is induced by a partial loss in observability. They bound the worst-case performance on control systems depending on the expected percentage of missing states and show that RL is still applicable to this class of problems in an efficient way (which generally means poly() any environment parameters).</li>
  <li>
<strong>Learning to Influence Human Behavior with Offline Reinforcement Learning</strong> <d-cite key="hong2023learning"></d-cite>: I feel like in any multi-agent or game theoretic setup, we always assume other players are playing optimally. This paper is unique in that they try to learn a policy in a cooperative multi-agent setup that assists the other agent towards a certain desirable behavior. The environment is a grid-world version of Overcooked <d-footnote>https://www.team17.com/games/overcooked/</d-footnote> which I find really funny, as this is the perfect environment for this kind of model.</li>
  <li>
<strong>Is RLHF More Difficult Than Standard RL?</strong> <d-cite key="wang2023rlhf"></d-cite>: They reduce RLHF and general preference-based reward signals to different classes of known problems in RL, motivating how RLHF is not inherently more difficult than standard RL problems. The paper goes into quite a few instances of preferenced-based RL, and is fully theoretically motivated.</li>
  <li>
<strong>A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</strong> <d-cite key="zhong2023theoretical"></d-cite>: I’ve understood PPO as a series of empirical tricks and approximations to the theoretically motivated Trust Region Policy Optimization (TRPO), so I always thought that studying its theoretical properties to provably converge has been lacking. Even if this study is applied to a simple RL setting, it’s an important first step towards theoretically motivating PPO.</li>
</ol>

<h2 id="generative-ai">Generative AI</h2>
<p>Generative AI as a whole has also been booming through 2023, especially as a marketable product. Given how accessible and easy to customize they have gotten, I think that the common layperson should understand at a high-level what kind of generative AI is out there. I should note that I structured this section to mostly exclude large language models or language as a modality altogether. These papers have mostly been targeted towards diffusion models, although there were still a few works at NeurIPS 2023 that focused on GANs like <d-cite key="hou2023augmentationaware,yang2023learning"></d-cite>. The general themes I’ve observed are as follows:</p>
<ol>
  <li>
<strong>Pre-trained Foundation Models.</strong> Generative models rely on a large backbone model that encodes the knowledge base of the domain that it acts on <d-cite key="wang2023facecomposer,liu2023weakly,chen2023bridging"> </d-cite>. I do wish there were some papers discussing techniques for scaling models robustly and efficiently, but perhaps it comes with experience.</li>
  <li>
<strong>2D to 3D View Synthesis.</strong> 2D generative models are at a pretty decent state, so extending their abilities to create 3D generative models is an open research question. Prior work on novel view synthesis encode scenes in the weights of the model, but recent work has looked models that can generalize to different scenes. For example, in <d-cite key="peng2023gens"></d-cite> they train a signed-distance function (SDF) based reconstruction model to generate 3D meshes from generic 2D image views at inference time. In this field, spatial hash encodings have proven to be effective on GPU hardware for drastically speeding up 3D generative models, so <d-cite key="wang2023masked"></d-cite> enables dynamic scenes (basically add time) to be encoded by learning to selectively employ different hash encodings for static and dynamic parts of the scene. Finally, with Segment Anything (SAM) <d-footnote>https://segment-anything.com</d-footnote> being an extremely powerful 2D vision-language foundation model capable of accurate semantic image segmentation, <d-cite key="cen2023segment"></d-cite> presents a way to extend this to 3D. This process involves generating view-conditioned prompts to generate views of an image and properly inverse projecting these 2D masks back to a 3D voxelized space. This work is exciting and the results are very noisy and not great, but it’s definitely an open research problem that will make significant progress soon!</li>
  <li>
<strong>Single-image 3D Reconstruction.</strong> An alternative to novel view synthesis is take a <em>single image</em> and try to extrapolate using domain knowledge what the 3D model looks like. Most of these methods leverage some kind of pre-trained 2D diffusion model to generate the alternate views, but they are distinct in how they choose to do this. Some do it by inferring 3D projective geometry <d-cite key="li2023generalizable,purushwalkam2023conrad"></d-cite> and others try optimizing directly with generations from a multi-view 3D diffusion model <d-cite key="liu2023one2345"></d-cite>.</li>
  <li>
<strong>Generating on new Modalities.</strong> Generative AI is not limited to language and vision. Audio and speech generative models <d-cite key="copet2023simple,le2023voicebox,deshmukh2023pengi"></d-cite> have found that tokenized representations of other modalities can be used in Transformers. Of course, the details are not that simple, and from my understanding encoding the tokens requires working over a spectrogram representation that is non-trivial.</li>
  <li>
<strong>Text-to-video.</strong> I’m sure you may have seen some clips of text-to-video AI on social media, and while it is impressive, it is far from being as robust as the text-to-image models. A lot of work goes into ensuring causal and cross-frame temporal consistency in these generations <d-cite key="wang2023videocomposer"></d-cite>. While these models have an obvious use case for generating videos, I did find an interesting use-case of these models as a form of planning for policy generalization <d-cite key="du2023learning"></d-cite>.</li>
</ol>

<h3 id="diffusion-models">Diffusion Models</h3>
<p>I decided to add a separate subsection on diffusion models with a focus on techniques that improve the base diffusion model process <d-footnote>As a sidenote, check out my roommate's repository on a really simple and intuitive implementation of diffusion models: https://github.com/edogariu/nice-diffusion</d-footnote>. Honestly, there were so many diffusion model papers (also applied to other fields like RL <d-cite key="he2023diffusion,kang2023efficient"></d-cite>) that this subsection alone is more rich than most of the other sections. From my understanding, diffusion models can be viewed from several different “lenses”, with one being as a Gaussian process and another being through Langevin dynamics. You can even view diffusion models as a sequence of Variational Autoencoders (VAE). For a bit of perspective, I had the opportunity to speak with Jascha-Sohl Dickstein <d-footnote>Jascha was first-author of the original diffusion models paper: https://arxiv.org/abs/1503.03585</d-footnote> through the AI Tiger Trek trip I helped organize last April, and he said that various practical formulations of the diffusion model implementation had sprung up at around the same time from these different viewpoints. There is a lot of ongoing research into using these models effectively, but here is what I noticed from this conference:</p>
<ol>
  <li>
<strong>Inference-speed.</strong> There have been strides by labs to make the training and fine-tuning process of diffusion models cheaper <d-footnote>https://www.mosaicml.com/blog/stable-diffusion-1</d-footnote>, but inference remains quite expensive. In <d-cite key="zhao2023unipc"></d-cite>, they motivate a training-free sampling method for performing an extremely low number of sampling steps (&lt;10) while maintaining generation quality (for comparison, the standard amount is ~1000 steps and ~20 is considered low from prior works), while in <d-cite key="xue2023sasolver"></d-cite> they motivate stochastic sampling solvers and relate them to other popular solvers like the previously mentioned one.</li>
  <li>
<strong>Interpreting the Latent feature representation.</strong> Can we understand the features that are learned by a diffusion model? This is generally done by probing the embedding space and clustering or checking if classes are linearly separable <d-cite key="zhang2023tale"></d-cite>, where they find Stable Diffusion features exhibit good spatial information but worse semantic understanding compared to another popular embedding method. Another step is to investigate and probe the latent seed space used to condition the generator, as done in <d-cite key="samuel2023normguided"></d-cite>.</li>
  <li>
<strong>Multi-input.</strong> Similar theme to multi-modal models in general since they are so closely related, but can we develop diffusion models that take both text and visual data as input <d-cite key="vuong2023languagedriven"></d-cite> and produce any desired output<d-cite key="tang2023anytoany"></d-cite>? Can we also make it robust to composition and more complicated prompting <d-cite key="doveh2023dense,zhao2023unicontrolnet"></d-cite>?</li>
  <li>
<strong>Filling missing information.</strong> Can we leverage diffusion models to fill in the missing gaps of information in an image or a dataset label <d-cite key="zhang2023unified,nguyen2023dataset"></d-cite>? This is actually really cool, because the implication is that unlabelled or noisy images contain enough structure to reconstruct the unknown parts without the model just making things up.</li>
</ol>

<h3 id="gen-ai--diffusion-interesting-papers">[<span style="color:orange">Gen AI + Diffusion</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images</strong> <d-cite key="wen2023treering"></d-cite>: Copyright and generative AI identification is going to become increasingly more important as the technology gets more accurate. This technique slowly applies an invisible watermark during the diffusion sampling process that is easy to recover when inverting the diffusion process. I have a feeling that similar to works on adversarial attacks, there is going to be a constant chase between watermark and watermark removal works in the near future.</li>
  <li>
<strong>Generator Born from Classifier</strong> <d-cite key="yu2023generator"></d-cite>: Can you take a trained image classifier and use it to generate images with minimal extra learning? We have seem class-conditional works like in diffusion models, but this work is trying to do something much stronger. This work is a first-step into leveraging a classifier for image generation, and they use the theory of Maximum-Margin Bias to extract training data information from the parameters of a classifier.</li>
  <li>
<strong>CL-NeRF: Continual Learning of Neural Radiance Fields for Evolving Scene Representation</strong> <d-cite key="wu2023clnerf"></d-cite>: NeRFs implicitly store the scene they are rendering in their weights, but they are generally fixed. But what if we want to capture an ever-changing scene? It seems natural to imply concepts in continual learning, as we mainly want to 1) not forget important static elements of the scene during weight updates and 2) dynamically add components to the scene through weight updates, and this work is a first step into solving this problem.</li>
  <li>
<strong>UE4-NeRF:Neural Radiant Field for Real-Time Rendering of Large-Scale Scene</strong> <d-cite key="gu2023uenerfneural"></d-cite>: As an avid fan of Unreal Engine and the games that have been produced by it, this is a really exciting work to me. There are companies like Luma and Volinga.ai that have a closed-source proprietary software for NeRF rendering in Unreal Engine, but this is the first work that open-sources it. I should note that their rendering process involves rendering sub-NeRFs in a partionined volume for efficiency purposes, but otherwise it follows a ray-marching procedure (ish).</li>
</ol>

<h2 id="computer-vision">Computer Vision</h2>
<p>Computer vision (CV) was the field that introduced me to the world of machine learning, and I had a glimpse when I was a little boy building robots with my Arduino of what it looked like pre-AlexNet. Perhaps because NeurIPS itself is not focused on CV, I was rather surprised by the themes I noticed. There seems to be a much larger emphasis on video understanding and human-centric perception, although fairness and bias still remains an issue that has yet to be addressed.</p>

<ol>
  <li>
    <p><strong>Open vocabulary methods</strong> for segmentation and understanding of semantics in images involves being able to adapt to labels unseen during training. In essence, we want to be able to generalize our models past fixed class labels so they don’t have to be re-trained every few months. My understanding is that with vision-language models being a thing now, these methods only need to generate suitable embeddings use with these methods <d-cite key="cui2023open"></d-cite>. A lot of works now focus on extending to 3D models as well <d-cite key="liu2023weakly,cao2023coda,vobeck2023popd"></d-cite>. I’m curious though how these methods handle language ambiguity and different abstractions of describing something, and whether or not this limitation is bottlenecked by the model’s language capabilities. I did find <d-cite key="wang2023hierarchical"></d-cite> that tries to address this in a hierarchical way.</p>
  </li>
  <li>
    <p><strong>Video understanding.</strong> I worked on a video understanding benchmark so I’m somewhat aware of the limitations in the field. Generally, video labels are quite difficult to procure, as they’re far more compositional and free-form, and they just take longer. At the same time, having models that can reason over videos is extremely useful because videos are the primary form of media consumed on the internet these days.Compared to language, videos take up much more memory, and finding associated labels through online scraping is hard. So an important work is to build up datasets and benchmarks<d-cite key="yang2023vidchapters7m"></d-cite>. Additionally, even with multi-modal models, they have not been sufficiently trained to understand temporal aspects of a video like actions and long-term causal reasoning, which works like <d-cite key="wang2023paxion,yu2023selfchained"></d-cite> make first steps towards addressing.</p>
  </li>
  <li>
    <p><strong>Human Data and Perception.</strong> 
I noticed some interest in human-centric perception, as we ideally want vision models to understand similarity the way we intuitively perceive it. In <d-cite key="fu2023dreamsim"></d-cite>, they propose a margin loss that shapes the embedding space based on human similarity judgement data. Meanwhile, in <d-cite key="tan2023egodistill"></d-cite>, they focus on ego-centric data (video footage from the perspective of a human). We also want models to be more accurate when perceiving humans, which <d-cite key="yuan2023hap"></d-cite> argues starts at the pre-training level. Lastly, there were a few papers on human-pose estimation, mainly for robustness on expressive poses <d-cite key="enpang2023robust,cai2023smplerx"></d-cite> and for improving accuracy on reconstructing the poses in 3D <d-cite key="zhao2023single"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Fairness.</strong> Fairness and bias is a long-standing issue in computer vision that is primarily rooted in dataset selection. A key research question is understanding which factors like human appearance <d-cite key="schumann2024consensus"></d-cite> or geographical location <d-cite key="gustafson2023pinpointing"></d-cite> are biased in our data. A further question is how to augment our data in the short term to mitigate these biases <d-cite key="teo2023measuring"></d-cite>.</p>
  </li>
</ol>

<h3 id="computer-vision-interesting-papers">[<span style="color:orange">Computer Vision</span>] Interesting Papers</h3>

<ol>
  <li>
    <p><strong>Segment Everything Everywhere All at Once</strong> <d-cite key="zou2023segment"></d-cite>: This is Microsoft’s alternative to Meta’s Segment Anything (SAM) model, with a focus on semantic-oriented text prompting for segmentation. I haven’t had the opportunity to compare the two, but they claim that their method captures semantics more accurately.</p>
  </li>
  <li>
    <p><strong>Diversifying Spatial-Temporal Perception for Video Domain Generalization</strong> <d-cite key="lin2023diversifying"></d-cite>: When you build a video understanding model, you of course want it to generalize to unseen domains. For video domains, however, which are high-dimensional and contain a lot of complicated structure, unless the data is perfectly diverse (requiring a lot of video data), you want to be able to filter out domain-specific cues from your training data and identify domain-invariant cues that will help as a prior for generalization. This work attempts to motivate how to identify these cues at a spatial and temporal level, and I think ideas from this work can be extended to other fields as well.</p>
  </li>
  <li>
    <p><strong>DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions</strong> <d-cite key="wang2023droppos"></d-cite>: Empirically, we have found that Vision Transformers (ViT) kind of suck at understanding positional encodings, i.e. they are sort of position invariant. In some cases this is a desirable property, but we do want these Vision Transformers to be spatially aware, so this paper offers a simple fix: in addition to standard ViT training, add a secondary objective to predict the position of the token/patch.</p>
  </li>
  <li>
    <p><strong>Patch N’ Pack: NaViT, A Vision Transformer for Any Aspect Ratio and Resolution</strong> <d-cite key="dehghani2023patch"></d-cite>: They train with token packing strategies used for language models, which involves feeding in multiple images (in tokenized patches) with varying resolutions during train time. They claim it works for arbitrary image resolutions, but I’m pretty sure it’s the resolution change they used during training. Regardless, it is an extremely useful work that applies to a wide range of visual tasks.</p>
  </li>
  <li>
    <p><strong>Color Equivariant Convolutional Networks</strong> <d-cite key="lengyel2023color"></d-cite>: I’m a big fan of equivariance as an inductive bias, and this paper is no exception. We generally want to separate geometry and color in visual models, and this work builds a plug-in block for common convolutional neural network architectures to add color equivariant convolution operations. These layers are not insensitive to color variation; rather, they allow for sharing information about visual geometry across different colors.</p>
  </li>
</ol>

<h2 id="adversarial-attacks-and-model-poisoning">Adversarial Attacks and Model Poisoning</h2>
<p>Generally, adversarial attacks can be partitioned into two main classes: white box, where an attacker has access to the model weights (e.g. for any open-source models), and black-box, where an attacker can only use model outputs (e.g. attacking GPT4). I still think that most attacks are pretty domain-specific, so I’ve decided to generally separate the themes based on domain (e.g. language, vision, RL) rather than the type of attack (e.g. red-teaming, gradient-based, etc.) Lastly, this section is primarily dedicated to attacks that alter or manipulate the outputs of a model to be harmful or incorrect. There is another class of attacks that try to reconstruct training data using model outputs, but I decided to move that to the <a href="#privacy-and-federated-learning">section on Privacy and Federated Learning</a>.</p>

<ol>
  <li>
<strong>Robustness vs. Performance &amp; Speed</strong> is an important tradeoff when developing models and considering defense mechanisms against attacks. Adversarial defenses have extra overhead, especially those with certification (provable robustness within $\epsilon$-ball), so it is important to understand this tradeoff. I was only able to recall <d-cite key="mao2023taps"></d-cite> in this conference that tackles this issue.</li>
  <li>
<strong>Model Poisoning.</strong> Distinct from the other attacks, model poisoning involves slightly editing the training data to plant exploits or backdoor triggers into a model <d-cite key="shu2023exploitability"></d-cite>. It is possible that poisoned models are deployed in the wild, so you may not even have access to modify its internals. So essentially, you can either fine-tune the model <d-cite key="zhu2023neural,tang2023setting"></d-cite> or directly augment its outputs with noise <d-cite key="shi2023blackbox"></d-cite> to remove the poisoning.</li>
  <li>
<strong>Attacks on LLMs.</strong> Given the theme of multi-modal models, there has been a few works examining defenses for vision-language models against known attacks for language or vision models <d-cite key="yin2023vlattack,zhao2023evaluating"></d-cite>. However, given the discrete nature of token representations, simple black-box attacks using seemingly harmless tokens like an exclamation point are possible <d-cite key="wang2023punctuationlevel"></d-cite>. Lastly, attacks and defenses against model poisoning were discussed <d-cite key="shu2023exploitability,tang2023setting"></d-cite>.</li>
  <li>
<strong>Attacks on Images.</strong> Unlike language, raw image representations are high-dimensional and therefore easily susceptible to noise and perturbation effects. In <d-cite key="gao2023perturbation"></d-cite>, they propose a noise generator that transfers black-box attacks from one image model to another. These attacks and defenses have evolved over the years, but it’s still an open research question even on older datasets like ImageNet <d-cite key="singh2023revisiting"></d-cite>.</li>
</ol>

<p>As an aside, there were lots of adversarial attack papers this year using specific attacks, targetting specific models (e.g. MARL <d-cite key="liu2023efficient"></d-cite>, federated learning <d-cite key="zhang2023afl"></d-cite>, graph neural networks <d-cite key="gosch2023adversarial"></d-cite>), or proposing specific defenses that are not reflected in the points above. I had a lot of trouble trying to categorize this section properly because of how diverse it is. This field is naturally reactive, as when someone comes up with a defense, someone will come up with an exploit (e.g. <d-cite key="kang2024diffattack"></d-cite>), and vice-versa. Some works even try to theoretically motivate the nature and existence of completely robust models like <d-cite key="pal2023adversarial"></d-cite>, but overall, it was hard for me to pinpoint the direction of these works at this conference.</p>

<h3 id="adversarial-attacks-and-model-poisoning-interesting-papers">[<span style="color:orange">Adversarial Attacks and Model Poisoning</span>] Interesting Papers</h3>
<ol>
  <li>
    <p><strong>Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypot</strong> <d-cite key="tang2023setting"></d-cite>: The strategy in this paper is really cool: basically, they first notice that backdoor triggers in poisoned models are “obvious”, in the sense that they appear in lower layers of the model in an obviously linearly separable way. Intuitively, this makes sense, as poisoned outputs are structurally out-of-distribution from “human language”. From here, they basically add these small “honeypot” layers (just a 1-layer transformer) with a classification head that purposefully get “poisoned” early on, and they use this auxiliary loss to weight the actual cross entropy loss. I think that is a really neat example of exploiting structure and abstract representations of data to achieve an effect.</p>
  </li>
  <li>
    <p><strong>Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features</strong> <d-cite key="zhu2023neural"></d-cite>: Poisoned data generally looks like a regular image with some small perturbations or tiny trigger features, so this work looks into inserting learnable filters into a trained model that reverses and removes these features while acting as an identity map for everything else. The main issue I see with this approach is that you have to know the type of adversarial attacks against the model a priori, so a fixed filter needs to be updated when new attacks arise, i.e. there are no provable guarantees against general adversarial perturbations.</p>
  </li>
</ol>

<h2 id="knowledge-distillation-and-memory-reduction-schemes">Knowledge Distillation and Memory Reduction Schemes</h2>
<p>As much as we like scaling models, building smaller models that can run on accessible hardware is extremely important for the growth of our community. This section is mostly referring to scaling down models so they can run <strong>on inference time</strong> on smaller hardware, which is a matter of memory efficiency. TinyML <d-footnote>A whole field of study is on ultra-low power ML: https://www.tinyml.org</d-footnote> works take it a step further and try to deploy these models on embedded systems and micro-controllers <d-cite key="liang2023mcuformer"></d-cite>. The three primary methods are weight quantization<d-footnote>Weight quantization involves using a lower-precision datatype for representing weights, which can reduce memory complexity by a multiplicative factor.</d-footnote>, pruning<d-footnote>There is extensive literature on network pruning, and it is actually quite complex. Pruning is literally taking out parts of the network (hence the name), but choosing what to take it is important. It is also important to select the time that you prune (before training, during training, after training), as this affects the performance and overhead of the pruning process. </d-footnote>, and knowledge distillation<d-footnote>Knowledge distillation involves taking a larger model and cloning its behavior in a smaller model, also known as "distilling". The idea is that a large models are often over-parameterized (there are benefits for training in this way), so once we have the model trained, we can cut down on its capacity by training a smaller model.</d-footnote>.</p>

<ol>
  <li>
<strong>Memory Reduction Techniques.</strong> We generally are intered in tradeoffs for different memory reduction schemes such as pruning and quantization. In <d-cite key="kuzmin2023pruning"></d-cite>, they claim that quantization is almost always better unless you care about extreme compression. Similar to older work doing weight quantization for MLPs and CNNs, newer works at this conference do it for transformers <d-cite key="xi2023training,dong2023packqvit"></d-cite>.</li>
  <li>
<strong>Lottery Ticket Hypothesis (LTH)</strong><d-footnote>The lottery ticket hypothesis is the notion that dense neural networks contain a much smaller subnetwork that accounts for most of the performance. Finding these subnetworks through pruning would, in theory, preserve performance while significantly reducing memory. Read more from the original paper: https://arxiv.org/abs/1803.03635 </d-footnote><strong>.</strong>  Following the original LTH, we want to understand what metrics (e.g. weight magnitude, gradient flow) and structure are useful for pruning modern architectures like LLMs, but this also depends on what we are pruning for. In <d-cite key="kurtic2023ziplm"></d-cite> they prune LLMs based on run-time bottlenecks for inference speed-ups, while in <d-cite key="ma2023llmpruner"></d-cite> they focus on shrinking the model. Meanwhile, in convolutional neural networks, pruning based on empirics has been widely studied, so <d-cite key="dacunha2023polynomially"></d-cite> provide some theoretical motivation into better pruning based on the structure of the model.</li>
  <li>
<strong>Knowledge Distillation</strong>. Knowledge distillation (KD) is an approach for trying to force a small student model to mimic the output probabilities of a larger teacher model. KD has seen a wide array of techniques being used to preserve functionality of the teacher in the student, and many empirical experiments have been done in the past to evaluate the lower-bound capacity of student models. Nevertheless, the works at this conference were pretty unique. In <d-cite key="huang2023knowledge"></d-cite> they observe that student models have noisier features and attempt to de-noise them using diffusion. In <d-cite key="gupta2023concept"></d-cite>, they motivate “concepts” in intermediate layers as an auxiliary signal for distillation. Finally, <d-cite key="ojha2023knowledge"></d-cite> investigates whether properties like adversarial robustness, invariances, and generalization are transferred effectively during distillation.</li>
</ol>

<h3 id="memory-reduction-interesting-papers">[<span style="color:orange">Memory Reduction</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</strong> <d-cite key="liang2023mcuformer"></d-cite>: They push the modern limits of Vision Transformers on ultra-low cost systems, using neural architecture search (NAS) to search for a compute-optimal architecture while also writing a library for performing each inference-level computation in a Vision Transformer efficiently. I’m not that aware of the pre-existing literature in this space, but this is one of the first papers I’ve seen do it for Vision Transformers.</li>
  <li>
<strong>What Knowledge Gets Distilled in Knowledge Distillation?</strong> <d-cite key="ojha2023knowledge"></d-cite>: Knowledge distillation is sort of this black-boxy approach where we try to get a small student model to be the same as a larger teacher model. It would be nice to know what kind of information easily transfers and even nicer to understand why, which this paper attempts to do. Most surprisingly, they find that <em>white-box vulnerabilities</em> in a teacher model transfer over to a student model despite being a different parameterization, which might be indicative of some structural similarities inherent to networks (it is inconclusive in this paper though). They try to motivate this transfer by a dimensionality argument to argue that the student model solution is unique, but honestly the argument is pretty weak because the assumptions are just generally untrue in almost any realistic problem where knowledge distillation is applied.</li>
  <li>
<strong>Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Lottery Tickets</strong> <d-cite key="dacunha2023polynomially"> </d-cite>: So I was really curious about this paper after reading through the abstract, because was is completely unclear to me how the Random Subset Sum problem <d-footnote>Subset sum is a classic NP-hard problem in CS theory where a program must decide if there exists a subset of a set of integers that sums to a number $T$. The randomized version is a set of random variables, and the sum can now be off by an error $\epsilon$ with high probability.</d-footnote> has anything to do with the existence of strong lottery tickets in an over-parameterized convolutional neural network.</li>
</ol>

<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>Graph neural networks (GNN) were really popular this year! I wish I had a stronger understanding overall of GNNs, but unfortunately I just haven’t found any specific use cases for them in my own research. I am aware of their use-cases in structured prediction (e.g. molecular dynamics <d-cite key="wu2023equivariant"></d-cite>, social networks) but their unique design and the prevalence of graph problems has allowed this field of research to grow steadily. I couldn’t really pinpoint the major themes at this conference, but I learned a few things about what people are interested in.</p>
<ol>
  <li>
<strong>Heterophily vs. Homophily.</strong> Earlier works with GNNs worked under the assumption of graph homophily, meaning similarly labelled nodes tend to be linked. This assumption neatly allows for even unsupervised methods to exploit graph structure when making predictions, but it is unclear what the impact of graph heterophily is on GNN performance <d-cite key="luan2024graph"></d-cite>. Thus, there has been work towards solving graphs under heterophily by focusing on non-local structure <d-cite key="liang2023predicting,liao2023ld"></d-cite>. In <d-cite key="platonov2023characterizing"></d-cite>, they even try to rigorously characterize the properties and effects of these node-level relationships.</li>
  <li>
<strong>Unsupervised graph learning.</strong> Unsupervised learning is natural in graph problems because regardless of the problem domain, the graph itself provides extremely useful structural information that can be leveraged for a prediction. There is still a lot ongoing research <d-cite key="tsitsulin2023graph,qiao2023truncated,sun2023lovsz"></d-cite> into identifying and targetting useful structure in graphs, which includes (1).</li>
  <li>
<strong>Spatio-temporal prediction</strong> involves time-series forecasting over spatially-varying data. This problem is significantly harder than stock prediction type forecasting over tabular data because of the inherent high dimensionality and structure (local vs. global) present in spatial data. Thus, a class of works this year <d-cite key="wu2023equivariant,xia2023deciphering,cini2023taming"></d-cite> have emerged to study these problems using GNNs.</li>
  <li>
<strong>Encoding representations in graphs.</strong> Typically GNN methods represent nodes or links with some kind of embedding representation, so understanding the mechanisms that shape these representations is important <d-cite key="wu2023demystifying"></d-cite>.</li>
</ol>

<p>Broadly speaking, a lot of advances in other fields that were discussed above are also active areas of research in GNNs (e.g. adversarial robustness <d-cite key="gosch2023adversarial,zhao2023adversarial"></d-cite>, interpretability <d-cite key="yin2023train"></d-cite>, multimodal <d-cite key="zhao2023gimlet"></d-cite>), so I expect to see a lot more advancements and use-cases of GNNs in the near future.</p>

<h3 id="gnn-interesting-papers">[<span style="color:orange">GNN</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>Zero-One Laws of Graph Neural Networks</strong> <d-cite key="adamday2023zeroone"></d-cite>: Zero-one laws generally study the limiting behavior of probabilities and show that they converges to $0$ or $1$. This paper proves equivalent zero-one laws for the outputs of certain classes of GNNs (e.g. boolean graph convolutional classifiers) as they get larger and larger. Practically speaking, I don’t currently see a use case for this kind of analysis, but it is cool nonetheless.</li>
  <li>
<strong>Unsupervised Learning for Solving the Travelling Salesman Problem</strong> <d-cite key="min2023unsupervised"></d-cite>: They use a simple unsupervised graph neural network with surrogate loss objectives that provably move towards the objective, that being minimizing the path cost and ensuring the path is a Hamiltonian cycle. I’m really curious to see future GNN works on approximating solutions to NP-hard/NP-complete problems based on derived surrogate objective functions.</li>
  <li>
<strong>Lovász Principle for Unsupervised Graph Representation Learning</strong> <d-cite key="sun2023lovsz"></d-cite>: Math researchers have done lots of incredible work in study global and local properties of graphs, and I expect that we will continue to see these results be useful in GNNs. Unsupervised learning for graph neural networks makes so much sense, because so much structure comes from the graph itself regardless of the domain it is describing. Having learned about Lovász numbers in an extremal combinatics course taught by Professor Alon Noga himself, it was cool to see them re-appear in an ML setting.</li>
</ol>

<h2 id="privacy-and-federated-learning">Privacy and Federated Learning</h2>
<p>Trust in AI and the companies that build these AIs is extremely important. This year’s conference had a strong emphasis on privacy and data protection methods, as well as federated learning methods <d-footnote>I would recommend read a survey paper or some online notes for a better explanation, but the basic idea behind federated learning is that in a distributed or cloud setting, we often want to use training data from clients (e.g. data on your mobile device), but we don't want to actually transfer this data to a centralized server for privacy reasons. Instead, we train a copy of the mobile locally, then transfer the gradients over to a server. Doing this at scale is quite difficult, as we are essentially doing sequences of delayed gradient updates. </d-footnote>. Privacy is a fairly math-heavy topic (especially outside of ML) because it often considers worst-case scenarios with high probability, so a lot of the papers in this domain are quite technical.</p>

<ol>
  <li>
<strong>Differential Privacy.</strong> Data privacy and anonymity can be mathematically guaranteed under differential privacy (DP) constraints, so adding these DP mechanisms to deep learning models with minimal overhead is an active area of research. Because DP is so mathematically sound, some work goes into studying DP under conditions common in machine learning <d-cite key="knop2023counting,jiang2023gaussian,ghazi2023userlevel"></d-cite> while others go into applying DP to machine learning problems <d-cite key="fan2022kmedian,qiao2023offline"></d-cite>.</li>
  <li>
<strong>Machine Unlearning</strong> looks into removing sensitive information that was present in a trained model’s training distribution, effectively wiping the information from a model altogether <d-cite key="kurmanji2023unbounded,chen2023fast"></d-cite>. These techniques are useful for combatting copyright issues, but they are not well understood <d-cite key="jia2024model"></d-cite> and can even lead to exploits <d-cite key="di2022hidden"> </d-cite>.</li>
  <li>
<strong>Client attacks on Federated learning.</strong> Federated learning involves lots of gradient information from different worker sources. If an attacker got a hold of some workers (e.g. a malicious mobile device user), they could, in theory, inject harmful information into a federated learning system (similar to an adversarial attack, formally called a Byzantine attack). It is far easier in a federated learning setting for attackers to become clients, so many studies look into poisoning attacks <d-cite key="nguyen2023iba,zhang2023afl"></d-cite> and robust defenses against them through things like trust scores <d-cite key="yan2023recess"></d-cite>, zero order optimization with DP guarantees <d-cite key="wang2023a"></d-cite>, and measuring divergence from the average <d-cite key="zhang2023fedfa"></d-cite>.</li>
  <li>
<strong>Failure modes of federated learning.</strong> Federated learning does gradient updates out of sync, which means 1) theoretical analysis is a serious pain and 2) failure modes are more apparent. Furthermore, with extra mechanisms for privacy, an open research question is understanding the convergence guarantees of federated learning under various techniques and mechanisms <d-cite key="chen2023finegrained,zhou2023every">.</d-cite>
</li>
</ol>

<h3 id="privacy-and-fed-learning-interesting-papers">[<span style="color:orange">Privacy and Fed. Learning</span>] Interesting Papers</h3>
<ol>
  <li>
<strong>Privacy Auditing with One (1) Training Run</strong> <d-cite key="steinke2023privacy"></d-cite>: We generally have to <em>prove</em> that an algorithm is differentially private (which is too hard in most cases!), but there are ways to audit or inspect empirically if an algorithm is differentially private. The problem is that DP is a probabilistic guarantee about the inclusion and exclusion of any data point, so we have to sample taking out data points. But sampling in the DP sense means re-training with or without data, which is extremely expensive. This work remarkably shows that they can audit with O(1) training runs under provable guarantees, which is a huge step from prior works. I skimmed the theoretical work, and it seems that they show the desired concentration bound of their method by showing that their process is stochasticly dominated by a binomial (a trick which appeared on my probability theory PSET!), and I’m excited to sit down and go through the math when I get the chance. Oh also, this paper won Outstanding Paper at this year’s conference.</li>
  <li>
<strong>Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training</strong> <d-cite key="huang2023lockdown"> </d-cite>: In federated learning, we want to defend against bad actors. But because we are adding gradients from many different sources to a centralized model, it is often hard to identify the source of these bad actors. In this work, they explicitly put sparsity constraints on the client to enforce training over a subspace of their data. The hope is that because these subspaces are generally disjoint, bad actors will not make updates in subspaces that good actors work over, making them easier to identify because they are isolated.</li>
  <li>
<strong>Training Private Models That Know What They Don’t Know</strong> <d-cite key="rabanser2023training"></d-cite>: I think this paper is a pretty simple example of the type of performance and computational overhead that privacy constraints can induce. I’m hoping to see these kinds of works extended to larger models and more modern datasets, but they’re nonetheless very important.</li>
</ol>

<h2 id="datasets-benchmarks-challenges">Datasets, Benchmarks, Challenges</h2>
<p>As NeurIPS is an AI-centric conference, there were datasets, benchmarks, and challenges for every topic above. There’s even a Datasets and Benchmarks track at NeurIPS. The more popular topics had more datasets (multimodal, LLM, etc.) and the datasets reflect the current needs of each field. A lot of the references put in earlier sections are dataset papers, so this section is going to be dedicated instead to some interest datasets I found while going through.</p>

<p>One thing I noticed though was a few papers on using <strong>synthetic data</strong> <d-cite key="yang2023freemask,pronovost2023scenario,ntavelis2023autodecoding,zhu2023genimage"></d-cite>! While these are not dataset papers, they seem to imply that synthetic data works well enough for training! I’m curious to see if synthetic datasets will become more prevalent, especially given how easy they are to scale.</p>

<h3 id="interesting-datasets">Interesting Datasets</h3>
<ol>
  <li>
<strong>Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text</strong> <d-cite key="zhu2023multimodal"></d-cite>: This is the multi-modal variant of the original C4 dataset, which has been a standard in LLM pre-training since its release. Because of how prevalent this dataset is going to be, I think it’s at worth at least taking a look at the data that’s going to be a part of most of our generative AI in the near future.</li>
  <li>
<strong>BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks</strong> <d-cite key="milani2023bedd"></d-cite>: MineRL is really cool. If you haven’t seen it already, I highly recommend taking a look, as Minecraft is the type of game that you would expect to be extremely complex for an AI to understand, but also simple enough that it seems feasible to eventually solve. This dataset provides a suite of labelled frame-action pairs and human labels that have been collected over the past two years and is extremely valuable for researchers working on this challenge.</li>
  <li>
<strong>GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image</strong><d-cite key="zhu2023genimage"></d-cite>: This is the closest thing to a synthetic dataset that I found at this year’s conference, but their focus was explicitly on creating AI-generated discriminators. I’m actually really curious to see if someone completely AI-generated a copy of the ImageNet dataset and trained models on it, how good would these models be? What kind of special differences, if any, could we find with these models and the originals?</li>
</ol>

<h2 id="other-topics">Other Topics</h2>
<p>The following sections are dedicated to topics that were either not as popular this year but are still broadly relevant or where I could not really get a sense of the central themes surrounding them. The main issue boils down to not having enough background on the topic, so I have to go through a few papers on the subject before comprehensively understanding what they’re doing. Regardless, they each had some interesting papers to highlight.</p>
<h3 id="interpretability-and-explainable-ai">Interpretability and Explainable AI</h3>
<p>Interpretability and explainable AI is really hard. We know that deep learning models tend to be a black box, and it’s generally because their inner mechanisms are too deep and intertwined with non-linearities that unless we make strong assumptions . I’d highly recommend going through the <a href="https://transformer-circuits.pub" rel="external nofollow noopener" target="_blank">https://transformer-circuits.pub</a> posts (start from the bottom), as they are extremely thorough and have been updated over time as well.</p>

<p>On the topic of mechanistic interpretability<d-footnote>Mechanistic interpretability is breaking down and reverse engineering a network to completely understand the inner workings and structure. Networks are often viewed as a computational graph composed of "circuits" that perform a specific function. I highly recommend looking at https://www.neelnanda.io/mechanistic-interpretability/quickstart</d-footnote>, I don’t necessarily think the works at NeurIPS 2023 are reflective of all that is going on in the community, but there were some interesting papers to share nonetheless.</p>

<ol>
  <li>
<strong>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer</strong> <d-cite key="tian2023scan"></d-cite> They analyze 1-layer transformers without positional encoding or residual connections, so their analysis is a bit different than some earlier transformer mechanistic interpretability works that focus on residual streams. I haven’t gotten the chance to read through their analysis carefully, but they claim that under these conditions, the attention mechanism initially attends to “distinct” (uncommon among many pairs) key tokens and continues putting weight on the highest co-occuring distinct tokens, but eventually these weights get fixed after a certain time in the training process. The idea is that common tokens (i.e. words that probably don’t really add to the semantics) are not attended, naturally filtering them out as dataset sizes increase.</li>
  <li>
<strong>Reverse Engineering Self-Supervised Learning</strong> <d-cite key="benshaul2023reverse"></d-cite>: They do self-supervised learning over CIFAR-100 and attempt to probe the intermediate layers, using the performance of probes over the course of training to justify their claims. Their conclusion is that self-supervised learning algorithms learn intermediate representations that are clustered based on semantic classes, and they show this using the performance of probes <strong>after</strong> accurate model performance, citing regularization constraints as the key driver.</li>
  <li>
<strong>The geometry of hidden representations of large transformer models</strong> <d-cite key="valeriani2023geometry"></d-cite>: This work attempts to uncover common geometrical patterns, mainly intrinsic dimension<d-footnote>Intrinsic dimension is the lowest dimension manifold that can approximate a dataset up to some error. The reason why we care about intrinsic dimension is that high-dimensional data is very hard to work with and significantly increases the complexity and failure modes of a learning algorithm. In practice, however, high-dimensional data like images often contain structure that leads to a low intrinsic dimension.</d-footnote> and a metric they call “neighborhood overlap”, across layers in transformer models. I am not familiar with the tool they use to measure intrinsic dimensionality and how accurate it is, but Figures 1 and 2 in their paper are pretty telling of the conclusions they draw.</li>
  <li>
<strong>Towards Automated Circuit Discovery for Mechanistic Interpretability</strong> <d-cite key="conmy2023automated"></d-cite>: There has been quite a lot of work on studying toy networks in mechanistic interpretability, and this paper attempts to write out a concrete framework for doing mechanistic interpretability research. They then attempt to automate one of the steps, which is activation patching (varying inputs to an activation) to find circuits that exhibit a particular behavior.</li>
  <li>
<strong>Explaining Predictive Uncertainty with Information Theoretic Shapley Values</strong> <d-cite key="watson2023explaining"></d-cite>: Shapley values <d-footnote>https://en.wikipedia.org/wiki/Shapley_value</d-footnote> are the solution to a cooperative game theory problem that satisfy a set of axioms. Informally (and related to explainable AI), they are a way of rigorously identifying which features contributed to a certain model prediction (although it is very expensive and scales poorly with training dataset sizes). In this paper, they motivate using a similar framework for measuring how training data features affect conditional entropy, which is directly linked to model uncertainty.</li>
  <li>
<strong>Theoretical and Practical Perspectives on What Influence Functions Do</strong><d-cite key="schioppa2023theoretical"></d-cite>: Influence Functions (IF) measure the change in a model prediction when re-weighting training examples, effectively relating a model’s output behavior to the training data. More formally, for a training dataset $\mathcal{S}$, suppose we perturb the weighting of data point $x$ by $\delta$. Let $\mathcal{L}(x,\theta)$ be the loss of a model with parameters $\theta$ on datapoint $x$, and let $\theta_{x,\delta}$ be the minimizer of the perturbed dataset. For a test data point $z$, the influence function $I(z,x,\theta^*)$ is defined as</li>
</ol>

\[I(z, x, \theta^{*}) = \nabla_{\delta} \mathcal{L} (z, \theta_{x, \delta}) \biggr|_{\delta=0}\]

<p>which is precisely the change in test loss through perturbation. Of course, this expression is not that interesting, but through Taylor expansion and assumptions about the loss function, it has an even nicer closed form solution (albeit with an inverse Hessian) in terms of $z,x$ and $\theta^*$, the minimizer of the original unperturbed dataset. For the aforementioned closed form solution to make sense, a lot of assumptions have to be made, which this paper tries to break down and explain why it may fail on real problems.</p>

<h3 id="implicit-bias">Implicit Bias</h3>
<p>From a statistical learning perspective, overparameterized neural networks under gradient descent should exhibit overfitting. However, it has been shown empirically that overparameterization is generally helpful and leads to good generalization ability. Additionally, it is well known that there are many machine learning algorithms that provably generalize well on certain domains (e.g. support vector machines on linearly separable data). Implicit bias is the notion that overparameterized deep neural networks tend towards solutions that are similar to algorithms that generalize well as an explanation for their generalization ability. There has been some nice theoretical work in the field, some of which was present at NeurIPS 2023.</p>

<ol>
  <li>
<strong>The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks</strong><d-cite key="frei2023doubleedged"></d-cite>: Usually implicit bias is understood as a net positive for pushing models to generalize better, but this paper rigorously shows convergence towards solutions that are weak to adversarial $\ell_2$ perturbations. The analysis is limited to logistic loss or exponential loss on 2-layer ReLU networks but is entirely theoretical.</li>
  <li>
<strong>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</strong> <d-cite key="wu2023implicit"></d-cite>: If you’ve ever done the proof of gradient descent for $L$-smooth convex functions, you’re probably aware of the Descent Lemma and the $&lt;1/L$ step size requirement. The <em>edge of stability</em><d-cite key="cohen2022gradient"></d-cite> is this step-size range where this monotonicity guarantee is broken, but empirically models still seem to be able to converge. Interestingly, they show superiority of logistic loss over exponential loss theoretically in that at regardless of the step size chosen, logistic loss will converge eventually while exponential loss will diverge from the implicit bias using gradient descent.</li>
</ol>

<h3 id="training-dynamics">Training Dynamics</h3>
<p>For generic gradient-based learning in neural networks, we are often interested in understanding common patterns that emerge during different phases of the training process. Grokking<d-footnote>https://openreview.net/pdf?id=9XFSbDPmdW</d-footnote> is an example of one type of phenomena, where (Nanda et al. 2023) show that in the overparameterized regime, models exhibit clear generalization behavior later into training despite maintaining low train error for a long time. Training dynamics that generalize across a certain class of models are difficult to identify, but discovering them will significantly improve our understanding of how to train models efficiently.</p>

<ol>
  <li>
<strong>Phase Diagram of Early Training Dynamics in Deep Neural Networks: Effect of The Learning Rate, Depth, and Width</strong> <d-cite key="kalra2023phase"></d-cite>: They motivate a bit about why sharpness matters as a metric for training dynamics, an analysis of how different hyperparameter choices affect how loss and sharpness over time. It’s a pretty interesting set of experiments in a toy setting, but I don’t know how observable the phases they observe are when you add all the tricks and complexities of modern deep learning.</li>
  <li>
<strong>Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?</strong> <d-cite key="george2023training"></d-cite>: Similar to the work above, it’s again a rigorous empirical analysis on a toy problem, but it’s important progress towards understanding what our models are learning during training.</li>
  <li>
<strong>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</strong> <d-cite key="adriaensen2023efficient"></d-cite>: I remember laughing when I first found this paper because it is literally inferencing what the training dynamics of another model will look like. It’s not the first work of its kind, but they incorporate the prior training curve data to do Bayesian inference of the training dynamics. I’m honestly also curious why regression doesn’t suffice.</li>
</ol>

<h3 id="embodied-ai">Embodied AI</h3>
<p>Embodied AI is a nascent field, but it focuses on building agents that can utilize multiple modalities. The field is set up to be a pre-cursor to the fabled AGI, but progress on this field hinges on the success of other fields like NLP, multi-modal learning, RL (although this is debated frequently). I didn’t see many works directly focusing on embodied AI, but I’m sure there will be many in the future.</p>

<ol>
  <li>
<strong>Egocentric Planning for Scalable Embodied Task Achievement</strong> <d-cite key="liu2023egocentric"></d-cite>: This was the winning agent for the ALFRED challenge at CVPR 2023<d-footnote>https://embodied-ai.org</d-footnote>, where agents solve language-specified tasks in a first-person simulation. It’s a domain-specific agent, but I think what’s interesting is understanding how they choose to ground skills and actions for planning the next action.</li>
  <li>
<strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</strong> <d-cite key="wang2023describe"></d-cite>: I’ve seen quite a few LLM-based approaches to Minecraft, but this is the first to do zero-shot planning. To add game context, they have a visual-language model map visual observations to language, and they use these models to further decide which LLM-generated goals are more feasible conditioned on the visual state.</li>
</ol>

<h3 id="neural-architecture-search">Neural Architecture Search</h3>
<p>Neural architecture search (NAS) is a class of algorithms that automatically search for model parameters (not just hyperparameters!) to optimize for some metric and has been around for a while. NAS is used a lot in finding model parameters for ultra low-cost machines like in <d-cite key="dong2023packqvit"></d-cite>. It’s actually quite complicated how these algorithms work, and I’d recommend reading Lillian Weng’s blog<d-footnote>https://lilianweng.github.io/posts/2020-08-06-nas/</d-footnote> to get a basic understanding of what people have done. The article is a bit old now, but I also don’t think the field itself has changed that much since.</p>

<ol>
  <li>
<strong>EvoPrompting: Language Models for Code-Level Neural Architecture Search</strong> <d-cite key="chen2023evoprompting"></d-cite>: Along the themes of can LLMs do everything, this paper looks into whether LLMs can aid in NAS. A lot of algorithms in NAS involve evolutionary search, so they query the LLM to generate the code for network parameters and mutate them over time through a soft-prompt tuning process.</li>
</ol>

<h3 id="neural-operators">Neural Operators</h3>
<p>Most data is inherently discretized, or it lies on some well-defined finite-dimensional Euclidean space. However, there are many problems that involve learning <em>functions</em> (e.g. approximating partial differential equations) where we instead want to learn mappings between functional spaces. I think this paper <d-footnote>https://arxiv.org/abs/2108.08481</d-footnote> explains it better than I will (and they write it in a way that’s quite intuitive to follow) but the basic idea is that the architecture and loss functions aren’t going to be any different than your standard neural network problem. The main difference is in framing, as our models are now acting over function spaces, so we treat the linear layers as linear integral operators on function spaces. The key benefit of working over function spaces is we no longer implicitly discretize our data, so varying resolutions of data do not affect our models, and there’s also some extra machinery that we can apply.</p>

<ol>
  <li>
<strong>Convolutional Neural Operators for robust and accurate learning of PDEs</strong> <d-cite key="raonić2023convolutional"></d-cite>: They prove similar universal approximation theorem guarantees for neural operators, but explicitly using convolutional layers. This is a pretty significant work in expanding neural operator use-cases, and they also show some examples of learning PDEs.</li>
</ol>

<h3 id="variational-inference-methods">Variational Inference Methods</h3>
<p>Bayesian methods are provably good at examining uncertainty in the posterior given your prior information and beliefs, but in practice they require approximating different intractible integrals in the closed for computation. More formally, suppose we want to compute the posterior $P(\theta|X)$, which represents the distribution over the parameters given our data. From Bayes rule,</p>

\[P \left(\theta|\mathbf{X} \right) = \frac{P \left(\mathbf{X}|\theta \right)P\left(\theta \right)}{P \left(\mathbf{X} \right)} = \frac{P \left(\mathbf{X}|\theta \right)P \left(\theta \right)}{\int_{\theta}P \left(\mathbf{X}|\theta^{\prime} \right) P \left(\theta^{\prime} \right) d \theta^{\prime}}\]

<p>The idea is that the denominator is intractible, but it’s actually a constant, so we can learn the posterior up to some normalization factor. So we instead will learn a simpler distribution \(Q(\mathbf{\theta})\) to approximate the posterior. The techniques for minimizing the difference between these distributions during optimization are well known and used frequently in applied ML.</p>

<ol>
  <li>
<strong>Joint Prompt Optimization of Stacked LLMs using Variational Inference</strong><d-cite key="sordoni2023joint"></d-cite>: This paper presents a very <em>unique</em> idea. Basically, if we stack langauge models (I had no idea people did this), you can treat the output of the $N-1$th language model as a latent parameterization of the $N$th language model, so we can perform variational inference to get a good estimate of the generative distribution of the $N$th language model. This theme of prompts being parameters is not new, especially if you’ve read earlier parts of this post, but the purpose of this work is to show that stacking language models can, in theory, provide better performance with a bit of extra machinery.</li>
</ol>

<h3 id="quantum-information-theory">Quantum Information Theory</h3>
<p>Quantum computers are notably faster at solving certain classes of problems (e.g. Shor’s algorithm for prime factorization), so if they end up replacing modern processors, we ideally want to ensure machine learning algorithms are efficient on them. While there were barely any papers on this topic, I did think it was interesting to look into.</p>

<ol>
  <li>
<strong>On quantum backpropagation, information reuse, and cheating measurement collapse</strong> <d-cite key="abbas2023quantum"></d-cite>: The backpropagation relies on re-using intermediate computations to achieve a linear runtime, which almost the entirety of deep learning is built on. In quantum mechanics, however, measurements fundamentally change the quantum state describing a system, so storing copies of a quantum state for future use doesn’t really make sense. It’s a really unique and interesting challenge to balance the tradeoffs between quantum computing and classical computing (freshman year me was once interested in pursuing quantum computing, but alas), and this paper provides a unique solution to achieve backpropagation scaling, potentially enabling the development of scalable overparameterized neural networks on quantum computers.</li>
</ol>

<h3 id="energy-based-models">Energy-based Models</h3>
<p>Energy based models (EBM) are a different way to view and train probabilistic models based on learning an energy function, but they provably can represent a wide varieties of algorithms such as k-means and maximum likelihood estimation<d-footnote>Yann Lecun has made lots of talks about energy-based models. I found these slides online: https://cs.nyu.edu/~yann/talks/lecun-20060816-ciar-1-ebm.pdf</d-footnote>. The key benefit is that we can ignore the intractible normalization constant computation needed for variational methods. Intuitively, I think it’s easiest to understand them in a contrastive learning framework, where an energy function $F: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ describes similarity between pairs of data points. GFlowNets<d-cite key="NEURIPS2021_e614f646"></d-cite> are an exciting recent development of EBMs for generative modeling, and a few papers at this conference studied applications of them<d-cite key="zhang2023let,atanackovic2023dyngfn,zhu2023sampleefficient"></d-cite>.</p>

<ol>
  <li>
<strong>Energy Transformer</strong><d-cite key="hoover2023energy"></d-cite>: I think EBMs have the potential for a major breakthrough, but it really depends on whether they can reliably crack a really hard or popular problem. This work adapts EBMs for modern deep learning mechanisms like attention, and I’m interested to see if people take this further to things like LMs!</li>
</ol>

<h3 id="curriculum-learning">Curriculum Learning</h3>
<p>Curriculum learning is the notion of learning simple tasks first before learning complex tasks. This intuitively makes sense in RL, where we want to learn simple skills before learning complex high-level strategies, but it is not limited to RL. I have always been kind of skeptic of curriculum learning because it is not well understood, but at the same time there have been some successful use cases of it.</p>

<ol>
  <li>
<strong>Curriculum Learning With Infant Egocentric Videos</strong><d-cite key="sheybani2023curriculum"></d-cite>: Funnily enough, this was a research direction I was interested in taking back in 2021, but I just never got access to the data to do it. I’m curious though if curriculum learning is particularly useful for self-supervised learning because it helps form the intermediate representation space in a nicer way or order, i.e. starting from representation space A, it’s a lot easier to move to B through gradient-based learning, so we should learn A first.</li>
</ol>

<h3 id="anomaly-detection">Anomaly Detection</h3>
<p>Anomaly detection (AD)<d-footnote>https://arxiv.org/abs/2007.02500</d-footnote> generally refers to detecting outliers or unexpected behavior in data. These methods are especially important for tasks where we want our models to be risk-averse. I was surprised to see quite a few works focused on this field at this year’s conference, but considering how diverse the applications are, it kind of makes sense. I’m not familiar with the field at all, but a few of them did pique my interest.</p>

<ol>
  <li>
<strong>Unsupervised Anomaly Detection with Rejection</strong><d-cite key="perini2023unsupervised"></d-cite>: In my mind anomaly detection makes the most sense in as unsupervised or self-supervised learning problem because they inherently pop up in any domain. Their work deals providing strong theoretical guarantees for a general unsupervised algorithm that to reject low-confidence outputs.</li>
  <li>
<strong>Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach</strong> <d-cite key="yoon2023energybased"></d-cite>: One way of detecting anomalies is to have a general understanding of what your data should look like. In this work, the authors follow this idea by training an energy-based model to approximate the low-dimensional manifold that the training data lies on and use the energy function as a score for identifying anomalies.</li>
</ol>

<h3 id="class-imbalance-approaches">Class Imbalance Approaches</h3>
<p>For multi-class classification problems, we generally want a uniform distribution of class labels across the dataset so models do not converge to the most frequently occurring label with probability 1. Class imbalance is a long-standing problem in optimization-based methods, and there has been a lot of work to try and combat it (e.g. data augmentation to balance classes, sampling, new loss funcitons).</p>

<ol>
  <li>
<strong>Simplifying Neural Network Training Under Class Imbalance</strong><d-cite key="shwartzziv2023simplifying"></d-cite>: This paper makes some pretty strong claims: that just by tuning hyperparameters like batch sizes, label smoothing, optimizers, and data augmentation, we can combat class imbalance. The experiments they run are on fairly simple and old datasets, but this is pretty common for works that study general deep learning phenomena. I’m just not sure how well these observations hold at scale.</li>
</ol>

<h3 id="continual-learning">Continual Learning</h3>
<p>I was interested in continual learning, also known as lifelong learning, when I first discovered machine learning. I think continual learning, like multimodal learning, is another important piece towards artificial general intelligence (AGI), but it is not as popular at the moment. The general idea is a learning framework that continues to adapt and learn over time, but well-known problems like catastrophic forgetting (model learns A then B. model will forget A.) and plasticity (how easily does a model learn something new) make continual learning extremely difficult. It is entirely possible that completely different paradigms are necessary for continual learning, but this is an active field of research<d-footnote>A useful introduction to pre-existing works is: https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning</d-footnote>.</p>

<ol>
  <li>
<strong>A Definition of Continual Reinforcement Learning</strong><d-cite key="abel2023definition"></d-cite>: Reinforcement learning is explicitly a reward-maximizing algorithm under a fixed environment, which is sort of at odds with continual learning, where we want a policy to continue to adapt. This paper lays some of the groundwork for defining the necessary vocabulary and tools to approach continual learning in a reinforcement learning setting.</li>
  <li>
<strong>RanPAC: Random Projections and Pre-trained Models for Continual Learning</strong><d-cite key="mcdonnell2023ranpac"></d-cite>: We ideally want to leverage pre-trained foundation models as a base for continual learning, but because we do not have a strong mechanistic understanding of these models, it is unclear how changing these weights over time in a continual learning sense will affect the model performance. This work is a first step into performing parameter updates on pre-trained models without forgetting.</li>
</ol>

<h3 id="deep-learning-theory">Deep Learning Theory</h3>
<p>Deep learning theory is specifically the study of deep neural network models, and generally centers around dense linear layers with activations (at least for now). The most well known result is probably the Neural Tangent Kernel (NTK) <d-cite key="jacot2020neural"></d-cite>, which describes the behavior of networks as you take their layer width to infinity. Deep learning theory is an active area of research that I personally know little about, but at the very least, I’ve observed two common approaches at this conference that were used to study it. That being said, these are definitely not the only two tools used.</p>
<ol>
  <li>
<strong>Kernel methods.</strong> This was formalized in the NTK work, but there is a provable duality between gradient descent in the infinite width limit and kernel gradient descent over the NTK as the kernel. Because of the rich theory present in kernel methods, we can apply these tools to study the behavior of neural networks as well.</li>
  <li>
<strong>Mean-field theory</strong> has historically been applied in probability theory and physics settings (thanks to my roommate Evan for explaining the basic idea to me), and is essentially a suite of tools for solving extremely high-dimensional and complex dynamics by approximating their behavior as an “average”. It has been applied extensively to deep learning theory as well for studying the behavior of infinite-width and infinite-depth networks as an alternative to viewing everything as kernels.</li>
</ol>

<p>There’s a bit too much necessary background information that goes into the machinery required for deep learning theory for me to really understand any of these papers, so unfortunately I do not have anything to list for interesting papers (yet at least!). I can list some examples though: <d-cite key="bordelon2023dynamics,fiedler2023kernelbased,kumano2023adversarial">&lt;/d-footnote&gt;</d-cite></p>

<h3 id="bio-inspired-ai">Bio-inspired AI</h3>
<p>Artificial Intelligence is an extension of our desire to mimic biological intelligence (although I wish it wasn’t), so naturally we have a lot to gain by using ideas we discover from biology. I think over time we will always see a steady number of works on bio-inspired AI, but so far, I haven’t seen too many works of this type that really take off (e.g. spiking neural networks). Part of the problem is that we know very little about our brains and how they function, so potentially once we figure that out, we’ll start implementing those mechanisms in our AI!</p>

<ol>
  <li>
<strong>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?</strong> <d-cite key="pandey2023vision"></d-cite>: I think it’s always sort of assumed that humans are far more data efficient than neural networks. They compare ViT performance on object recognition against newborn chicks with the same visual data and show that ViTs actually solved the same tasks. This paper is interesting, but I think the task is too simple and doesn’t capture the efficiency and task complexity tradeoff that would be more interesting to know.</li>
</ol>

<h3 id="domain-specific-applications-of-ai">Domain-specific applications of AI</h3>
<p>There were a lot of very cool domain-specific applications of AIML at NeurIPS this year, most of
which was completely beyond me. Applications and datasets for the natural sciences were probably the most popular, especially related to protein modeling or protein functional prediction <d-cite key="liu2023predicting,gao2023proteininvbench,ahdritz2023openproteinset"></d-cite> and molecules <d-cite key="liu2023symmetryinformed"></d-cite>. There were also works in chemistry <d-cite key="guo2023large,tavakoli2023ai">, law <d-cite key="östling2024cambridge"></d-cite>, and even circuit prediction<d-cite key="zou2023circuit"></d-cite>!</d-cite></p>

<ol>
  <li>
<strong>Circuit As Set of Points</strong><d-cite key="zou2023circuit"></d-cite>: I wanted to highlight this paper because I thought it was zany. They literally treat circuit discovery as a <em>point-cloud prediction</em> problem instead of like a graph as in prior works. The authors say it’s to avoid pre-processing, so they’re literally just feeding in a raw circuit and treating it like a point cloud. Works like these are honestly really exciting, even if doesn’t end up become the standard method.</li>
</ol>

<h3 id="reproducibility-studies">Reproducibility Studies</h3>
<p>So I didn’t know reproducibility experiments were publishable at major AI conferences like NeurIPS, but I’m happy to discover that it is. This year featured quite a few reproducibility experiments for older works, and it seems like they all follow a kind of template of how they should be conducted. I have experience working with repositories that just do not align with the results of described in the papers, so I know how annoying it is to not know if a paper actually works. I saw this paper <d-cite key="kleuver2023reproducibility"></d-cite> that attempts to reproduce the results of <d-cite key="keswani2022proto2proto"></d-cite>, but I found it funny because they basically roast their documentation and go into detail about the weaknesses in their experiments. With AI research rapidly growing, I think it’s getting more and more important that we set a bar for reproducibility.</p>

<h2 id="other-papers-i-liked">Other Papers I Liked</h2>

<ol>
  <li>
    <p><strong>Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models.</strong> <d-cite key="ortizjimenez2023task"></d-cite>: It seems that literally adding the weights of a bunch of fine-tuned experts on distinct downstream tasks can actually lead to a generalized model that solves all of them. From a linear algebra perspective, this only makes sense if the fine-tuning process pushes the weights into their own distinctive regions of the weight space. They motivate this sort of behavior as weight disentanglement and use it to fine-tune models for better weight addition properties.</p>
  </li>
  <li>
    <p><strong>Language-based Action Concept Spaces for Video Self-Supervised Learning.</strong> <d-cite key="ranasinghe2023languagebased"></d-cite>: I’ve recently been interested in more abstract representations of concepts like actions for building agents that can “think” using these representations. I think a lot of works on thinking in terms of actions and skills has been in terms of LLMs, but this work examines it from an encoder perspective.</p>
  </li>
  <li>
    <p><strong>The Grand Illusion: The Myth of Software Portability and Implications for ML Progress</strong> <d-cite key="mince2023grand"></d-cite>: I generally take for-granted the libraries that I use, so it was cool to see a study on the performance of popular frameworks across a variety of devices. These kinds of details are the things that I’ve been more and more interested in understanding, so I’m happy to see a paper like this at the conference.</p>
  </li>
  <li>
    <p><strong>Human-Guided Complexity-Controlled Abstractions</strong> <d-cite key="peng2023humanguided"></d-cite>: To develop agents that can interact with the environment the way we do, we first have to understand how to integrate our mental abstraction hierarchy into an agent’s. If you look at some of the papers I talked about earlier like <d-cite key="yang2023hierarchical,evans2023creating"></d-cite>, it is clear that people are interested in building action hierarchies for agents. This paper looks into understanding what level of abstraction or complexity is required to understand and execute actions for different types of tasks. I’m hoping to build a mental model of how we would go about building robust systems that can act in this way.</p>
  </li>
  <li>
    <p><strong>The Tunnel Effect: Building Data Representations in Deep Neural Networks</strong> <d-cite key="masarczyk2023tunnel"></d-cite>: This paper provides some empirical insight into how data is represented throughout generic overparameterized neural networks. They propose this tunnel effect hypothesis, where the early layers of the network focus on linearly separable representations that focus on learning the actual task, while the later layers are just compression layers that harm generalization performance. They also suggest that regardless of model capacity (as long as it is sufficient), models will allocate the same amount of capacity to a specific task, which may also offer some insight into the implicit bias of models and why they tend not to overfit in the overparameterized regime.</p>
  </li>
</ol>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This was a really rewarding process for me, not only from a knowledge standpoint. Before doing this little exercise, I was feeling a bit burnt out from school and learning in general. For a while, I had been thinking really hard researching how to inject language information for policy learning, but a lot of directions I had in mind just didn’t seem to make sense in the end. I had spent a lot of time reading in this specific direction, and I was a bit tired of seeing the same flavour of methods being applied. So it was honestly really refreshing to take a step back and reel in what people had been doing. I didn’t get a chance to go to NeurIPS 2023 (or any major conference for that matter) because of my courseload, but at least now I can say I know at least a little bit about what was going on there!</p>

<p>During this process I also compiled a list of open research questions I think are worth pursuing, which I may clean up in the future and put up. If you made it this far, thanks for giving it a read! It took a long time to synthesize these resources and figure out the structure of this article, so regardless of your background or prior knowledge, I hope this was at least somewhat useful for you!</p>

<h2 id="random-terms">Random Terms</h2>
<p>I kept track of a list of terms that I had to Google while going through these abstracts. I probably
also ended up looking into more and forgot to put them in this list, but in case you’re interested,
<del>here they are below</del>. Edit: Ok so, this article ended up being <strong>way</strong> longer than intended, so instead if you want this list you should email me. My email is on my home page.</p>

<h2 id="citation">Citation</h2>
<p>Just as a formality, if you want to cite this for whatever reason, use the BibTeX below.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2024neuripshighlights,
  title   = "Highlights of NeurIPS 2023 from Abstracts",
  author  = "Zhang, Alex",
  journal = "Alex's Writing",
  year    = "2024",
  month   = "Jan",
  url     = "https://alexzhang13.github.io/blog/2024/neurips2023/"
}
</code></pre></div></div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/neurips2023.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    

    <div class="container">
      © Copyright 2025
      Alex
      L.
      Zhang. 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-K2L3VESDMP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-K2L3VESDMP');
  </script>




    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


  
</body>
</html>
